
CONTAINER -->
=================================================================================
Imagine you're developing a killer web app that has three main components - a React frontend, a Python API, and a PostgreSQL database. 
If you wanted to work on this project, you'd have to install Node, Python, and PostgreSQL.
How do you make sure you have the same versions as the other developers on your team? Or your CI/CD system? Or what's used in production?
How do you ensure the version of Python (or Node or the database) your app needs isn't affected by what's already on your machine? How do you manage potential conflicts?

To solve the above, we have CONTAINERS. These are isolated processes for each of your app's components. 
Each component, the frontend React app, the Python API engine, and the database - runs in its own isolated environment, completely isolated from everything else on your machine.

Self-contained -> Each container has everything it needs to function with no reliance on any pre-installed dependencies on the host machine.
Isolated       -> Since containers are run in isolation, they have minimal influence on the host and other containers, increasing the security of your applications.
Independent    -> Each container is independently managed. Deleting one container won't affect any others.
Portable       -> Containers can run anywhere! The container that runs on your development machine will work the same way in a data center or anywhere in the cloud!


VM's vs CONTAINER :
    VM is an entire operating system with its own kernel, hardware drivers, programs, and applications. Spinning up a VM only to isolate a single application is a lot of overhead.
    A container is simply an isolated process with all of the files it needs to run. If you run multiple containers, they all share the same kernel, allowing you to run more applications on less infrastructure.

    While running containers from an image, it asks for a name and port binding
    - If a name is not given, a random name is generated
    - If a port is not given, a random port is allocated. If given an already in-use port, container simply doesnt run.


EXPOSING PORTS -->
=================================================================================
Containers provide isolated processes for each component of your application. 
Each component - a React frontend, a Python API, and a Postgres database - runs in its own sandbox environment, completely isolated from everything else on your host machine. 
This isolation is great for security and managing dependencies, but it also means you can’t access them directly. 
For example, you can’t access the web app in your browser. That’s where port publishing comes in.

SPECIFIC PORT : 
    Publishing a port provides the ability to break through a little bit of networking isolation by setting up a forwarding rule. 
    As an example, you can indicate that requests on your host’s port 8080 should be forwarded to the container’s port 80. 
    Publishing ports happens during container creation using the -p (or --publish) flag with docker run.
    When a port is published, it's published to all network interfaces by default. This means any traffic that reaches your machine can access the published application. 
    Be mindful of publishing databases or any sensitive information

        --> $ docker run -d -p HOST_PORT:CONTAINER_PORT nginx
            HOST_PORT       : The port number on your host machine where you want to receive traffic
            CONTAINER_PORT  : The port number within the container that's listening for connections


EPHEMERAL PORTS :
    At times, you may want to simply publish the port but don’t care which host port is used. 
    In these cases, you can let Docker pick the port for you. To do so, simply omit the HOST_PORT configuration.
    For example, the following command will publish the container’s port 80 onto an ephemeral port on the host:

            --> $ docker run -p 80 nginx

    Once the container is running, using docker ps will show you the port that was chosen:
        $ docker ps
        CONTAINER ID   IMAGE         COMMAND                  CREATED          STATUS          PORTS                    NAMES
        a527355c9c53   nginx         "/docker-entrypoint.…"   4 seconds ago    Up 3 seconds    0.0.0.0:54772->80/tcp    romantic_williamson

ALL PORTS :
    When creating a container image, the EXPOSE instruction is used to indicate the packaged application will use the specified port. 
    These ports aren't published by default. With the -P or --publish-all flag, you can automatically publish all exposed ports to ephemeral ports. 
    This is quite useful when you’re trying to avoid port conflicts in development or testing environments.
    For example, the following command will publish all of the exposed ports configured by the image:

        --> $ docker run -P nginx

NOTE :
    -->$ docker run -dp 0.0.0.0:3000:3000 YOUR-USER-NAME/getting-started
    -->$ docker run -dp 127.0.0.1:3000:3000 YOUR-USER-NAME/getting-started
    Binding to 127.0.0.1 only exposes a container's ports to the loopback interface. 
    Binding to 0.0.0.0, however, exposes the container's port on all interfaces of the host, making it available to the outside world.



ENVIRONIMENTAL VARIABLES -->
=================================================================================
This option sets an environment variable foo inside the container with the value bar.

    --> $ docker run -e foo=bar postgres env

    You will see output like the following:
    HOSTNAME=2042f2e6ebe4
    foo=bar

The .env file acts as a convenient way to set environment variables for your Docker containers without cluttering your command line with numerous -e flags. 
To use a .env file, you can pass --env-file option with the docker run command.

    --> $ docker run --env-file .env postgres env

You can use the --memory and --cpus flags with the docker run command to restrict how much CPU and memory a container can use. 
For example, you can set a memory limit for the Python API container, preventing it from consuming excessive resources on your host. 

    --> $ docker run -e POSTGRES_PASSWORD=secret --memory="512m" --cpus="0.5" postgres


CONTAINER's FILE SYATEM -->
=================================================================================
When a container runs, it uses the various layers from an image for its filesystem. 
Each container also gets its own "scratch space" to create/update/remove files. 
Any changes won't be seen in another container, even if they're using the same image.

To see this in action, you're going to start two containers. 
In one container, you'll create a file. In the other container, you'll check whether that same file exists.

    -->$ docker run --rm alpine touch greeting.txt
        Any commands you specify after the image name (in this case, alpine) are executed inside the container. 
        In this case, the command touch greeting.txt puts a file named greeting.txt on the container's filesystem.

    --> Run a new Alpine container and use the stat command to check whether the file exists.
        docker run --rm alpine stat greeting.txt
        You should see output similar to the following that indicates the file does not exist in the new container.
        stat: can't stat 'greeting.txt': No such file or directory
        The greeting.txt file created by the first container did not exist in the second container. 
        That is because the writeable "top layer" of each container is isolated. 
        Even though both containers shared the same underlying layers that make up the base image, the writable layer is unique to each container.


PERSISTING CONTAINER DATA --> [[ Using VOLUMES - Can be viewd in docker desktop ]]
=================================================================================
When a container starts, it uses the files and configuration provided by the image. 
Each container is able to create, modify, and delete files and does so without affecting any other containers. 
When the container is deleted, these file changes are also deleted.
While this ephemeral nature of containers is great, it poses a challenge when you want to persist the data. 
For example, if you restart a database container, you might not want to start with an empty database. So, how do you persist files?

Volumes provide the ability to connect specific filesystem paths of the container back to the host machine. 
If you mount a directory in the container, changes in that directory are also seen on the host machine. 
If you mount that same directory across container restarts, you'd see the same files.

Volumes are a storage mechanism that provide the ability to persist data beyond the lifecycle of an individual container. 
Think of it like providing a shortcut or symlink from inside the container to outside the container.
As an example, imagine you create a volume named log-data.

    -->$ docker volume create log-data

    -->$ docker run -d -p 80:80 -v log-data:/logs docker/welcome-to-docker
         When starting a container with the following command, the volume will be mounted (or attached) into the container at /logs
         If the volume log-data doesn't exist, Docker will automatically create it for you.

When the container runs, all files it writes into the /logs folder will be saved in this volume, outside of the container. 
If you delete the container and start a new container using the same volume, the files will still be there.
You can attach the same volume to multiple containers to share files between containers. 
This might be helpful in scenarios such as log aggregation, data pipelines, or other event-driven applications.

Volumes have their own lifecycle beyond that of containers and can grow quite large depending on the type of data and applications you’re using. 
The following commands will be helpful to manage volumes:

    -->$ docker volume ls                     - list all volumes
    -->$ docker volume rm <volume-name-or-id> - remove a volume (only works when the volume is not attached to any containers)
    -->$ docker volume prune                  - remove all unused (unattached) volumes


HANDS-ON :
    -->$ docker run --name=db -e POSTGRES_PASSWORD=secret -d -v postgres_data:/var/lib/postgresql/data postgres
    -->$ docker exec -ti db psql -U postgres

    --> CREATE TABLE tasks (
        id SERIAL PRIMARY KEY,
        description VARCHAR(100)
        );
        INSERT INTO tasks (description) VALUES ('Finish work'), ('Have fun');
    
    --> SELECT * FROM tasks;
    -->     id | description
            ----+-------------
            1 | Finish work
            2 | Have fun
            (2 rows)
    
    -->$ \q ( Exit from a container )

    -->$ docker stop db
    -->$ docker rm db
    -->$ docker run --name=new-db -d -v postgres_data:/var/lib/postgresql/data postgres 
    -->$ docker exec -ti new-db psql -U postgres -c "SELECT * FROM tasks"


REMOVE VOLUMES :
Before removing a volume, it must not be attached to any containers. 
If you haven’t removed the previous container, do so with the following command (the -f will stop the container first and then remove it):

    -->$ docker rm -f new-db

There are a few methods to remove volumes, including the following:
    --> Select the Delete Volume option on a volume in the Docker Desktop Dashboard.
    --> Use the docker volume rm command:
        -->$ docker volume rm postgres_data
    --> Use the docker volume prune command to remove all unused volumes:
        -->$ docker volume prune



SHARE HOST FILES WITH CONTAINER -->
=================================================================================
Containers are very much isolated and run on their own but it also means containers can't directly access data on the host machine by default.
Consider a scenario where you have a web application container that requires access to configuration settings stored in a file on your host system. 
This file may contain sensitive data such as database credentials or API keys. 
Storing such sensitive information directly within the container image poses security risks, especially during image sharing. 

To address this challenge, Docker offers storage options that bridge the gap between container isolation and your host machine's data.
    
    --> VOLUME POINTS [[ Resides in Docker's virtual machine ]]
        If you want to ensure that data generated or modified inside the container persists even after the container stops running, you would opt for a volume.
        A volume mount is a great choice when you need somewhere persistent to store your application data.
        The mountopoint location resides in a docker virtual machine and cant be accesses directly by macos terminal. 
        
        --> SYNTAX : type=volume,src=my-volume,target=/usr/local/data

        -->$ docker volume create todo-db
        -->$ docker run -dp 127.0.0.1:3000:3000 --mount type=volume,src=todo-db,target=/etc/todos getting-started


    --> BIND MOUNTS [[ Resides in host machine directly ]]
        If you have specific files or directories on your host system that you want to directly share with your container, like configuration files or development code, then you would use a bind mount. 
        It's like opening a direct portal between your host and container for sharing. 
        Bind mounts are ideal for development environments where real-time file access and sharing between the host and container are crucial.
        
        --> SYNTAX : type=bind,src=/path/to/data,target=/usr/local/data
        
        -->$ docker run -it --mount type=bind,src="$(pwd)",target=/src ubuntu bash


        +---------------------+--------------------------------------------+-------------------------------------------+
        | Feature             | Volume Mounts                              | Bind Mounts                               |
        +---------------------+--------------------------------------------+-------------------------------------------+
        | **Location**        | Managed by Docker, stored in Docker’s      | Linked directly to a path on the host     |
        |                     | storage directory  filesystem.             |                                           |
        |                     | (e.g., `/var/lib/docker/volumes/`).        |                                           |
        +---------------------+--------------------------------------------+-------------------------------------------+
        | **Persistence**     | Data persists even if the container is .   | Data is tied to the host filesystem and   |
        |                     | removed                                    | remains as long as the host path exists.  |
        +---------------------+--------------------------------------------+-------------------------------------------+
        | **Use Case**        | Best for persistent data storage,          | Best for development environments where   |
        |                     | especially when working with databases or  | files on the host need to be synchronized |
        |                     | application data.                          | with the container.                       |
        +---------------------+--------------------------------------------+-------------------------------------------+
        | **Performance**     | Optimized by Docker for performance.       | Depends on the host's filesystem; not     |
        |                     |                                            | optimized by Docker.                      |
        +---------------------+--------------------------------------------+-------------------------------------------+
        | **Flexibility**     | Can be shared between multiple containers. | Tied to the host filesystem; can be       |
        |                     |                                            | shared, but only if mounted from the host.|
        +---------------------+--------------------------------------------+-------------------------------------------+
        | **Permissions**     | Docker manages permissions and ownership.  | Permissions and ownership are managed     |
        |                     |                                            | by the host system.                       |
        +---------------------+--------------------------------------------+-------------------------------------------+
        | **Portability**     | Can be backed up or moved between Docker   | Dependent on the host machine, so not as  |
        |                     | hosts.                                     | portable.                                 |
        +---------------------+--------------------------------------------+-------------------------------------------+


Both -v (or --volume) and --mount flags used with the docker run command let you share files or directories between your local machine (host) and a Docker container. 
The -v flag is simpler and more convenient for basic volume or bind mount operations. 
If the host location doesn’t exist when using -v or --volume, a directory will be automatically created.

Imagine you're a developer working on a project. You have a source directory on your development machine where your code resides. 
When you compile or build your code, the generated artifacts (compiled code, executables, images, etc.) are saved in a separate subdirectory within your source directory. 
In the following examples, this subdirectory is /HOST/PATH. Now you want these build artifacts to be accessible within a Docker container running your application. 
Additionally, you want the container to automatically access the latest build artifacts whenever you rebuild your code.
Docker recommends using the --mount syntax instead of -v. It provides better control over the mounting process and avoids potential issues with missing directories.
Here's a way to use docker run to start a container using a bind mount and map it to the container file location.

        -->$ docker run -v /HOST/PATH:/CONTAINER/PATH -it nginx

The --mount flag offers more advanced features and granular control, making it suitable for complex mount scenarios or production deployments. 
If you use --mount to bind-mount a file or directory that doesn't yet exist on the Docker host, the docker run command doesn't automatically create it for you but generates an error.

        -->$ docker run --mount type=bind,source=/HOST/PATH,target=/CONTAINER/PATH,readonly nginx


VIEW DETAILS OF MOUNTS:
    -->$ docker volume inspect todo-db
        You should see output like the following:

        [
            {
                "CreatedAt": "2019-09-26T02:18:36Z",
                "Driver": "local",
                "Labels": {},
                "Mountpoint": "/var/lib/docker/volumes/todo-db/_data",
                "Name": "todo-db",
                "Options": {},
                "Scope": "local"
            }
        ]
        
    The Mountpoint is the actual location of the data on the disk. 
    Note that on most machines, you will need to have root access to access this directory from the host.
    But these MountPoint locations are created in docker virtual machine and not directly accessible from macOS. 


FILE PERMISSIONS WHILE MOUNTING :
    When using bind mounts, it's crucial to ensure that Docker has the necessary permissions to access the host directory. 
    To grant read/write access, you can use the :ro flag (read-only) or :rw (read-write) with the -v or --mount flag during container creation. 
    For example, the following command grants read-write access permission.

        -->$ docker run -v HOST-DIRECTORY:/CONTAINER-DIRECTORY:rw nginx

    Read-only bind mounts let the container access the mounted files on the host for reading, but it can't change or delete the files. 
    With read-write bind mounts, containers can modify or delete mounted files, and these changes or deletions will also be reflected on the host system. 
    Read-only bind mounts ensures that files on the host can't be accidentally modified or deleted by a container.


SYNCHRONIZED FILE SHARE :
    As your codebase grows larger, traditional methods of file sharing like bind mounts may become inefficient or slow, especially in development environments where frequent access to files is necessary. 
    Synchronized file shares improve bind mount performance by leveraging synchronized filesystem caches. 
    This optimization ensures that file access between the host and virtual machine (VM) is fast and efficient.