====================================================================
TOTAL AVAILABLE SERVICE CATEGORIES  IN AWS : (As of January 2021)
====================================================================

NOTE :
########
This page contains a detailed description of what the services do in each of the categories .
Topics having a higher importance have individual chapters written after them.
Recommended to View the YouTube Video , link provided beside the topic heading before going through the material.
Also view the reference pictures attached if any in \Git_Atom\Cloud_AWS\Reference_pics .
If we have 2 or more services performing a similar activity , the likely chances are that the basic one is a free service while others are PAID services .
  --> AWS Cloud Watch (free) - AWS Detective (30 days free)- AWS Guard Duty  (30 days free)

-------------------------------------------------------------------------------------------------------------------
SERVICES COVERED :                                                SERVICES NOT COVERED :
-------------------------------------------------------------------------------------------------------------------
Analytics                                                         AR & VR
Application Integration                                           Blockchain
Billing & Cost Management                                         Front-End Web & Mobile
Business Applications                                             Game Development
Compute                                                           Internet of Things (IoT)
Containers                                                        Machine Learning
Cryptography & PKI                                                Media Services
Customer Enablement Services                                      Quantum Computing
Customer Engagement                                               Robotics
Data Base                                                         Satellite
Developer Tools
End User Computing
Management & Governance
Migration & Transfer
Networking & Content Delivery
Security, Identity, & Compliance
Storage



##################################################################################################################
##################################################################################################################
ANALYTICS :
##################################################################################################################
##################################################################################################################

1. AMAZON APP FLOW :  https://www.youtube.com/watch?v=USzaWjjjOJI
Bidirectional data transfer from SAAS Applications To AWS
Amazon App Flow is a fully-managed integration service that enables you to securely exchange data between software as a service (SaaS) applications, such as Salesforce, Slack and AWS services, such as Amazon Simple Storage Service (Amazon S3) and Amazon Redshift.
For example, you can ingest contact records from Salesforce to Amazon Redshift or pull support tickets from Zendesk to an Amazon S3 bucket.

2. AMAZON ATHENA :  https://www.youtube.com/watch?v=M5ptG0YaqAs&t=1s
Amazon Athena is an interactive query service that makes it easy to analyze data directly in Amazon Simple Storage Service (Amazon S3) using standard SQL.
With a few actions in the AWS Management Console, you can point Athena at your data stored in Amazon S3 and begin using standard SQL to run ad-hoc queries and get results in seconds.
Athena is serverless, so there is no infrastructure to set up or manage, and you pay only for the queries you run.
Athena scales automatically—running queries in parallel—so results are fast, even with large datasets and complex queries.
You can query structured , unstructured and semi structured data as well. Examples include CSV, JSON, or columnar data formats such as Apache Parquet and Apache ORC.
You can then store results from queries directly into another bucket in S3 or download then to local.

3. AMAZON CLOUD-SEARCH :  https://www.youtube.com/watch?v=gpG16MFnEH8&t=14s
With Amazon CloudSearch, you can quickly add rich search capabilities to your website or application.
You don't need to become a search expert or worry about hardware provisioning, setup, and maintenance.
With a few clicks in the AWS Management Console, you can create a search domain and upload the data that you want to make searchable, and Amazon CloudSearch will automatically provision the required resources and deploy a highly tuned search index.
You can easily change your search parameters, fine tune search relevance, and apply new settings at any time.
      Free text, Boolean, and Faceted search - Autocomplete suggestions - Customizable relevance ranking and query-time rank expressions - Field weighting
      Geospatial search - Highlighting - Support for 34 languages

4. AWS DATA EXCHANGE : https://www.youtube.com/watch?v=Lu9QVJ0Rml4&t=26s
AWS Data Exchange is a service that makes it easy for AWS customers to securely exchange file-based data sets in the AWS Cloud.
As a subscriber, you can find and subscribe to hundreds of products from qualified data providers.
Then, you can quickly download the data set or copy it to Amazon S3 for use across a variety of AWS analytics and machine learning service.
Providers in AWS Data Exchange have a secure, transparent, and reliable channel to reach AWS customers and grant existing customers their subscriptions more efficiently.
To promote a safe, secure, and trustworthy service for everyone, AWS Data Exchange scans all data published by providers before it is made available to subscribers. If AWS detects malware, the affected asset is removed.
Used by colleges, hospitals , scientists to get data from various data lakes .

5. AWS DATA PIPELINE :
AWS Data Pipeline is a web service that you can use to automate the movement and transformation of data.
With AWS Data Pipeline, you can define data-driven workflows, so that tasks can be dependent on the successful completion of previous tasks.
You define the parameters of your data transformations and AWS Data Pipeline enforces the logic that you've set up.
A pipeline schedules and runs tasks by creating Amazon EC2 instances to perform the defined work activities.
You upload your pipeline definition to the pipeline, and then activate the pipeline.
Task Runner polls for tasks and then performs those tasks. For example, Task Runner could copy logfiles to Amazon S3 and launch Amazon EMR clusters.
Task Runner is installed and runs automatically on resources created by your pipeline definitions.

6. AMAZON ELASTIC-SEARCH SERVICE : https://www.youtube.com/watch?v=4Zw1IOxW-oA&t=26s
Used to analyse and get real time  insights on machine generated data at a peta byte scale. Supports Log stash , Kibana etc ;
Amazon Elasticsearch Service (Amazon ES) is a managed service that makes it easy to deploy, operate, and scale Elasticsearch clusters in the AWS Cloud.
Elasticsearch is a popular open-source search and analytics engine for use cases such as log analytics, real-time application monitoring, and click stream analysis.
With Amazon ES, you get direct access to the Elasticsearch APIs; existing code and applications work seamlessly with the service.

7. AMAZON EMR : https://www.youtube.com/watch?v=QuwaBOESGiU
APACHE SPARK - APACHE HADOOP - CLUSTERS - SPARK  --> Maintaining Hadoop and Spark is costly so EMR does this for  us .
Amazon EMR is a managed cluster platform that simplifies running big data frameworks, such as Apache Hadoop and Apache Spark, on AWS to process and analyze vast amounts of data.
By using these frameworks and related open-source projects, such as Apache Hive and Apache Pig, you can process data for analytics purposes and business intelligence workloads.
The central component of Amazon EMR is the cluster. A cluster is a collection of Amazon Elastic Compute Cloud (Amazon EC2) instances.
Each instance in the cluster is called a node. Each node has a role within the cluster, referred to as the node type.
Amazon EMR also installs different software components on each node type, giving each node a role in a distributed application like Apache Hadoop.
Master node - Core Node - Task Node


8. AWS GLUE : https://www.youtube.com/watch?v=oAxvd547kMU&t=28s
AWS Glue is a fully managed ETL (extract, transform, and load) service that makes it simple and cost-effective to categorize your data, clean it, enrich it, and move it reliably between various data stores and data streams.
It is used to combine data [ Prepare a data lake ] from various sources and clean , normalize and prepare data to one common syntax and save to S3.
It uses other AWS services to orchestrate your ETL jobs to build data warehouses and data lakes and generate output streams.
AWS Glue calls API operations to transform your data, create runtime logs, store your job logic, and create notifications to help you monitor your job runs.

9. AMAZON KINESIS : https://www.youtube.com/watch?v=MbEfiX4sMXc&t=19s      &&    https://www.youtube.com/watch?v=07iZOEl0knc&t=29s
Save data - logs - audio - video -> live stream -> analyze -> take action
    Kinesis Streams -> low latency streaming of data / video
    Kinesis Analytics -> perform real time analytics using SQL
    Kinesis Firehose : Load streams into S3, Elastic Search etc;

Amazon Kinesis Video Streams is a fully managed AWS service that you can use to stream live video from devices to the AWS Cloud, or build applications for real-time video processing or batch-oriented video analytics.
Kinesis Video Streams isn't just storage for video data. You can use it to watch your video streams in real time as they are received in the cloud.
You can use Kinesis Video Streams to capture massive amounts of live video data from millions of sources, including smartphones, security cameras, webcams, cameras embedded in cars, drones, andother sources.
You can also send non-video time-serialized data such as audio data, thermal imagery, depth data, RADAR data, and use kinesis to process these at a later Time.
you can stream video from a computer's webcam using the GStreamer  library, or from a camera on your network using RTSP.
You can also configure your Kinesis video stream to durably store media data for the specified retention period.
Kinesis Video Streams automatically stores this data and encrypts it at rest.
--> logs from mobile applications - logs from e-commers purchases - information from social media - satellite data - audio - video - spy cams - security cams etc;

10. AWS LAKE FORMATION :
The data lake is your persistent data that is stored in Amazon S3 and managed by Lake Formation using a Data Catalog.
The Data Catalog is your persistent metadata store. It is a managed service that lets you store, annotate, and share metadata in the AWS Cloud.
Metadata about data sources and targets is in the form of databases and tables.
AWS Lake Formation is a fully managed service that makes it easier for you to build, secure, and manage data lakes.
Lake Formation simplifies and automates many of the complex manual steps that are usually required to create data lakes.
These steps include collecting, cleansing, moving, and cataloging data, and securely making that data available for analytics and machine learning.
You point Lake Formation at your data sources, and Lake Formation crawls those sources and moves the data into your new Amazon Simple Storage Service (Amazon S3) data lake.
After the data is securely stored in the data lake, users can access the data through their choice of analytics services, including Amazon Athena, Amazon Redshift, and Amazon EMR.

11. AMAZON  MSK : Managed Streaming for Apache Kafka
Amazon Managed Streaming for Apache Kafka (Amazon MSK) is a fully managed service that enables you to build and run applications that use Apache Kafka to process streaming data.
Amazon MSK provides the control-plane operations, such as those for creating, updating, and deleting clusters.
It lets you use Apache Kafka data-plane operations, such as those for producing and consuming data. It runs open-source versions of Apache Kafka.

12. AMAZON QUICKSIGHT : https://www.youtube.com/watch?v=2V1bHRLRG-w&t=8s
Amazon QuickSight is a cloud-scale business intelligence (BI) service that you can use to deliver easy-to-understand insights to the people who you work with, wherever they are.
It connects to your data in the cloud and combines data from many different sources.
In a single data dashboard, QuickSight can include AWS data, third-party data, big data, spreadsheet data, SaaS data, B2B data, and more.
Securely push these dashboards to customers. Used to forecast future results and take decisions.

13. AMAZON REDSHIFT : https://www.youtube.com/watch?v=_qKm6o1zK3U&t=9s
Amazon Redshift is a fully managed, petabyte-scale data warehouse service in the cloud. An Amazon Redshift data warehouse is a collection of computing resources called nodes, which are organized into a group called a cluster.
Each cluster runs an Amazon Redshift engine and contains one or more databases.
Redshift lets you easily save the results of your queries back to your S3 data lake using open formats, like Apache Parquet, so that you can do additional analytics from other analytics services like Amazon EMR, Amazon Athena, and Amazon SageMaker.




##################################################################################################################
##################################################################################################################
COMPUTE  :
##################################################################################################################
##################################################################################################################

1. ELASTIC COMPUTE CLOUD :  < Has a separate detailed chapter >
Amazon Elastic Compute Cloud (Amazon EC2) provides scalable computing capacity in the Amazon Web Services (AWS) Cloud.
Using Amazon EC2 eliminates your need to invest in hardware up front, so you can develop and deploy applications faster.
You can use Amazon EC2 to launch as many or as few virtual servers as you need, configure security and networking, and manage storage.
Amazon EC2 enables you to scale up or down to handle changes in requirements or spikes in popularity, reducing your need to forecast traffic.

2. AWS BATCH : https://www.youtube.com/watch?v=j_iI1DzSi5g      &&   https://www.youtube.com/watch?v=T4aAWrGHmxQ&t=29s
AWS Batch enables you to run batch computing workloads on the AWS Cloud.
Batch computing is a common way for developers, scientists, and engineers to access large amounts of compute resources.
AWS Batch removes the undifferentiated heavy lifting of configuring and managing the required infrastructure.
As a fully managed service, AWS Batch helps you to run batch computing workloads of any scale.
AWS Batch automatically provisions compute resources and optimizes the workload distribution based on the quantity and scale of the workloads.
With AWS Batch, there is no need to install or manage batch computing software, which allows you to focus on analysing results and solving problems.
After a compute environment is up and associated with a job queue, you can define job definitions that specify which Docker container images to run your jobs.
  --> Break work into bathes and tell AWS which batch needs which resources are required and when to execute a batch

3. AWS ELASTI BEANSTALK :  < Has a separate detailed chapter > https://www.youtube.com/watch?v=SrwxAScdyT0&t=28s
--> Focus on your code --> Write it --> Upload it -->  AWS takes care of required resources .
With AWS Elastic Beanstalk, you can quickly deploy and manage applications in the AWS Cloud without worrying about the infrastructure that runs those applications.
AWS Elastic Beanstalk reduces management complexity without restricting choice or control.
You simply upload your application, and AWS Elastic Beanstalk automatically handles the details of capacity provisioning, load balancing, scaling, and application health monitoring.

4. AMAZON EC2 IMAGE BUILDER : https://www.youtube.com/watch?v=K5d2VdByohs&t=151s
It's a fully-managed AWS service that makes it easier to automate the creation, management, and deployment of customized, secure, and up-to-date “golden” server images that are pre-installed and pre-configured with software and settings to meet specific IT standards.
Crete a custom image based on required parameters and share the AMI across accounts or for instances in your account .

5. AWS LAMBDA :  < Has a separate detailed chapter >
--> Do something in response to events such as an api call / SNS / a data change in S3 /etc ;
With AWS Lambda, you can run code without provisioning or managing servers.
You pay only for the compute time that you consume—there’s no charge when your code isn’t running.
You can run code for virtually any type of application or backend service—all with zero administration.
Just upload your code and Lambda takes care of everything required to run and scale your code with high availability.
You can set up your code to automatically trigger from other AWS services or call it directly from any web or mobile app.

6. AWS LAUNCH WIZARD :
--> Used by enterprise applications .
AWS Launch Wizard reduces the time it takes to deploy application and domain-controller solutions to the cloud by providing easy step-by-step guidance.
You input your application or domain controller requirements, and AWS Launch Wizard identifies the right AWS resources to deploy and run your solution.
AWS Launch Wizard provides an estimated cost of deployment, and gives you the ability to modify your resources and instantly view the updated cost assessment.
When you approve, AWS Launch Wizard provisions and configures the selected resources in a few hours to create fully-functioning, production-ready applications or domain controllers. It also creates custom AWS CloudFormation templates, which can be reused and customized for subsequent deployments.

7. AMAZON LIGHT SAIL : https://www.youtube.com/watch?v=wzhTAwRbdXw
--> Deploying a word press site etc ;
Amazon Lightsail helps developers get started using AWS to build websites or web applications.
It includes the features that you need to launch your project: instances (virtual private servers), managed databases, SSD-based block storage, static IP addresses, load balancers, content delivery network (CDN) distributions, DNS management of registered domains, and snapshots (backups).
These features are all available for a low, predictable monthly price.

8. AWS OUTPOSTS : https://www.youtube.com/watch?v=ppG2FFB0mMQ&t=9s
--> Even though we can use cloud , some applications still need to be on premises for latency issues . Hence OUTPOSTS.
    So instead of changing applications to meet both cloud and on premises apps , we can use same API's to access resources in both.
    AWS Technician comes and set up all the resources needed to connect resources in cloud and resources in on-premises.
An Outpost is a pool of AWS compute and storage capacity deployed at a customer site.
AWS Outposts brings native AWS services, infrastructure, and operating models to virtually any data center, co-location space, or on-premises facility.
You can use the same services, tools, and partner solutions to develop for the cloud and on premises .

9. AWS PARALLEL CLUSTER : HPC // 1000's of CPU or GPU.   https://www.youtube.com/watch?v=r4RxT-IMtFY
AWS Parallel Cluster is an AWS supported open source cluster management tool that helps you to deploy and manage high performance computing (HPC) clusters in the AWS Cloud.
You can use AWSParallelCluster with batch schedulers, such as AWS Batch and Slurm.
AWS Parallel Cluster facilitates quick start proof of concept deployments and production deployments.
You can also build higher level workflows, such as a genomics portal that automates an entire DNA sequencing workflow, on top of AWSParallelCluster.

10. AWS SAM : [ SERVERLESS APPLICATION MODEL ]
The AWS Serverless Application Model (AWS SAM) is an open-source framework that you can use to buildserverless applications on AWS.
You can use AWS SAM to define your serverless applications.
It is an extension of CLOUD FORMATION

11. AWS SERVERLESS APPLICATION REPOSITORY :
The AWS Serverless Application Repository is a managed repository for serverless applications.
It enables teams, organizations, and individual developers to find, deploy, publish, share, store, and easily assemble serverless architectures.
Using SAM from above , we can publish and deploy applications to the public or to our team.

12. AWS WAVE LENGTH : To fully utilize 5G capabilities . // https://www.youtube.com/watch?v=EhMqwPqPzcY&t=6s
AWS Wavelength allows developers to build applications that deliver ultra-low latencies to mobile devices and end users.
Wavelength deploys standard AWS compute and storage services to the edge of telecommunication carriers' 5G networks.
Developers can extend an Amazon Virtual Private Cloud (VPC) to one or more Wavelength Zones.
Then use AWS resources like Amazon Elastic Compute Cloud (EC2) instances to run applications that require ultra-low latency and a connection to AWS services in the Region.
To help 5G --> Gaming experience - smart cities - robotics - autonomous driving cars



##################################################################################################################
##################################################################################################################
CONTAINERS   :
##################################################################################################################
##################################################################################################################

// DOCKER FILE  is a text document that contains commands that are used to assemble an image.
   We can use any command that call on the command line.
   Docker builds images automatically by reading the instructions from the Dockerfile.

    $ docker build  /path/to/a/Dockerfile  --> To build an image.

// DOCKER image is a read-only template with instructions for creating a Docker container.
   A docker image is described in text file called a Dockerfile, which has a simple, well-defined syntax.

// Docker container is a running instance of an image.
   You can use Command Line Interface (CLI) commands to run, start, stop, move, or delete a container.
   You can also provide configuration for the network and environment variables.
   Docker container is an isolated and secure application platform, but it can share and access to resources running in a different host or container.

// DOCKER packages software into standardized units called CONTAINERS .
   They have everything your software needs to run including Libraries , system tools , code and run time .
   It lets you quickly deploy and scale applications into any environment and know your code will run .

    $ docker run hello-world   // $ docker run <image> --> To create a container from an image and run it.
    https://www.javatpoint.com/docker-java-example

1. AMAZON ECR :  [ Amazon Elastic Container Registry ] // https://www.youtube.com/watch?v=H73uX0TOX9g&t=2s
It is a fully managed Docker container registry that makes it easy for developers to store, manage, and deploy DOCKER CONTAINER IMAGES.
Supports private container image repositories with resource-based permissions using AWS IAM. This is so that specified users or Amazon EC2 instances can access your container repositories and images.
Sports public container image repositories as well.
A repository is where you store your Docker or Open Container Initiative (OCI) images in Amazon ECR.
Each time you push or pull an image from Amazon ECR, you specify the repository and the registry location which informs where to push the image to or where to pull it from.

// Once an image is registered in the ECR , we can run that image as a container in the ECS service. -- ECS is a logical group of EC2 instances.
   ECS CLUSTER is a logical group of EC2 instances that you can place containers into .
   Ec2 instances can be of different configurations , types , and in diff. AZ's

2.AMAZON ECS : [ Amazon Elastic Container Service ] // https://www.youtube.com/watch?v=zBqjh61QcB4&t=4s && https://www.youtube.com/watch?v=eq4wL2MiNqo
Amazon Elastic Container Service (Amazon ECS) is a highly scalable, fast container management service that makes it easy to run, stop, and manage containers on a cluster.
Amazon ECS enables you to launch and stop your container-based applications by using simple API calls.
You can create Amazon ECS clusters within a new or existing VPC.
After a cluster is up and running, you can create task definitions that define which container images run across your clusters.
Your task definitions are used to run tasks or create services. Container images are stored in and pulled from container registries, for example, the Amazon Elastic Container Registry.

To prepare your application to run on Amazon ECS, you must create a task definition.
The task definition is a text file (in JSON format) that describes one or more containers (up to a maximum of ten) that form your application.
The task definition can be thought of as a blueprint for your application. It specifies various parameters for your application.
For example, these parameters can be used to indicate which containers should be used, which ports should be opened for your application, and what data volumes should be used with the containers in the task.
The specific parameters available for your task definition depend on the needs of your specific application.

A task is the instantiation of a task definition within a cluster.
After you have created a task definition for your application within Amazon ECS, you can specify the number of tasks to run on your cluster.
The Amazon ECS task scheduler is responsible for placing tasks within your cluster. There are several different scheduling options available.

// ECS AGENT manages the state of containers on an EC2 instance.
   Manages how ECS communicates with the docker daemon on the ec2.
   Is present on every ec2 instance.
   Is included with an ECS-Optimised Amazon Machine Image - AMI

// KUBERNETIS also known as K8s, is an open-source system for automating deployment, scaling, and management of containerized applications.
   It groups containers that make up an application into logical units for easy management and discovery.

3. AMAZON EKS : [ Amazon Elastic Kubernetes Service  ]
It s a managed service that you can use to run Kubernetes on AWS without needing to install, operate, and maintain your own Kubernetes control plane or nodes.
Kubernetes is an open-source system for automating the deployment, scaling, and management of containerized applications.
Amazon EKS automatically detects and replaces unhealthy control plane instances, and it provides automated version upgrades and patching for them.
Amazon EKS runs up-to-date versions of the open-source Kubernetes software, so you can use all of the existing plugins and tooling from the Kubernetes community.

4. AWS FARGATE :
AWS Fargate is a serverless compute engine for containers that works with both Amazon Elastic Container Service (ECS) and Amazon Elastic Kubernetes Service (EKS).
Fargate makes it easy for you to focus on building your applications.
Fargate removes the need to provision and manage servers, lets you specify and pay for resources per application, and improves security through application isolation by design.
Fargate allocates the right amount of compute, eliminating the need to choose instances and scale cluster capacity.
You only pay for the resources required to run your containers, so there is no over-provisioning and paying for additional servers.
Fargate runs each task or pod in its own kernel providing the tasks and pods their own isolated compute environment.
This enables your application to have workload isolation and improved security by design.
This is why customers such as Vanguard, Accenture, Foursquare, and Ancestry have chosen to run their mission critical applications on Fargate.

5. AWS APP2CONTAINER :
It is a command-line tool for modernizing .NET and Java applications into containerized applications.
Using A2C simplifies your migration tasks by performing inventory and analysis of your existing applications, creating Docker containers that include your application dependencies, and generating deployment templates based on AWS best practices with known values filled in for you.
After you have reviewed your templates, A2C helps you register your containers to Amazon ECR, deploy to Amazon ECS or Amazon EKS, and build CI/CD pipelines using AWS CodeStar.
It is also a command line tool to help you lift and shift applications that run in your on-premises data centers or on virtual machines, so that they run in containers that are managed by Amazon ECS or Amazon EKS. Thus providing a step towards modernization .



##################################################################################################################
##################################################################################################################
STORAGE   :
##################################################################################################################
##################################################################################################################

1. AMAZON SIMPLE STORAGE SERVICE - S3 :  < Has a separate detailed chapter >
--> Has 11 9's of durability . 99.999999999
It is storage for the internet. You can use Amazon S3 to store and retrieve any amount of data at any time, from anywhere on the web.
You can accomplish these tasks using the simple and intuitive web interface of the AWS Management Console.
A bucket is a container for objects stored in Amazon S3. Every object is contained in a bucket.

2. AWS BACKUP : https://www.youtube.com/watch?v=QDiXzFx2iMU&t=17s
AWS Backup is a fully managed backup service that makes it easy to centralize and automate the backup of data across AWS services in the cloud as well as on premises [ uses AWS STORAGE GATEWAY for on premisis].
Using AWS Backup, you can configure backup policies and monitor backup activity for your AWS resources in one place.
AWS Backup automates and consolidates backup tasks that were previously performed service-by-service, and removes the need to create custom scripts and manual processes.
It provides a fully managed backup service and a policy-based backup solution .
These features include Amazon Elastic Block Store (Amazon EBS) snapshots, Amazon Relational Database Service (Amazon RDS)snapshots, Amazon DynamoDB backups, AWS Storage Gateway snapshots, and others.
AWS Backup implements its backup features using the existing capabilities of these AWS services.
You can write backup PLANS to tell when to back up and when to move backup to Glacier etc;

3. AMAZON ELASTIC BLOCK STORE - EBS :  < Has a separate detailed chapter > // https://www.youtube.com/watch?v=77qLAl-lRpo
High performance and low latency block storage - has 4 types.
Amazon Elastic Block Store (Amazon EBS) is a web service that provides block level storage volumes for use with EC2 instances.
EBS volumes are highly available and reliable storage volumes that can be attached to any running instance and used like a hard drive.

4. AMAZON ELASTIC FILE SYSTEM - EFS : -> Its a Cloud File  https://www.youtube.com/watch?v=6ZIPBC78U0s&t=8s
It provides file storage for your Amazon EC2 instances.
With Amazon EFS, you can create a file system, mount the file system on your EC2 instances, and then read and write data from your EC2 instances to and from your file system.
It provides a simple, scalable, fully managed elastic NFS filesystem for use with AWS Cloud services and on-premises resources.
Amazon EFS has a simple web services interface that allows you to create and configure file systems quickly and easily.
Multiple Amazon EC2instances can access an Amazon EFS file system at the same time, providing a common data source for workloads and applications running on more than one instance or server.
Using Amazon EFS with Microsoft Windows–based Amazon EC2 instances is not supported.

                        EC2-a --
                        EC2-b   | <---------->  |||||||||  -> EFS
                        EC2-c --       NFS
            So multiple instances can access NFS at the same time using a standard NFSv4 Protocol .
            Once EFS is create d, we can mount EC2 instances on EFS.
            Useful when application runs on multiple EC2.

Whenyou first create your file system, there is only one root directory at /.
By default, only the root user (UID0) has read-write-execute permissions. For other users to modify the file system, the root user must explicitly grant them access.
You use EFS access points to provision directories that are writable from a specific application

// EFS for Linux
// FSx for Windows

5. AMAZON FSx : https://www.youtube.com/watch?v=4v08-CzjH1U
Amazon FSx provides fully managed third-party file systems with the native compatibility and feature sets for workloads such as Microsoft Windows–based storage, high-performance computing, machine learning, and electronic design automation.
Amazon FSx supports two file system types: Lustre and Windows File Server.
With file storage on Amazon FSx, the code, applications, and tools that Windows developers and administrators use today can continue to work unchanged.
Windows applications and workloads ideal for Amazon FSx include business applications, home directories, web serving, content management, data analytics, software build setups, and media processing workloads.

6. AMAZON SNOW FAMILY : < Has a separate detailed chapter - Storage_s3 >
 -> Snow cone        - https://www.youtube.com/watch?v=X_8LM7E_hiE      -- 8 TB storage   --> 1 SMALL BOX [ uses opshub application for data transfer ]
    Snow Ball        - https://www.youtube.com/watch?v=9uc2DSZ1wL8&t=9s -- 80 TB storage  --> 1 SUITCASE
    Snow Ball edge   - https://www.youtube.com/watch?v=bxSD1Nha2k8      -- 100 TB storage --> 1 SUITCASE  [ Embedded Ec2 - Lambda for running jobs in remote areas like ships , hills for an immediate results. ]
    Snow Mobile      - https://www.youtube.com/watch?v=8vQmTZTq7nw      -- 1 Exabyte      --> 1 Truck

   1 Exabyte = 1000 petabytes
   Snow ball can be connected in series to have a Petabyte's storage  = 1 million GB = 1024 Terabytes

7. AMAZON STORAGE GATEWAY : < Has a separate detailed chapter - Storage_s3 > -- https://www.youtube.com/watch?v=DPyc0q4MYsM -- https://www.youtube.com/watch?v=Spzdj1NUJbA -- https://www.youtube.com/watch?v=2I4CKdNESoQ&t=8s --
--> HYBRID CLOUD STORAGE SERVICE WITH A LOCAL CACHE . -- FILE GATEWAY -- VOLUME GATEWAY -- TAPE GATEWAY .
AWS Storage Gateway is a service that connects an on-premises software appliance with cloud-based storage to provide seamless and secure integration between your on-premises IT environment and the AWS storage infrastructure in the cloud.
Securely takes data from on premises data centre and uploads in S3 using SSL. We can get back data when ever we need it , for example if a hard disk crashed in on-premisi then we need to restore it back .
Storage gateway keeps regular backups from S3 so that it can restore to on-premises when needed.

The above process is needed until you are ready to move completely to cloud .
Slowly we can also launch an EC2 and attach an EBS with data from S3 and migrate to cloud directly.



##################################################################################################################
##################################################################################################################
DATABASE   :
##################################################################################################################
##################################################################################################################

DATABASE TYPE     USE CASE                                                      AWS SERVICE
================================================================================================================================================
Relational        Traditional applications, ERP, CRM, e-commerce                Amazon Aurora - Amazon RDS -  Amazon Redshift
================================================================================================================================================
Key-value         High-traffic web apps, e-commerce systems,                    Amazon DynamoDB
                  gaming applications
================================================================================================================================================
In-memory         Caching, session management, gaming leader boards,            Amazon ElastiCache for Memcached -  Amazon ElastiCache for Redis
                  geospatial applications
================================================================================================================================================
Document          Content management, catalogs, user profiles                   Amazon DocumentDB (with MongoDB compatibility)
================================================================================================================================================
Wide column       High scale industrial apps for equipment maintenance,         Amazon Keyspaces (for Apache Cassandra)
                  fleet management, and route optimization
================================================================================================================================================
Graph             Fraud detection, social networking, recommendation engines    Amazon Neptune
================================================================================================================================================
Time series       IoT applications, DevOps, industrial telemetry                Amazon Timestream
================================================================================================================================================
Ledger            Systems of record, supply chain, registrations,               Amazon QLDB
                  banking transactions
================================================================================================================================================

1. AMAZON AURORA : < Has a separate detailed chapter - Database >  https://www.youtube.com/watch?v=eMzCI7S1P9M&t=19s
Amazon Aurora (Aurora) is a fully managed relational database engine that's compatible with MySQL and PostgreSQL.
With some workloads, Aurora can deliver up to five times the throughput of MySQL and up to three times the throughput of PostgreSQL without requiring changes to most of your existing applications.
Aurora is part of the managed database service Amazon Relational Database Service (Amazon RDS).
Aurora management operations typically involve entire clusters of database servers that are synchronized through replication, instead of individual database instances.
The automatic clustering, replication, and storage allocation make it simple and cost-effective to set up, operate, and scale your largest MySQL and PostgreSQL deployments.

2. AMAZON DOCUMENT-DB :  https://www.youtube.com/watch?v=tkzDp9T8V-k
Amazon DocumentDB (with MongoDB compatibility) is a fast, reliable, and fully managed database service that makes it easy for you to set up, operate, and scale MongoDB-compatible databases.
Typically, in the application tier, data is represented as a JSON document because it is more intuitive for developers to think of their data model as a document.
The popularity of document databases has grown because they let you persist data in a database by using the same document model format that you use in your application code.
Document databases are a practical solution to online profiles in which different users provide different types of information. Using a document database, you can store eachuser's profile efficiently by storing only the attributes that are specific to each user.
Suppose that a user elects to add or remove information from their profile. In this case, their document could be easily replaced with an updated version that contains any recently added attributes anddata or omits any newly omitted attributes and data.

Document databases are used for storing semi structured data as a document—rather than normalizing data across multiple tables, each with a unique and fixed structure, as in a relational database.
Different types of documents can be stored in the same document database, thus meeting the requirement for processing similar data that is in different formats.

3. AMAZON DYNAMO-DB : < Has a separate detailed chapter - Database >   https://www.youtube.com/watch?v=sI-zciHAh-4
Amazon DynamoDB is a fully managed NoSQL database service that provides fast and predictable performance with seamless scalability.
You can use Amazon DynamoDB to create a database table that can store and retrieve any amount of data, and serve any level of request traffic.
Amazon DynamoDB automatically spreads the data and traffic for the table over a sufficient number of servers to handle the request capacity specified by the customer and the amount of data stored, while maintaining consistent and fast performance.
DynamoDB lets you offload the administrative burdens of operating and scaling a distributed database so that you don't have to worry about hardware provisioning, setup and configuration, replication, software patching, or cluster scaling.
You can create on-demand backups and enable point-in-time recovery for your Amazon DynamoDB tables.
Point-in-time recovery helps protect your tables from accidental write or delete operations.
With point-in-time recovery, you can restore a table to any point in time during the last 35 days
Allows you to delete expired items from tables automatically to help you reduce storage usage and the cost of storing data that is no longer relevant.

4. AMAZON ELASTIC CACHE FOR REDIS: -> https://www.youtube.com/watch?v=GoNsuTqeMto&t=18s
--> Caching data and providing it to users quickly rather than getting it from Disks. // example for applications in mobiel .
Amazon ElastiCache makes it easy to set up, manage, and scale distributed in-memory cache environments in the AWS Cloud.
It provides a high performance, resizable, and cost-effective in-memory cache, while removing complexity associated with deploying and managing a distributed cache environment.
ElastiCache works with both the Redis and Memcached engines.

5. AMAZON KEYSPACES [APAHE ASSANDRA] :

6. AMAZON NEPTUNE: Used for fraud detection // https://www.youtube.com/watch?v=Rl6UwE7kLio
Amazon Neptune is a fast, reliable, fully managed graph database service that makes it easy to build and run applications that work with highly connected datasets.
The core of Neptune is a purpose-built, high-performance graph database engine that is optimized for storing billions of relationships and querying the graph with milliseconds latency.
Neptune supports the popular graph query languages Apache TinkerPop Gremlin and W3C’s SPARQL, allowing you to build queries that efficiently navigate highly connected datasets.
Neptune powers graph use cases such as recommendation engines, fraud detection, knowledge graphs, drug discovery, and network security.

Each node represents an entity (a person, place, thing, category or other piece of data), and each relationship represents how two nodes are associated.
This general-purpose structure allows you to model all kinds of scenarios – from a system of roads, to a network of devices, to a population’s medical history or anything else defined by relationships.

7. AMAZON QUANTUM LEDGER DATABASE - QLDB :  https://www.youtube.com/watch?v=jcZ_rsLJrqk
Amazon Quantum Ledger Database (Amazon QLDB) is a fully managed ledger database that provides a transparent, immutable, and cryptographically verifiable transaction log owned by a central trusted authority.
You can use Amazon QLDB to track all application data changes, and maintain a complete and verifiable history of changes over time.
Ex : Banks data -> keeps full history of changes. -- Immutable and Transparent.

8. AMAZON RDS : < Has a separate detailed chapter - Database >   https://www.youtube.com/watch?v=eMzCI7S1P9M&t=19s
It is a web service that makes it easier to set up, operate, and scale a relational database in the cloud.
It provides cost-efficient, resizable capacity for an industry-standard relational database and manages common database administration tasks.

  • When you buy a server, you get CPU, memory, storage, and IOPS, all bundled together. With AmazonRDS, these are split apart so that you can scale them independently.
    If you need more CPU, less IOPS, or more storage, you can easily allocate them.
  • Amazon RDS manages backups, software patching, automatic failure detection, and recovery.
  • To deliver a managed service experience, Amazon RDS doesn't provide shell access to DB instances. It also restricts access to certain system procedures and tables that require advanced privileges.
  • You can have automated backups performed when you need them, or manually create your own backup snapshot. You can use these backups to restore a database. The Amazon RDS restore processworks reliably and efficiently.
  • You can use the database products you are already familiar with: MySQL, MariaDB, PostgreSQL, Oracle, Microsoft SQL Server.
  • In addition to the security in your database package, you can help control who can access your RDS databases by using AWS Identity and Access Management (IAM) to define users and permissions.

9. AMAZON REDSHIFT :  https://www.youtube.com/watch?v=_qKm6o1zK3U&t=22s
For data from BIG-DATA , enterprise applications //  used mainly for data analytics by quickly mining data .
Amazon Redshift is a fast, fully managed, petabyte-scale data warehouse service that makes it simple and cost-effective to efficiently analyze all your data using your existing business intelligence tools.
It is optimized for datasets ranging from a few hundred gigabytes to a petabyte or more and costs less than $1,000 per terabyte per year, a tenth the cost of most traditional data warehousing solutions.

10. AMAZON TIMESTREAM :
With Amazon Timestream, you can easily store and analyze sensor data for IoT applications, metrics for DevOps use cases, and telemetry for application monitoring scenarios such as clickstream data analysis.
Used to store and analyze trillions of time series data points per day.
Timestream saves you time and cost in managing the lifecycle of time series data by keeping recent data in memory and moving historical data to a cost optimized storage tier based upon user defined policies.
Timestream’s purpose-built query engine lets you access and analyze recent and historical data together, without having to specify its location.
Examples of a growing list of use cases for Timestream include:
--> Monitoring metrics to improve the performance and availability of your applications.
--> Storage and analysis of industrial telemetry to streamline equipment management and maintenance.
--> Tracking user interaction with an application over time.
--> Storage and analysis of IoT sensor data



##################################################################################################################
##################################################################################################################
SECURITY - IDENTITY - COMPLIANCE   :
##################################################################################################################
##################################################################################################################

1. AWS IDENTITY & ACCESS MANAGEMENT : < Has a separate detailed chapter - IAM >  https://www.youtube.com/watch?v=Ul6FW4UANGc
It is a web service for securely controlling access to AWS services. You use IAM to control who is authenticated (signed in) and authorized (has permissions) to use resources.
With IAM, you can centrally manage users, security credentials such as access keys, and permissions that control which AWS resources users and applications can access.
You can use IAM to control all the available ~180 services available in AWS. And using cloudtrail you can keep track of the activities .
Users can focus on developing applications and IAM handles the security. And it is free of cost .

2. AWS ARTIFACT : https://www.youtube.com/watch?v=ILEoLqpbfXM&t=5s
No cost, self-service portal for on-demand access to AWS compliance reports and for entering into select online agreements.
Use AWS Artifact reports to access several compliance reports from third-party auditors who have tested and verified our compliance with a variety of global, regional, and industry specific security standards and regulations.
Use AWS Artifact agreements to review, accept, and terminate agreements with AWS for an individual account, and for all accounts that are a part of your organization in AWS Organizations.

AWS Artifact is a web service that enables you to download AWS security and compliance documents such as ISO certifications, , Payment Card Industry (PCI), and Service Organization Control (SOC) reports.
You can submit the security and compliance documents (also known as audit artifacts) to your auditors or regulators to demonstrate the security and compliance of the AWS infrastructure and services that you use.
You can also use these documents as guidelines to evaluate your own cloud architecture and assess the effectiveness of your company's internal controls.
AWS Artifact provides documents about AWS only. You can also use AWS Artifact to review, accept, and track the status of AWS agreements such as theBusiness Associate Addendum (BAA).
AWS customers are responsible for developing or obtaining documents that demonstrate the security and compliance of their companies.

3. AWS AUDIT MANAGER :
When it is time for an audit, AWS Audit Manager helps you manage stakeholder reviews of your controls, which means you can build audit-ready reports with much less manual effort.
AWS Audit Manager helps you continuously audit your AWS usage to simplify how you manage risk and compliance with regulations and industry standards.
AWS Audit Manager makes it easier to evaluate whether your policies, procedures, and activities—also known as controls—are operating as intended.
The service offers prebuilt frameworks with controls that are mapped to well-known industry standards and regulations, full customization of frameworks and controls, and automated collection and organization of evidence as designed by each control requirement.

4. AWS COGNITO :
Amazon Cognito handles user authentication and authorization for your web and mobile apps.
With user pools, you can easily and securely add sign-up and sign-in functionality to your apps.
Your users can sign in directly with a user name and password, or through a third party such asFacebook, Amazon, Google or Apple.
The two main components of Amazon Cognito are user pools and identity pools.
  --> User pools are user directories that provide sign-up and sign-in options for your app users.
  --> Identity pools enable you to grant your users access to other AWS services.
  You can use identity pools and user pools separately or together.

5. AWS DIRETORY SERVICE : -> To use Microsoft Active Directory - MAD // https://www.youtube.com/watch?v=XNTsmRe8k7Q&t=9s
--> MAD is  used to manage computers and other devices on a network.
--> It is a primary feature of Windows Server, an operating system that runs both local and Internet-based servers.
--> So a similar functionality is provided by AWS Directory service , used to handle unlimited number of servers and their credentials and let users login through single sign on with existing corporate credentials on premisis.
--> As an admin you  can manage both windows and linux instances and no more needed to manage credentials to individual servers.
AWS Directory Service provides multiple ways to set up and run Microsoft Active Directory with other AWS services such as Amazon EC2, Amazon RDS for SQL Server, Amazon FSx for Windows File Server, and AWS Single-Sign On.
AWS Directory Service for Microsoft Active Directory, also known as AWS Managed Microsoft AD, enables your directory-aware workloads and AWS resources to use a managed Active Directory in the AWS Cloud.

 // The below three services AWS WAF - AWS SHIELD - AWS FIREWALL MANAGER are interlinked. So which one to use ??
    --> It all starts with AWS WAF. You can automate and then simplify AWS WAF management using AWSFirewall Manager.
    --> Shield Advanced adds additional features on top of AWS WAF, such as dedicated support from the DDoS Response Team (DRT) and advanced reporting.
    --> To use the services of the DRT, you must be subscribed to the Business Support plan or theEnterprise Support plan.
    --> If you want granular control over the protection that is added to your resources, AWS WAF alone is the right choice.
    --> If you want to use AWS WAF across accounts, accelerate your AWS WAF configuration, or automate protection of new resources, use Firewall Manager with AWS WAF.
    --> Finally, if you own high visibility websites or are otherwise prone to frequent DDoS attacks, you should consider purchasing the additional features that Shield Advanced provides.

6. AWS WEB APPLICATION FIREWALL - WAF :
A web application firewall (WAF) helps protect your web applications from common web exploits that could affect application availability, compromise security, or consume excessive resources.
AWS WAF is a web application firewall that lets you monitor the HTTP and HTTPS requests that are forwarded to an Amazon Cloud Front distribution, an Amazon API Gateway REST API, an Application Load Balancer, or an AWS AppSync GraphQL API.
AWS WAF also lets you control access to your content.
Based on conditions that you specify, such as the IP addresses that requests originate from or the values of query strings, Amazon Cloud Front, Amazon API Gateway, Application Load Balancer, or AWS AppSync responds to requests either with the requested content or with an HTTP 403 status code (Forbidden).
You also can configure CloudFront to return a custom error page when a request is blocked.
You can use AWS WAF web access control lists (web ACLs) to help minimize the effects of a distributed denial of service (DDoS) attack
You can also use AWS WAF to protect your applications that are hosted in Amazon Elastic Container Service (Amazon ECS) containers.
AWS WAF is available today anywhere CloudFront is available. Pricing is $5 per web ACL, $1 per rule, and $0.60 per million HTTP requests.

You can chose to :
  • Allow all requests except the ones that you specify
  • Block all requests except the ones that you specify
  • Count the requests that match the properties that you specify
Setting up WAF :
  • Create a web access control list (web ACL) using the wizard in the AWS WAF console.
  • Choose the AWS resources that you want AWS WAF to inspect web requests for.
  • Add the rules and rule groups that you want to use to filter web requests.
    --> A string match rule statement identifies strings that you want AWS WAF to search for in a request, such as a specified value in a header or in a query string.
    --> Attackers attack with a particular string and when you write a string matching rule , those requests are blocked .
  • Specify a default action for the web ACL, either block or allow.

7. AWS SHIELD : --> Mainly for the DDOS attacks.
You can use AWS WAF web access control lists (web ACLs) to help minimize the effects of a distributed denial of service (DDoS) attack.
For additional protection against DDoS attacks, AWS also provides AWS Shield Standard and AWS Shield Advanced.
AWS Shield Standard is automatically included at no extra cost beyond what you already pay for AWS WAF and your other AWS services.
AWS Shield Advanced provides expanded DDoS attack protection for your Amazon EC2 instances, Elastic Load Balancing load balancers, CloudFront distributions, Route 53 hosted zones, and AWS Global Accelerator accelerators.
AWS Shield Advanced incurs additional charges. You can add protection for any of the following resource types:
  • Amazon CloudFront distributions
  • Amazon Route 53 hosted zones
  • AWS Global Accelerator accelerators
  • Application load balancers
  • Elastic Load Balancing (ELB) load balancers
  • Amazon Elastic Compute Cloud (Amazon EC2) Elastic IP addresses

8. AWS FIREWALL MANAGER :  --> Used to manage the above WAF and SHIELD .
AWS Firewall Manager simplifies your administration and maintenance tasks across multiple accounts and resources for AWS WAF, AWS Shield Advanced, Amazon VPC security groups, and AWS NetworkFirewall.
With Firewall Manager, you set up your AWS WAF firewall rules, Shield Advanced protections, Amazon VPC security groups, and Network Firewall firewalls just once.
The service automatically applies your rules across your accounts and resources, even as you add new resources.

  • Helps to protect resources across accounts
  • Helps to protect all resources of a particular type, such as all Amazon CloudFront distributions
  • Helps to protect all resources with specific tags
  • Automatically adds protection to resources that are added to your account
  • Allows you to subscribe all member accounts in an AWS Organizations organization to AWS Shield Advanced, and automatically subscribes new in-scope accounts that join the organization
  • Allows you to apply security group rules to all member accounts or specific subsets of accounts in an AWS Organizations organization, and automatically applies the rules to new in-scope accounts that join the organization
  • Lets you use your own rules, or purchase managed rules from AWS Marketplace

// Once configured, Firewall Manager will automatically create a Network Firewall with the sets of rules, deploying an endpoint in a dedicated subnet for every availability zone containing public subnets, in the accounts and VPCs you specify.
   At the same time, any changes to the centrally configured set of rules are automatically updated downstream on the deployed Network Firewalls.
   Network firewall manager manages all netwrok firewalls , vpc's , subnets etc;
   Network firewall manages a particular VPC . You can create one for each VPC .

9. AWS NETWORK FIREWALL : --> Supported by NETWORK FIREWALL MANAGER // https://aws.amazon.com/about-aws/whats-new/2020/11/aws-firewall-manager-supports-centralized-management-aws-network-firewall/
AWS Network Firewall is a stateful, managed, network firewall and intrusion detection and prevention service for your virtual private cloud (VPC).
With Network Firewall, you can filter traffic at the perimeter of your VPC. This includes filtering traffic going to and coming from an internet gateway, NAT gateway, or over VPN or AWS Direct Connect.
  • Pass traffic through only from known AWS service domains or IP address endpoints, such as AmazonS3.
  • Use custom lists of known bad domains to limit the types of domain names that your applications can access.
  • Perform deep packet inspection on traffic entering or leaving your VPC.
  • Use stateful protocol detection to filter protocols like HTTPS, independent of the port used.
To enable Network Firewall for your VPC, you perform steps in both Amazon VPC and in Network Firewall.
The firewall protects the subnets within your VPC by filtering traffic going between the subnets and locations outside of your VPC.
To enable the firewall's protection, you modify your Amazon VPC route tables to send your networktraffic through the Network Firewall firewall endpoint.


10. AWS CLOUD DIRECTORY :  https://www.youtube.com/watch?v=OhaGbCeNLs0&t=27s
--> Graph-based directory -- Different from AWS DIRECTORY SERVICE above.
--> Cloud Directory is NOT a directory service for IT Administrators who want to manage or migrate their directory infrastructure and do authentication and authorization.
--> The concept is to generate a conceptual relationship between the objects .
Amazon Cloud Directory is a cloud-native directory that can store hundreds of millions of application-specific objects with multiple relationships and schemas.
Use Cloud Directory when you need a cloud-scale directory to share and control access to hierarchical data between your applications.
With Cloud Directory, you can organize application data into multiple hierarchies to support many organizational pivots and relationships across directory information.
For example, a directory of users may provide a hierarchical view based on reporting structure, location, and project affiliation.
Similarly, a directory of devices may have multiple hierarchical views based on its manufacturer, current owner, and physical location.

/// The below AWS DETECTIVE - AWS GUARD DUTY - AWS INSPECTOR - AWS MACIE are similar in concepts.
    --> AWS DETECTIVE is mainly for the security findings in login attempts , API calls.
    --> AWS GUARD DUTY does DETETIVE duty plus reports compromised EC2 instanes , compromised s3 instanes .
    --> AWS INSPECTOR does an vulnerability assessment based on common security standards and vulnerability definitions updated frequently by AWS .
    --> AWS MACIE is for protecting personal and financial data stored  in S3 across your organization.
    --> All the services are free for 30 days .Pay from the next month!! During the 30-day free trial period, the cost estimation projects what your estimated costs will be after the free trial period.


11. AMAZON DETECTIVE :
Amazon Detective makes it easy to analyze, investigate, and quickly identify the root cause of security findings or suspicious activities.
Detective automatically collects log data from your AWS resources and uses machine learning, statistical analysis, and graph theory to help you visualize and conduct faster and more efficient security investigations.
Detective automatically extracts time-based events such as login attempts, API calls, and network traffic from AWS CloudTrail and Amazon VPC flow logs.
It also ingests findings detected by GuardDuty. You can explore this behavior graph to examine disparate actions such as failed logon attempts or suspicious API calls.
Free for 30days. Pay from the next month!! During the 30-day free trial period, the cost estimation projects what your estimated costs will be after the free trial period.

12. AWS GUARD DUTY :  https://www.youtube.com/watch?v=ocZjGirQT9A&t=28s
Amazon Guard Duty is a continuous security monitoring service.
Amazon Guard Duty can help to identify unexpected and potentially unauthorized or malicious activity in your AWS environment.
It a continuous security monitoring service that analyzes and processes the following Data sources : VPC Flow Logs, AWS CloudTrail management event logs, Cloudtrail S3 data event logs, and DNS logs.
It uses threat intelligence feeds, such as lists of malicious IP addresses and domains, and machine learning to identify unexpected and potentially unauthorized and malicious activity within your AWS environment.
This can include issues like escalations of privileges, uses of exposed credentials, or communication with malicious IP addresses, or domains.

    •   compromised EC2 instance
    •   Compromised S3 Bucket
    •   compromised AWS credentials

For example, GuardDuty can detect compromised EC2 instances serving malware or mining bitcoin.
It also monitors AWS account access behaviour for signs of compromise, such as unauthorized infrastructure deployments, like instances deployed in a Region that has never been used, or unusual API calls, like a password policy change to reduce password strength.
AWS DETECTIVE checks only security findings but guard duty can do additional stuff stated above .
Free for 30days. Pay from the next month!! During the 30-day free trial period, the cost estimation projects what your estimated costs will be after the free trial period.

13. AWS INSPECTOR : https://www.youtube.com/watch?v=xxvrmBPbNPs
Amazon Inspector is a security vulnerability assessment service that helps improve the security and compliance of your AWS resources.
It tests the network accessibility of your Amazon EC2 instances and the security state of your applications that run on those instances. Amazon Inspector assesses applications for exposure,vulnerabilities, and deviations from best practices.
Amazon Inspector also offers predefined software called an agent that you can optionally install in the operating system of the EC2 instances that you want to assess.
Amazon Inspector automatically assesses resources for vulnerabilities or deviations from best practices, and then produces a detailed list of security findings prioritized by level of severity.
Amazon Inspector includes a knowledge base of hundreds of rules mapped to common security standards and vulnerability definitions that are regularly updated by AWS security researchers.

14. AWS MACIE :
Amazon Macie is a fully managed data security and data privacy service that uses machine learning and pattern matching to discover, classify, and help you protect your sensitive data in Amazon S3.
Macie automates the discovery of sensitive data, such as personally identifiable information (PII) and financial data, to provide you with a better understanding of the data that your organization storesin Amazon Simple Storage Service (Amazon S3).
Macie also provides you with an inventory of your S3buckets, and it automatically evaluates and monitors those buckets for security and access control.
Within minutes, Macie can identify and report overly permissive or unencrypted buckets for your organization.

15. AWS RESOURCE ACCESS MANAGER - RAM : < Has a separate detailed chapter - Resource_groups_&_RAM >
AWS Resource Access Manager (AWS RAM) enables you to share your resources with any AWS account or organization in AWS Organizations.
Customers who operate multiple accounts can create resources centrally and use AWS RAM to share them with all of their accounts to reduce operational overhead.
AWS RAM is available at no additional charge.
You can share resources that you own by creating a resource share.
When you create a resource share, you specify a name, the resources to share, and the principals with whom to share.
Principals can be AWS accounts, organizational units, or an entire organization from AWS Organizations.
Your account retains full ownership of the resources that you share.

  • Reduces operational overhead
  • Provides security and consistency
  • Provides visibility and auditability

The following sections list the services that integrate with AWS RAM, and the resources that support sharing.
    • AWS App Mesh     • Amazon Aurora        • AWS Certificate Manager Private Certificate Authority
    • AWS Code Build   • Amazon EC2           • EC2 Image Builder
    • AWS Glue         • AWS License Manager  • AWS Network Firewall
    • AWS Outposts     • AWS Resource Groups  • Amazon Route 53
    • Amazon VPC

16. AWS RESOURCE GROUPS & TAG EDITOR : < Has a separate detailed chapter - Resource_groups_&_RAM >
AWS Resource Groups lets you organize AWS resources into groups, tag resources using virtually any criteria, and manage, monitor, and automate tasks on grouped resources.
A resource group is a collection of AWS resources that are all in the same AWS region, and that match criteria provided in a query.
Tags are words or phrases that act as metadata that you can use to identify and organize your AWS resources.
With Tag Editor, you build a query to find resources in one or more AWS Regions that are available for tagging.
You can choose up to 20 individual resource types, or build a query on All resource types.

17. AWS SECRETS MANAGER :
In the past, when you created a custom application to retrieve information from a database, you typically embedded the credentials, the secret, for accessing the database directly in the application.
When the time came to rotate the credentials, you had to do more than just create new credentials. You had to invest time to update the application to use the new credentials.
Then you distributed the updated application. If you had multiple applications with shared credentials and you missed updating one of them, the application failed.

AWS Secrets Manager helps you to securely encrypt, store, and retrieve credentials for your databases and other services.
This helps ensure the secret can't be compromised by someone examining your code, because the secret no longer exists in the code.
Instead of hardcoding credentials in your apps, you can make calls to Secrets Manager to retrieve your credentials whenever needed.
Secrets Manager helps you protect access to your IT resources and data by enabling you to rotate and manage access to your secrets.
You create roles and grant permissions to apps to use that role to access secret manager.

18. AWS SECURITY HUB :
AWS Security Hub provides you with a comprehensive view of the security state of your AWS resources against security industry standards and best practices.
Security Hub collects security data from across AWS accounts and services, and helps you analyze your security trends to identify and prioritize the security issues across your AWS environment.

SINGLE-SIGN-ON :
------------------
Security Assertion Markup Language (SAML) is an open standard that allows identity providers (IdP) to pass authorization credentials to service providers (SP).
What that jargon means is that you can use one set of credentials to log into many different websites.
Single Sign On uses SAML .

Single sign-on (SSO) is an authentication method that enables users to securely authenticate with multiple applications and websites by using just one set of credentials.
SSO works based upon a trust relationship set up between an application, known as the service provider, and an identity provider.
This trust relationship is often based upon a certificate that is exchanged between the identity provider and the service provider.
This certificate can be used to sign identity information that is being sent from the identity provider to the service provider so that the service provider knows it is coming from a trusted source.
In SSO, this identity data takes the form of tokens which contain identifying bits of information about the user like a user’s email address or a username.

The login flow usually looks like this:
  --> A user browses to the application or website they want access to, aka, the Service Provider.
  --> The Service Provider sends a token that contains some information about the user, like their email address, to the SSO system, aka, the Identity Provider, as part of a request to authenticate the user.
  --> The Identity Provider first checks to see whether the user has already been authenticated, in which case it will grant the user access to the Service Provider application and skip to step 5.
  --> If the user hasn’t logged in, they will be prompted to do so by providing the credentials required by the Identity Provider. This could simply be a username and password or it might include some other form of authentication like a One-Time Password (OTP).
  --> Once the Identity Provider validates the credentials provided, it will send a token back to the Service Provider confirming a successful authentication.
  --> This token is passed through the user’s browser to the Service Provider.
  --> The token that is received by the Service Provider is validated according to the trust relationship that was set up between the Service Provider and the Identity Provider during the initial configuration.
  --> The user is granted access to the Service Provider.

  The use of SSO is a very popular method of allowing access with just a single sign in. LDAP, on the other hand, is the protocol used in authentication of the SSO systems.
  The Acronym LDAP refers to Lightweight Directory Access Protocol.
  LDAP is an Identity repository.
  SAML ( used by SSO ) is an Identity standard that could use LDAP as the repository. Or it could use something else like Active directory.


19. AWS SINGLE SIGN-ON :
AWS Single Sign-On (AWS SSO) is a cloud-based service that simplifies managing SSO access to AWS accounts and business applications.
You can control SSO access and user permissions across all your AWS accounts in AWS Organizations.
You can also administer access to popular business applications and custom applications that support Security Assertion Markup Language (SAML) 2.0.
In addition, AWS SSO offers a user portal where your users can find all their assigned AWS accounts, business applications, and custom applications in one place.



#########################################################################################################################
#########################################################################################################################
CRYPTOGRAPHY AND PKI [ Public Key Infrastructure ]  SERVICES : // View chapter Cryptography_KMS_CMK for complete Details.
#########################################################################################################################
#########################################################################################################################

1. AWS CLOUD-HSM :
HARDWARE SECURITY MODULE is a computing device that performs cryptographic operations and provides secure storage for cryptographic keys.
AWS CloudHSM organizes HSMs in clusters, which are automatically synchronized collections of HSMs within a given Availability Zone (AZ).
If you want a managed service for creating and controlling encryption keys, but do not want or need tooperate your own HSM, consider using AWS Key Management Service.
When you use an HSM from AWS CloudHSM, you can perform a variety of cryptographic tasks:
    • Generate, store, import, export, and manage cryptographic keys, including symmetric keys and asymmetric key pairs.
    • Use symmetric and asymmetric algorithms to encrypt and decrypt data.
    • Use cryptographic hash functions to compute message digests and hash-based message authentication codes (HMACs).
    • Cryptographically sign data (including code signing) and verify signatures.
    • Generate cryptographically secure random data.

2. AWS KEY MANAGEMENT SERVICE :
It provides tools for generating master keys and other data keys.
AWS KMS also interacts with many other AWS services to encrypt their service-specific data.
It is an AWS service that makes it easy for you to create and control the encryption keys that are used to encrypt your data.
The customer master keys (CMKs) that you create in AWS KMS are protected by FIPS 140-2 validated cryptographic modules.
They never leave AWS KMS unencrypted. To use or manage your CMKs, you interact with AWS KMS.
AWS KMS is also integrated with AWS CloudTrail to deliver detailed logs of all cryptographic operations that use your CMKs and management operations that change their configuration.

  ****VIMP :
  AWS KMS protects the customer master keys that protect your data.
  In the classic scenario, you encrypt your data using data key A. But you need to protect data key A, so you encrypt data key A by using data key B.
  Now data key B is vulnerable, so you encrypt it by using data key C. And, so on.
  This encryption technique, which is called envelope encryption, always leaves one last encryption key unencrypted so you can decrypt your encryption keys and data.
  That last unencrypted (or plaintext) key is called a master key.

AWS KMS customer master keys (CMKs) are 256-bit Advanced Encryption Standard (AES) symmetric keys that are not exportable. They spend their entire lifecycle entirely within AWS KMS.
You can also create asymmetric RSA or elliptic curve (ECC) CMKs backed by asymmetric key pairs. The public key in each asymmetric CMK is exportable, but the private key remains within AWS KMS.
You can use your CMKs to encrypt small amounts of data (up to 4096 bytes). However, CMKs are typically used to generate, encrypt, and decrypt the data keys that encrypt your data.
Unlike CMKs, data keys can encrypt data of any size and format, including streamed data.

AWS KMS does not store or manage data keys, and you cannot use KMS to encrypt or decrypt with data keys. To use data keys to encrypt and decrypt, use the AWS Encryption SDK.
AWS KMS CMKs are backed by FIPS-validated hardware service modules (HSMs) that KMS manages. Tomanage your own HSMs, use AWS CloudHSM.

3. AWS ENCRYPTION SDK :
The AWS Encryption SDK is a client-side encryption library to help you implement best-practice encryption and decryption in any application even if you're not a cryptography expert.
The AWS Encryption SDK works on all types of data. Every successful call to encrypt returns a single portable, formatted encrypted message that contains metadata and the message ciphertext.
The AWS Encryption SDK is developed as an open source project.
It is available in multiple programming languages, including a command line interface that is supported on Linux, macOS, and Windows.
All implementations are interoperable. For example, you can encrypt your data with the Java library and decrypt it with the Python library. Or you can encrypt data with the C library and decrypt it with the CLI.

4. AMAZON DYNAMO-DB ENCRYPTION CLIENT :
The Amazon DynamoDB Encryption Client is a client-side encryption library that helps you to protect your table data before you send it to Amazon DynamoDB.
Encrypting your sensitive data in transit and at rest helps ensure that your plaintext data isn’t available to any third party, including AWS.
It encrypts the attribute values in each table item using a unique encryption key.
It then signs the item to protect it against unauthorized changes, such as adding or deleting attributes or swapping encrypted values.
It also verifies and decrypts them when you retrieve them.
It is an open source project and is available in JAVA and python and is interoperable.

5. AMAZON S3 CLIENT SIDE ENCRYPTION :
  OPTION-1 : Using a CMK stored in AWS KMS :
        When uploading an object —
            Using the CMK ID, the client first sends a request to AWS KMS for a CMK that it can use to encrypt your object data.
            The client obtains a unique data key for each object that it uploads.
            AWS KMS returns two versions of a randomly generated data key:
              --> A plaintext version of the data key that the client uses to encrypt the object data.
              --> A cipher blob of the same data key that the client uploads to Amazon S3 as object metadata.

        When downloading an object —
        The client downloads the encrypted object from Amazon S3 along with the cipher blob version of the data key stored as object metadata.
        The client then sends the cipher blob to AWS KMS to get the plaintext version of the data key so that it can decrypt the object data

  OPTION-2 : Using a master key stored within your application
        When uploading an object —
            You provide a client-side master key to the Amazon S3 encryption client.
            The client uses the master key only to encrypt the data encryption key that it generates randomly.
              --> The Amazon S3 encryption client generates a one-time-use symmetric key (also known as a data encryption key or data key) locally.
                  It uses the data key to encrypt the data of a single Amazon S3 object. The client generates a separate data key for each object.
              --> The client encrypts the data encryption key using the master key that you provide.
                  The client uploads the encrypted data key and its material description as part of the object metadata.
                  The client uses the material description to determine which client-side master key to use for decryption.
              --> The client uploads the encrypted data to Amazon S3 and saves the encrypted data key as object metadata (x-amz-meta-x-amz-key) in Amazon S3.

        When downloading an object —
        The client downloads the encrypted object from Amazon S3.
        Using the material description from the object's metadata, the client determines which master key to use to decrypt the data key.
        The client uses that master key to decrypt the data key and then uses the data key to decrypt the object.

NOTE :
Your client-side master keys and your unencrypted data are never sent to AWS. It's important that you safely manage your encryption keys. If you lose them, you can't decrypt your data.
The client-side master key that you provide can be either a symmetric key or a public/private key pair. The following code examples show how to use each type of key.

5. AWS SECRET MANAGER : Documented in SECURITY-IDENTITY-COMPLIANCE
provides encryption and rotation of encrypted secrets used with AWS-supported databases.

To protect DynamoDB table items before you send them to DynamoDB, use the DynamoDB Encryption Client.
To protect Amazon S3 objects before you send them to an Amazon S3 bucket, use Amazon S3 client-side encryption. Amazon S3 also offers server-side encryption.
To protect all other types of data at their source, use the AWS Encryption SDK.

WHEN TO USE AWS-CLOUD HSM AND WHEN TO USE KMS :
Use AWS CloudHSM when you need to manage the HSMs that generate and store your encryption keys.
In AWS CloudHSM, you create and manage HSMs, including creating users and setting their permissions.
You also create the symmetric keys and asymmetric key pairs that the HSM stores.
If you need to secure your encryption keys in a service backed by FIPS-validated HSMs, but you do not need to manage the HSM, try AWS Key Management Service (p

6. AWS CERTIFICATE MANAGER - ACM :
It is used to generate, issue, and manage public and private SSL/TLS certificates for use with your AWS based websites and applications.
These public certificates verify the identity and authenticity of your web server and the ownership of your public keys.
In doing so, public certificates initiate a trusted, encrypted connection between you and your users.

7. AWS CERTIFICATE MANAGER PRIVATE CERTIFICATE AUTHORITY - ACM PCA:
is service is for enterprise customers building a public key infrastructure (PKI)inside the AWS cloud and intended for private use within an organization.
It is a managed private certificate authority (CA) service with which you can manage your CA infrastructure and your private certificates.
With private certificates you can authenticate resources inside an organization.
Private certificates allow entities like users, web servers, VPN users, internal API endpoints, and IoT devices to prove their identity and establish encrypted communications channels.

8. AWS SIGNER :
AWS Signer is a fully managed code-signing service to ensure the trust and integrity of your code.
Organizations validate code against a digital signature to confirm that the code is unaltered and from a trusted publisher.
With AWS Signer, your security administrators have a single place to define your signing environment, including what AWS Identity and Access Management (IAM) role can sign code and in what regions.
AWS Signer manages the code-signing certificate public and private keys and enables central management of the code-signing lifecycle.
  --> With Code Signing for AWS Lambda, you can ensure that only trusted code runs in your Lambda functions.
  --> With Code Signing for AWS IoT, you can sign code that you create for IoT devices supported by Amazon Free RTOS and AWS IoT device management.

9. AWS CRYPTO TOOLS :
Cryptography is hard to do safely and correctly.
The AWS Crypto Tools libraries are designed to help everyone do cryptography right, even without special expertise.
Our client-side encryption libraries help you to protect your sensitive data at its source using secure cryptographic algorithms, envelope encryption, and signing.
It is a part of AWS ENCRYPTION SDK .



##################################################################################################################
##################################################################################################################
DEVELOPER TOOLS :
##################################################################################################################
##################################################################################################################


1. AWS CLOUD-9 : --> We get an IDE to work with online.
AWS Cloud9 is a cloud-based integrated development environment (IDE) that you use to write, run, and debug code.
The AWS Cloud9 IDE offers a rich code-editing experience with support for several programming languages and runtime debuggers, and a built-in terminal.
It contains a collection of tools that you use to code, build, run, test, and debug software, and helps you release software to the cloud.
sing the AWS Cloud9 IDE, you can:
  •  Store your project's files locally on the instance or server.
  •  Clone a remote code repository — such as a repo in AWS CodeCommit — into your environment.
  •  Work with a combination of local and cloned files in the environment.

In AWS Cloud9, a development environment (or just environment) is a place where you STORE your development project's files and where you run the tools to develop your applications.
You can instruct AWS Cloud9 to create an Amazon EC2 instance, and then connect the environment to that newly created EC2 instance. This type of setup is called an EC2 environment.
You can instruct AWS Cloud9 to connect an environment to an existing cloud compute instance or to your own server. This type of setup is called an SSH environment
Free of cost but if you integrate an EC2 , compute is billed.

2. AWS CLOUD-SHELL : --> We get a shell to work with online .
AWS CloudShell is a browser-based shell that you can use to manage AWS services using the AWS Command Line Interface (AWS CLI) and a range of pre-installed development tools.
AWS CloudShell is a browser-based, pre-authenticated shell that you can launch directly from the AWS Management Console.
You can run AWS CLI commands against AWS services using your preferred shell (Bash, PowerShell, or Z shell).
And you can do this without needing to download or install command line tools.

3. AWS CODE-ARTIFACT :
AWS CodeArtifact is a secure, scalable, and cost-effective artifact management service for software development.
CodeArtifact is a fully managed artifact repository service that makes it easy for organizations to securely store, publish and share software packages used for application development.
You can use CodeArtifact with popular build tools and package managers such as NuGet, Maven, Gradle, npm, yarn, pip, and twine.

Artifacts refers to the collection of data, such as application source code, built applications, dependencies, definitions files, templates, and so on, that is worked on by pipeline actions.
Artifacts are produced by some actions and consumed by others.
In a pipeline, artifacts can be the set of files worked on by an action (input artifacts) or the updated output of a completed action (output artifacts).

4. AWS CODE-BUILD : One of the steps in CodePipeline.
AWS CodeBuild is a fully managed build service in the cloud.
CodeBuild compiles your source code, runs unit tests, and produces artifacts that are ready to deploy.
CodeBuild eliminates the need toprovision, manage, and scale your own build servers.
It provides prepackaged build environments forpopular programming languages and build tools such as Apache Maven, Gradle, and more.
You can use the AWS CodeBuild or AWS CodePipeline console to run CodeBuild.

--> As input, you must provide CodeBuild with a build project.
    A build project includes information about how to run a build, including where to get the source code, which build environment to use, which build commands to run, and where to store the build output.
    A build environment represents a combination of operating system, programming language runtime, and tools that CodeBuild uses to run a build.
--> CodeBuild uses the build project to create the build environment.
--> CodeBuild downloads the source code into the build environment and then uses the build specification ( buildspec ), as defined in the build project or included directly in the source code.
    A buildspec is a collection of build commands and related settings, in YAML format, that CodeBuild uses to run a build
--> If there is any build output, the build environment uploads its output to an S3 bucket.
    The build environment can also perform tasks that you specify in the buildspec (for example, sending build notifications to an Amazon SNS topic).
--> While the build is running, the build environment sends information to CodeBuild and Amazon CloudWatch Logs.

PROCESS :::
 Source Code + BUILD PROJECT [ BUILD SPEC -> OS + Language + Run time + Build env + where to put output ] -> CODE BUILD creates a BUILD ENVIRONMENT  -> Send output to S3 ->  Send SNS if specified .

**** If you use AWS CodePipeline to run builds, you can get limited build information from CodePipeline.
     Hence we use codebuild seperately.

5. AWS CODE-COMMIT : Similar to GITHUB and Versioning in S3.
AWS CodeCommit is a version control service that enables you to privately store and manage Git repositories in the AWS Cloud.
CodeCommit eliminates the need for you to manage your own source control system or worry about scaling its infrastructure. You can use CodeCommit to store anything from code to binaries and documents.
    1. Use the AWS CLI or the CodeCommit console to create a CodeCommit repository.
    2. From your development machine, use Git to run git clone, specifying the name of the CodeCommit repository. This creates a local repo that connects to the CodeCommit repository.
    3. Use the local repo on your development machine to modify (add, edit, and delete) files, and then run git add to stage the modified files locally. Run git commit to commit the files locally, and then run git push to send the files to the CodeCommit repository.
    4. Download changes from other users. Run git pull to synchronize the files in the CodeCommit repository with your local repo. This ensures you're working with the latest version of the files.

CodeCommit is optimized for team software development. It manages batches of changes acrossmultiple files, which can occur in parallel with changes made by other developers.
Amazon S3 versioning supports the recovery of past versions of files, but it's not focused on collaborative file tracking featuresthat software development teams need.
You can use AWS Cloud9 to make code changes in a CodeCommit repository. You need to have an AWS Cloud9 EC2 development environment running on Amazon Linux.

6. AWS CODE-DEPLOY : https://www.youtube.com/watch?v=Wx-ain8UryM&t=9s
CodeDeploy is a deployment service that automates application deployments to Amazon EC2 instances, on-premises instances, serverless Lambda functions, or Amazon ECS services.
CodeDeploy can deploy application content that runs on a server and is stored in Amazon S3 buckets, GitHub repositories, or Bitbucket repositories.
The service scales with your infrastructure so you can easily deploy to one instance or thousands.
It Make sure that the patches are deployed in all instances equally. Free of cost and pay only for EC2.

  • Rapidly release new features.
  • Update AWS Lambda function versions.
  • Avoid downtime during application deployment.
  • Handle the complexity of updating your applications, without many of the risks associated with error-prone manual deployments.

--> First, you create deployable content on your local development machine or similar environment, and then you add an application specification file (AppSpec file).
    The AppSpec file is unique to CodeDeploy. It defines the deployment actions you want CodeDeploy to execute.
    You bundle your deployable content and the AppSpec file into an archive file, and then upload it to an Amazon S3 bucket or aGitHub repository.
    This archive file is called an application revision (or simply a revision).
--> Next, you provide CodeDeploy with information about your deployment, such as which AmazonS3 bucket or GitHub repository to pull the revision from and to which set of Amazon EC2 instances to deploy its contents.
    CodeDeploy calls a set of Amazon EC2 instances a deployment group.
    A deployment group contains individually tagged Amazon EC2 instances, Amazon EC2 instances in Amazon EC2 Auto Scaling groups, or both.
--> Next, the CodeDeploy agent on each instance polls CodeDeploy to determine what and when to pull from the specified Amazon S3 bucket or GitHub repositor.
--> Finally, the CodeDeploy agent on each instance pulls the target revision from the Amazon S3 bucket or GitHub repository and, using the instructions in the AppSpec file, deploys the contents to theinstance.

PROCESS :::
 Deployable content [ from CODE BUILD ] + APPSPEC File -> S3 or GITHUB -> Tell CODE DEPLOY [ s3 url / guthub url && No; of instances ] -> CODE DEPLOY Agent in each instance pulls the revision .

7. AWS CODE-PIPELINE :
AWS CodePipeline is a continuous delivery service that enables you to model, visualize, and automate the steps required to release your software.
You can quickly model and configure the different stages of a software release process. CodePipeline automates the steps required to release your software changes continuously.
CodePipeline is a continuous delivery service that automates the building, testing, and deployment of your software into production.
  --> Continuous delivery is a software development methodology where the release process is automated.
      Every software change is automatically built, tested, and deployed to production.
      Before the final push to production, a person, an automated test, or a business rule decides when the final push should occur.
  --> Continuous integration is a software development practice where members of a team use a version control system and frequently integrate their work to the same location, such as a main branch.
      Each change is built and verified to detect integration errors as quickly as possible.
      Continuous integration is focused on automatically building and testing code, as compared to continuous delivery, which automates the entire software release process up to production.

when developers commit changes to a source repository, CodePipeline automatically detects the changes. Those changes are built, and if any tests are configured, those tests are run.
After the tests are complete, the built code is deployed to staging servers for testing.
From the staging server, CodePipeline runs more tests, such as integration or load tests.
Upon the successful completion of those tests, and after a manual approval action that was added to the pipeline is approved, CodePipeline deploys the tested and approved code to production instances.

CodePipeline can deploy applications to EC2 instances by using CodeDeploy, AWS Elastic Beanstalk, or AWS OpsWorks Stacks.
CodePipeline can also deploy container-based applications to services by using Amazon ECS.

PROCESS ::: // Check_Diagram in Refrence pics .
You can trigger an execution when you change your source code or manually start the pipeline.
You can also trigger an execution through an Amazon CloudWatch Events rule that you schedule.
For example, when a source code change is pushed to a repository configured as the pipeline's source action, the pipeline detects the change and starts an execution.

--> The application source code is maintained in a repository configured as a GitHub source action in the pipeline.
    When developers push commits to the repository, CodePipeline detects the pushed change, and a pipeline execution starts from the Source Stage.
--> The GitHub source action completes successfully (that is, the latest changes have been downloaded and stored to the artifact bucket unique to that execution).
    The output artifacts produced by the GitHub source action, which are the application files from the repository, are then used as the input artifacts to be worked on by the actions in the next stage.
--> The pipeline execution transitions from the Source Stage to the Prod Stage.
    The first action in the Prod Stage runs a build project created in CodeBuild and configured as a build action in the pipeline.
    The build task pulls a build environment image and builds the web application in a virtual container.
--> The next action in the Prod Stage is a unit test project created in CodeBuild and configured as a test action in the pipeline.
--> The unit tested code is next worked on by a deploy action in the Prod Stage that deploys the application to a production environment.
    After the deploy action completes successfully, the final action in the stage is an integration testing project created in CodeBuild and configured as a tes taction in the pipeline.
    The test action calls to shell scripts that install and run a test tool, such as a link checker, on the web application.
    After successful completion, the output is a built web application and a set of test results.

To use the console to stop a pipeline execution, you can choose Stop execution on the pipeline visualization page.
--> Stop and wait: All in-progress action executions are allowed to complete, and subsequent actions are not started.
    The pipeline execution does not continue to subsequent stages.
    You cannot use this option on an execution that is already in a Stopping state.
--> Stop and abandon: All in-progress action executions are abandoned and do not complete, and subsequent actions are not started.
    The pipeline execution does not continue to subsequent stages.
    You can use this option on an execution that is already in a Stopping state.


8. AWS CODE-STAR :
AWS CodeStar lets you quickly develop, build, and deploy applications on AWS.
An AWS CodeStar project creates and integrates AWS services for your project development toolchain.
Depending on your choice of AWS CodeStar project template, that toolchain might include source control, build, deployment, virtual servers or serverless resources, and more.
An AWS CodeStar project is a combination of source code and the resources created to deploy the code.
The collection of resources that help you build, release, and deploy your code are called tool chain resources.
At project creation, an AWS CloudFormation template provisions your toolchain resources in a continuous integration/continuous deployment (CI/CD) pipeline.
You can create a project both from CLI and console .

--> • Start new software projects on AWS in minutes using templates for web applications, web services, and more
--> • Manage project access for your team
--> • Visualize, operate, and collaborate on your projects in one place

PROCESS :::
Goto AWS -> Coe Star -> Select a Template -> Add team members -> create a repo for code -> Create a pipeline -> Deploy Project .

A developer with the AWSCodeStarFullAccess policy applied that creates a project and adds team members to it.
Together they write, build, test, and deploy code.
The project dashboard provides tools that can be used in real time to view application activity and monitor builds, the flow of code through the deployment pipeline, and more.
The team uses the team wiki tile to share information, best practices, and links.
They integrate their issue-tracking software to help them track progress and tasks.
As customers provide requests and feedback, the team adds this information to the project and integrates it into their project planning and development.
As the project grows, the team adds more team members to support their code base.

9. AWS X-RAY :
AWS X-Ray makes it easy for developers to analyze the behavior of their distributed applications by providing request tracing, exception collection, and profiling capabilities.
AWS X-Ray is a service that collects data about requests that your application serves, and provides tools you can use to view, filter, and gain insights into that data to identify issues and opportunities for optimization.
For any traced request to your application, you can see detailed information not only about the request and response, but also about calls that your application makes to downstream AWS resources, microservices, databases and HTTP web APIs.




##################################################################################################################
##################################################################################################################
MANAGEMENT & GOVERNANCE :
##################################################################################################################
##################################################################################################################
