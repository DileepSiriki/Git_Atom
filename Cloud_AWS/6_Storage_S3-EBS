------------------------------
AMAZON SIMPLE STORAGE SERVICE :
------------------------------

#### S3 does not require region selection. --> S3 GLOBAL by default
#### But BUCKETS are region specific. Select the closest region available for faster access.

Amazon Simple Storage Service (Amazon S3) is storage for the internet.
You can use Amazon S3 to store and retrieve any amount of data at any time, from anywhere on the web.
Amazon S3 stores data as objects within buckets. An object is a file and any optional metadata that describes the file.
To store a file in Amazon S3, you upload it to a bucket. When you upload a file as an object, you can set permissions on the object and any metadata.
Buckets are containers for objects. You can have one or more buckets.
You can control access for each bucket, deciding who can create, delete, and list objects in it.
You can also choose the geographical Region where Amazon S3 will store the bucket and its contents and view access logs for the bucket and its objects.

          Create a Bucket -> Upload a file -> Aws Adds metadata -> Aws creates an OBJECT [ file + metadata ]
          BUCKET = OBJECT 1 + OBJECT 2 + . . . . .
          OBJECT = FILE + METADATA
          ## Before you can store data in Amazon S3, you must create a bucket.
          ## You are not charged for creating a bucket. You are charged only for storing objects in the bucket and for transferring objects in and out of the bucket.

S3 is used to :
         • Backup and storage – Provide data backup and storage services for others.
         • Application hosting – Provide services that deploy, install, and manage web applications.
         • Media hosting – Build a redundant, scalable, and highly available infrastructure that hosts video, photo, or music uploads and downloads.
         • Software delivery – Host your software applications that customers can download.


BUCKET :
-----------
Data is stored in key-value pairs .
KEY : Sequence of UTF-8 chars up to 1024bytes long , it is the name of the file
VALUE : It is the actual data that you store .

You can use buckets to group related objects in the same way that you use a directory to group files in a file system.
By default, you can create up to 100 buckets in each of your AWS accounts.
If you need additional buckets, you can increase your account bucket quota to a maximum of 1,000 buckets by submitting a service quota increase.
    •  Bucket names must be between 3 and 63 characters long.
    •  After you create the bucket, you can't change its name.
    •  Bucket names can consist only of lowercase letters, numbers, dots (.), and hyphens (-).
    •  Bucket names must begin and end with a letter or number.
    •  Bucket names must not be formatted as an IP address (for example, 192.168.5.4).
    •  Bucket names can't begin with xn-- (for buckets created after February 2020).
    •  Bucket names must be unique within a partition.
       --> A partition is a grouping of Regions. AWS currently has three partitions.
           •   aws         (Standard Regions)
           •   aws-cn      (China Regions)
           •   aws-us-gov  (AWS GovCloud [US] Regions).

CREATE A BUCKET :
  --> Specify a unique name -->  region --> Public access --> Bucket versioning --> Encryption --> Object lock [ write once , read many , available if versioning is enabled]

UPLOAD AN OBJECT :
  --> Select Bucket --> Upload [ drag/drop or Addfiles/folders ] --> storage class -> Encryption --> ACL [ Permissions for other users / Accounts ] --> Upload

ACTIONS ON OBJECTS :
  --> Select Bucket --> Select Object / Objects --> Actions [ open , copy , move, delet, download , rename, edit meta data/tags/encryption/storage class ]

DELETE BUCKET :
  --> If you plan to delete your bucket, you must first empty your bucket, which deletes all the objects in the bucket.
  --> Buckets --> select Bucket --> top --> first EMPTY --> then DELETE

SERVER SIDE ENCRYPTION :
  --> Amazon S3 key (SSE-S3 ) : encryption key that Amazon S3 creates, manages, and uses for you.
  --> AWS Key Management Service key (SSE-KMS) : An encryption key protected by AWS Key Management Service (AWS KMS).

--> Select a bucket and check all options , Under MANAGEMENT we can write rules to replicate bucket to another region/account etc;

OBJECT :
----------
                    --------------------------------------------------------------
                    |                   GLOBLALLY UNIQUE KEY                     |
                    --------------------------------------------------------------
                    |  META DATA                |       DATA                     |
                    --------------------------------------------------------------
                    |  ACCESS CONTROL           |       HTML                     |
                    |  FILE TYPE                |       CSS                      |
                    |  TAGS                     |       IMAGES                   |
                    |  SIZE                     |       VEDIOS                   |
                    |  CEATION DATE             |       EXECUTABLES              |
                    |                           |                                |
                    --------------------------------------------------------------

OBJECT : Photos/puppy.jpg
BUCKET : Dileep
URL : http://Dileep.s3.aws.com/photos/puppy.jpg

STORAGE CLASSES :
-------------------
1. FOR FREQUENTLY ACCESSED OBJECTS :
   For performance-sensitive use cases (those that require millisecond access time) and frequently accessed data, Amazon S3 provides the following storage classes:

    --> S3 STANDARD         —  The default storage class If you don't specify the storage class when you upload an object, Amazon S3 assigns the S3 Standard storage class.
    --> REDUCED REDUNDANCY  —  The Reduced Redundancy Storage (RRS) storage class is designed for noncritical, reproducible data that can be stored with less redundancy than the S3 Standard storage class.

2. FOR AUTOMATICALLY OPTIMIZING DATA WITH CHANGING PATTERNS :
   S3 Intelligent Tiering is an Amazon S3 storage class designed to optimize storage costs by automatically moving data to the most cost-effective access tier, without operational overhead.
   It works by storing objects in four access tiers:
    --> two low latency access tiers optimized for frequent and infrequent access, and
    --> two opt-in archive access tiers designed for asynchronous access that are optimized for rare access.

  S3 Intelligent-Tiering works by monitoring access patterns and then moving the objects that have not been accessed in " N " consecutive days
    --> Objects that are uploaded or transitioned to S3 Intelligent-Tiering are automatically stored in the Frequent Access tier.
    --> 30 DAYS  : to the Infrequent Access tier.
    --> 90 DAYS  : to the Archive Access tier,
    --> 180 DAYS : to the Deep Archive Access tier.
    -->If the objects are accessed later, the objects are moved back to the Frequent Access tier.

3. FOR INFREQUENTLY ACCESSED OBJECTS :
   The S3 Standard-IA and S3 One Zone-IA storage classes are designed for long-lived and infrequently accessed data. (IA stands for infrequent access.)
   IA means  INFREQUENT ACCESS .
   USED FOR : storing backups &  For older data that is accessed infrequently, but that still requires millisecond access.

   --> S3 STANDARD-IA : Amazon S3 stores the object data redundantly across multiple geographically separated Availability Zones (similar to the S3 Standard storage class).
       S3 Standard-IA objects are resilient to the loss of an Availability Zone.
       This storage class offers greater availability and resiliency than the S3 One Zone-IA class.

   --> S3 ONE ZONE-IA : Amazon S3 stores the object data in only one Availability Zone, which makes it less expensive than S3 Standard-IA.
       However, the data is not resilient to the physical loss of the Availability Zone resulting from disasters, such as earthquakes and floods.

4. FOR ARCHIVING OBJECTS :
   --> S3 GLACIER : Has a minimum storage duration period of 90 days and can be accessed in as little as 1-5 minutes using expedited retrieval.
       If you have deleted, overwritten, or transitioned to a different storage class an object before the 90-day minimum, you are charged for 90 days.

  --> S3 GLACIER DEEP ARCHIVE : Has a minimum storage duration period of 180 days and a default retrieval time of 12 hours.
      S3 Glacier Deep Archive is the lowest cost storage option in AWS

==============================
OTHER STORAGE OPTIONS IN AWS
==============================

AWS STORAGE GATEWAY :
-----------------------
The AWS Storage Gateway is a service connecting an on-premises software appliance with cloud-based storage to provide seamless and secure integration
between an organization's on-premises IT environment and AWS's storage infrastructure.

Storage Gateway offers three different types of gateways – File Gateway, Tape Gateway, and Volume Gateway
that seamlessly connect on-premises applications to cloud storage, caching data locally for low-latency access.
Your applications connect to the service through a virtual machine or gateway hardware appliance using standard storage protocols, such as NFS, SMB, and iSCSI.
The gateway connects to AWS storage services, such as Amazon S3, Amazon S3 Glacier, Amazon S3 Glacier Deep Archive, Amazon EBS, and AWS Backup, providing storage for files, volumes, snapshots, and virtual tapes in AWS.

               -----------                                                       -----------
              |            |                         --                         |           |
              |    AWS     | <--------------------> |  | <--------------------> |   SITE    |
              |            |                         --                         |           |
               -----------                                                       -----------
                            <------------ TAKES FREQUENT SNAPSHOTS ----------->

AWS SNOWBALL :
----------------
Snowball is a petabyte-scale data transport solution that uses secure appliances to transfer large amounts of data into and out of the AWS cloud.
Using Snowball addresses common challenges with large-scale data transfers including high network costs, long transfer times, and security concerns.
Snowball is limited to 50 & 80Tb, Snowball Edge is limited to 100Tb but support a clustering feature.

How Pricing Works: Snowball pricing has four main cost components:
    (1) a service fee for each job you run,
    (2) data transfer fees from Amazon S3,
    (3) the shipping costs to transport a Snowball appliance to and from your address, and
    (4) the number of days you keep Snowball onsite.


GLACIER :
------------
Explained above .
Used for data archiving.

CLOUD FRONT :
---------------
Its a content delivery network . Similar to a cache.
Its stores the static pages that are frequently requested buy users .

Amazon EC2 instance store :
Many instances can access storage from disks that are physically attached to the host computer.
Thisdisk storage is referred to as instance store.
Instance store provides temporary block-level storage forinstances.
The data on an instance store volume persists only during the life of the associated instance.
if you stop, hibernate, or terminate an instance, any data on instance store volumes is lost.



ELASTIC BLOCK STORE : AWS -> EC2 -> Elastic Block Store -> create a volume -> if you specify snapshot , you create EBS from a snapshot else a normal EBS .
-----------------------
Its a hard drive to EC2 instances .
Persists its data even after termination of an instance .
EBS can be connected to only one Instance at a time .
But one instance can be connected more than one EBS .

             -----------
            |            | <-------------------->   /_\   EBS-1
            |    EC2     |
            |            | <-------------------->   /_\   EBS-2
             -----------
Amazon EBS provides durable, block-level storage volumes that you can attach to a running instance.
You can use Amazon EBS as a primary storage device for data that requires frequent and granularupdates.
For example, Amazon EBS is the recommended storage option when you run a database on aninstance.
The volume persists independently from the running life of an instance. After an EBS volumeis attached to an instance, you can use it like any other physical hard drive.
You can also detach an EBS volume from oneinstance and attach it to another instance. Y
ou can dynamically change the configuration of a volumeattached to an instance.
EBS volumes can also be created as encrypted volumes using the Amazon EBSencryption feature.
To keep a backup copy of your data, you can create a snapshot of an EBS volume, which is stored inAmazon S3.

EBS volumes are created in a specific Availability Zone, and can then be attached to any instancesin that same Availability Zone.
To make a volume available outside of the Availability Zone, you cancreate a snapshot and restore that snapshot to a new volume anywhere in that Region.
You can copysnapshots to other Regions and then restore them to new volumes there.
You can increase the volumes of an instance at any time by attaching more EBS to it.

        Amazon EBS provides the following volume types: DEPENDS ON IPOS / THROUGHPUT / HDD / SDD
        A hard disk drive (HDD) is an old-school storage device that uses mechanical platters and a moving read/write head to access data.
        A solid-state drive (SSD) is a newer, faster type of device that stores data on instantly-accessible memory chips.

        IOPS measures the number of read and write operations per second, while throughput measures the number of bits read or written per second.
        You have 4 buckets (Disk blocks) of the same size that you want to fill or empty water.
        You'll be using a jug to transfer the water into the buckets. Now your question will be ,
            At a given time (per second), how many jugs of water can you pour(write) or withdraw (read)? This is IOPS
            At a given time (per second) what's the amount(bit, kb, mb, ect) of water the jug can transfer into/out of the bucket continuously? This is throughput.

          SOLID STATE DRIVES SSD : costly

              General Purpose SSD :
                General Purpose SSD volumes (gp2 and gp3) balance price and performance for a wide variety oftransactional workloads.
                These volumes are ideal for a use cases such as boot volumes, medium-size single instance databases, and development and test environments.

              Provisioned IOPS SSD :
                Provides high performance for mission-critical, low-latency, or high-throughput workloads.
                Provisioned IOPS SSD volumes (io1 and io2) support up to 64,000 IOPS and 1,000 MiB/s ofthroughput.
                This enables you to predictably scale to tens of thousands of IOPS per EC2 instance.

         HARD DISK DRIVES HDD: Cheap

              Throughput Optimized HDD :
                Throughput Optimized HDD volumes (st1) provide low-cost magnetic storage that defines performance in terms of throughput rather than IOPS.
                These volumes are ideal for large, sequential workloads such as Amazon EMR, ETL, data warehouses, and log processing.

              Cold HDD :
                Cold HDD volumes (sc1) provide low-cost magnetic storage that defines performance in terms ofthroughput rather than IOPS.
                These volumes are ideal for large, sequential, cold-data workloads.
                If you require infrequent access to your data and are looking to save costs, these volumes provides inexpensive block storage.

        USES OF EBS :
          Data availability :
             These are  automatically replicated within its Availability Zone to prevent dataloss due to failure of any single hardware component.
             You can attach an EBS volume to any EC2 instancein the same Availability Zone.

          Data persistence : --> Delete on Termination checkbox
            EBS volumes that are attached to a running instance can automatically detach from the instance withtheir data intact when the instance is terminated.
            if you uncheck the Delete on Termination checkbox when you configure EBS volumes for your instance on the EC2 console

          Data encryption :
            Amazon EBS encryption uses AWS Key Management Service (AWS KMS) .
            The snapshots created from this encrypted EBS is also encrypted.

          Snapshots : use Amazon Data Lifecycle Manager 
            Ebs -> snapshots -> create ->
                                          either from a volume  --> Single volume snapshot
                                          or instance --> Multi volume snapshot [ as instance contain more than 1 volume attached ]

            Thevolume does not need to be attached to a running instance in order to take a snapshot.
            Snapshots of encrypted EBS volumes are automatically encrypted.
            When you create a new volume from a snapshot, it's an exact copy of the original volume at the timethe snapshot was taken.
            Snapshots are incremental backups, meaning that only the blocks on the volume that have changedafter your most recent snapshot are saved.
            If you have a volume with 100 GiB of data, but only 5 GiB ofdata have changed since your last snapshot, only the 5 GiB of modified data is written to Amazon S3.

          Flexibility : ebs -> select a volume-> actions -> attach/detach
            EBS volumes support live configuration changes while in production. You can modify volume type,volume size, and IOPS capacity without service interruptions.

      Amazon Data Lifecycle Manager : SNAPSHOTS AUTOMATION  AWS -> Ebs -> life cycle manager

        You can use Amazon Data Lifecycle Manager to automate the creation, retention, and deletion of EBSsnapshots and EBS-backed AMIs.
        When you automate snapshot and AMI management, it helps you to ,
          •Protect valuable data by enforcing a regular backup schedule.
          •Create standardized AMIs that can be refreshed at regular intervals.
          •Retain backups as required by auditors or internal compliance.
          •Reduce storage costs by deleting outdated backups.
          •Create disaster recovery backup policies that back up data to isolated accounts.
