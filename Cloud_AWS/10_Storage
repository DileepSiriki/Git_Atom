------------------------------
AMAZON SIMPLE STORAGE SERVICE :
------------------------------

#### S3 does not require region selection. --> S3 GLOBAL by default
#### But BUCKETS are region specific. Select the closest region available for faster access.
Cheaper: lifecycle policies, reduced redundancy, IA, Glacier
Resilient: versioning, s3 replication, regional replication
Performant: CloudFront (for web), regional S3 endpoints, hashing key names
Secure: VPC endpoints, encryption at rest

Amazon Simple Storage Service (Amazon S3) is storage for the internet.
You can use Amazon S3 to store and retrieve any amount of data at any time, from anywhere on the web.
Amazon S3 stores data as objects within buckets. An object is a file and any optional metadata that describes the file.
To store a file in Amazon S3, you upload it to a bucket. When you upload a file as an object, you can set permissions on the object and any metadata.
Buckets are containers for objects. You can have one or more buckets.
You can control access for each bucket, deciding who can create, delete, and list objects in it.
You can also choose the geographical Region where Amazon S3 will store the bucket and its contents and view access logs for the bucket and its objects.

          Create a Bucket -> Upload a file -> Aws Adds metadata -> Aws creates an OBJECT [ file + metadata ]
          BUCKET = OBJECT 1 + OBJECT 2 + . . . . .
          OBJECT = FILE + METADATA
          ## Before you can store data in Amazon S3, you must create a bucket.
          ## You are not charged for creating a bucket. You are charged only for storing objects in the bucket and for transferring objects in and out of the bucket.

S3 is used to :
         • Backup and storage – Provide data backup and storage services for others.
         • Application hosting – Provide services that deploy, install, and manage web applications.
         • Media hosting – Build a redundant, scalable, and highly available infrastructure that hosts video, photo, or music uploads and downloads.
         • Software delivery – Host your software applications that customers can download.


BUCKET :
-----------
Data is stored in key-value pairs .
KEY : Sequence of UTF-8 chars up to 1024bytes long , it is the name of the file
VALUE : It is the actual data that you store .

You can use buckets to group related objects in the same way that you use a directory to group files in a file system.
By default, you can create up to 100 buckets in each of your AWS accounts.
If you need additional buckets, you can increase your account bucket quota to a maximum of 1,000 buckets by submitting a service quota increase.
    •  Bucket names must be between 3 and 63 characters long.
    •  After you create the bucket, you can't change its name.
    •  Bucket names can consist only of lowercase letters, numbers, dots (.), and hyphens (-).
    •  Bucket names must begin and end with a letter or number.
    •  Bucket names must not be formatted as an IP address (for example, 192.168.5.4).
    •  Bucket names can't begin with xn-- (for buckets created after February 2020).
    •  Bucket names must be unique within a partition.
       --> A partition is a grouping of Regions. AWS currently has three partitions.
           •   aws         (Standard Regions)
           •   aws-cn      (China Regions)
           •   aws-us-gov  (AWS GovCloud [US] Regions).

CREATE A BUCKET :
  --> Specify a unique name -->  region --> Public access --> Bucket versioning --> Encryption --> Object lock [ write once , read many , available if versioning is enabled]

UPLOAD AN OBJECT :
  --> Select Bucket --> Upload [ drag/drop or Addfiles/folders ] --> storage class -> Encryption --> ACL [ Permissions for other users / Accounts ] --> Upload

ACTIONS ON OBJECTS :
  --> Select Bucket --> Select Object / Objects --> Actions [ open , copy , move, delet, download , rename, edit meta data/tags/encryption/storage class ]

DELETE BUCKET :
  --> If you plan to delete your bucket, you must first empty your bucket, which deletes all the objects in the bucket.
  --> Buckets --> select Bucket --> top --> first EMPTY --> then DELETE

SERVER SIDE ENCRYPTION :
  --> Amazon S3 key (SSE-S3 ) : encryption key that Amazon S3 creates, manages, and uses for you.
  --> AWS Key Management Service key (SSE-KMS) : An encryption key protected by AWS Key Management Service (AWS KMS).

--> Select a bucket and check all options , Under MANAGEMENT we can write rules to replicate bucket to another region/account etc;

OBJECT :
----------
                    --------------------------------------------------------------
                    |                   GLOBLALLY UNIQUE KEY                     |
                    --------------------------------------------------------------
                    |  META DATA                |       DATA                     |
                    --------------------------------------------------------------
                    |  ACCESS CONTROL           |       HTML                     |
                    |  FILE TYPE                |       CSS                      |
                    |  TAGS                     |       IMAGES                   |
                    |  SIZE                     |       VEDIOS                   |
                    |  CEATION DATE             |       EXECUTABLES              |
                    |                           |                                |
                    --------------------------------------------------------------

OBJECT : Photos/puppy.jpg
BUCKET : Dileep
URL : http://Dileep.s3.aws.com/photos/puppy.jpg

REPLICATION IN S3 :
-------------------------
Replication enables automatic, asynchronous copying of objects across Amazon S3 buckets.
Buckets that are configured for object replication can be owned by the same AWS account or by different accounts.
Object may be replicated to a single destination bucket or multiple destination buckets.
Destination buckets can be in different AWS Regions or within the same Region as the source bucket.

To enable object replication, you add a replication configuration to your source bucket. The minimum configuration must provide the following:
--> The destination bucket or buckets where you want Amazon S3 to replicate objects
--> An AWS Identity and Access Management (IAM) role that Amazon S3 can assume to replicate objects on your behalf

Replication can help you do the following:
------------------------------------------

Replicate objects while retaining metadata :
   You can use replication to make copies of your objects that retain all metadata, such as the original object creation time and version IDs.
   This capability is important if you need to ensure that your replica is identical to the source object.
Replicate objects into different storage classes :
   You can use replication to directly put objects into S3 Glacier, S3 Glacier Deep Archive, or another storage class in the destination buckets.
   You can also replicate your data to the same storage class and use lifecycle policies on the destination buckets to move your objects to a colder storage class as it ages.
Maintain object copies under different ownership :
   Regardless of who owns the source object, you can tell Amazon S3 to change replica ownership to the AWS account that owns the destination bucket.
   This is referred to as the owner override option. You can use this option to restrict access to object replicas.
Keep objects stored over multiple AWS Regions :
   You can set multiple destination buckets across different AWS Regions to ensure geographic differences in where your data is kept.
   This could be useful in meeting certain compliance requirements.
Replicate objects within 15 minutes :
   You can use S3 Replication Time Control (S3 RTC) to replicate your data in the same AWS Region or across different Regions in a predictable time frame.
   S3 RTC replicates 99.99 percent of new objects stored in Amazon S3 within 15 minutes (backed by a service level agreement).

 WHEN TO USE CROSS_REGION REPLICATION :
 -----------------------------------------
   S3 Cross-Region Replication (CRR) is used to copy objects across Amazon S3 buckets in different AWS Regions. CRR can help you do the following-
     --> Meet compliance requirements :
         Although Amazon S3 stores your data across multiple geographically distant Availability Zones by default, compliance requirements might dictate that you store data at even greater distances. Cross-Region Replication allows you to replicate data between distant AWS Regions to satisfy these requirements.
     --> Minimize latency :
         If your customers are in two geographic locations, you can minimize latency in accessing objects by maintaining object copies in AWS Regions that are geographically closer to your users.
     --> Increase operational efficiency :
         If you have compute clusters in two different AWS Regions that analyze the same set of objects, you might choose to maintain object copies in those Regions.

 WHEN TO USE SAME_REGION REPLICATION :
 -----------------------------------------
   Same-Region Replication (SRR) is used to copy objects across Amazon S3 buckets in the same AWS Region. SRR can help you do the following-
     --> Aggregate logs into a single bucket :
         If you store logs in multiple buckets or across multiple accounts, you can easily replicate logs into a single, in-Region bucket. This allows for simpler processing of logs in a single location.
     --> Configure live replication between production and test accounts :
         If you or your customers have production and test accounts that use the same data, you can replicate objects between those multiple accounts, while maintaining  object metadata.
     --> Abide by data sovereignty laws :
         You might be required to store multiple copies of your data in separate AWS accounts within a certain Region. Same-Region Replication can help you automatically replicate critical data when compliance regulations don't allow the data to leave your country.

 REQUIREMENTS FOR REPLICATION :
 ------------------------------
 --> The source bucket owner must have the source and destination AWS Regions enabled for their account.
     The destination bucket owner must have the destination Region-enabled for their account.
     For more information about enabling or disabling an AWS Region, see AWS Service Endpoints in the AWS General Reference.
 --> Both source and destination buckets must have versioning enabled. For more information about versioning, see Using versioning.
 --> Amazon S3 must have permissions to replicate objects from the source bucket to the destination bucket or buckets on your behalf.
 --> If the owner of the source bucket doesn't own the object in the bucket, the object owner must grant the bucket owner READ and READ_ACP permissions with the object access control list (ACL).
     For more information, see Managing Access with ACLs.
 --> If the source bucket has S3 Object Lock enabled, the destination buckets must also have S3 Object Lock enabled. For more information, see Locking objects using S3 Object Lock.
 --> To enable replication on a bucket that has Object Lock enabled, contact AWS Support.

 If you are setting the replication configuration in a cross-account scenario, where source and destination buckets are owned by different AWS accounts, the following additional requirement applies:
  --> The owner of the destination buckets must grant the owner of the source bucket permissions to replicate objects with a bucket policy.
      For more information, see Granting permissions when source and destination buckets are owned by different AWS accounts.
  --> The destination buckets cannot be configured as Requester Pays buckets. For more information, see Requester Pays buckets.


STORAGE CLASSES : https://aws.amazon.com/s3/storage-classes/
-------------------
1. FOR FREQUENTLY ACCESSED OBJECTS :
   For performance-sensitive use cases (those that require millisecond access time) and frequently accessed data, Amazon S3 provides the following storage classes:

    --> S3 STANDARD         —  The default storage class If you don't specify the storage class when you upload an object, Amazon S3 assigns the S3 Standard storage class.
    --> REDUCED REDUNDANCY  —  The Reduced Redundancy Storage (RRS) storage class is designed for noncritical, reproducible data that can be stored with less redundancy than the S3 Standard storage class.
                               The reduced redundancy storage class has the lowest durability of all the storage classes.
                               This means object stored in this storage class have the highest probability of being lost.
                               So you should only store objects in this storage class if they can be easily reproduced.
                               In exchange for the lower durability, the cost is lower than the standard storage class.

    It is appropriate for cloud applications, dynamic websites, content distribution, mobile and gaming applications, and big data analytics.
    --> Low latency and high throughput performance
    --> Designed for durability of 99.999999999% of objects across multiple Availability Zones
    --> Designed for 99.99% availability over a given year.

2. FOR AUTOMATICALLY OPTIMIZING DATA WITH CHANGING PATTERNS :
   The only cloud storage class that delivers automatic cost savings by moving objects to the most cost-effective access tier between four access tiers when access patterns change.
   S3 Intelligent Tiering is an Amazon S3 storage class designed to optimize storage costs by automatically moving data to the most cost-effective access tier, without operational overhead.
   There are no retrieval fees when using the S3 Intelligent-Tiering storage class, and no additional tiering fees when objects are moved between access tiers within S3 Intelligent-Tiering.

   It works by storing objects in the remaiming four access tiers.
    --> two low latency access tiers optimized for frequent and infrequent access, and
    --> two opt-in archive access tiers designed for asynchronous access that are optimized for rare access.

   S3 Intelligent-Tiering works by monitoring access patterns and then moving the objects that have not been accessed in " N " consecutive days
    --> Initially Objects that are uploaded or transitioned to S3 Intelligent-Tiering are automatically stored in the Frequent Access tier.
    --> 30 DAYS  - to the Infrequent Access tier.
    --> 90 DAYS  - to the Archive Access tier,
    --> 180 DAYS - to the Deep Archive Access tier.
    --> If the objects are accessed later, the objects are moved back to the Frequent Access tier.

   It is ideal for new applications and Data lakes.
    --> Designed for durability of 99.999999999% of objects across multiple Availability Zones
    --> Designed for 99.9% availability over a given year

3. FOR INFREQUENTLY ACCESSED OBJECTS :
   The S3 Standard-IA and S3 One Zone-IA storage classes are designed for long-lived and infrequently accessed data. (IA stands for infrequent access.)
   S3 Standard-IA is for data that is accessed less frequently, but requires rapid access when needed.
   S3 Standard-IA offers the high durability, high throughput, and low latency of S3 Standard, with a low per GB storage price and per GB retrieval fee.

   ## IA means  INFREQUENT ACCESS .
   --> S3 STANDARD-IA : Amazon S3 stores the object data redundantly across multiple geographically separated Availability Zones (similar to the S3 Standard storage class).
       S3 Standard-IA objects are resilient to the loss of an Availability Zone.
       This storage class offers greater availability and resiliency than the S3 One Zone-IA class.

   It is ideal for long-term storage, backups, and as a data store for disaster recovery files.
   --> Designed for durability of 99.999999999% of objects across multiple Availability Zones
   --> Designed for 99.9% availability over a given year

   ## IA means  INFREQUENT ACCESS .
   --> S3 ONE ZONE-IA : It is for data that is accessed less frequently, but requires rapid access when needed.
       Unlike other S3 Storage Classes which store data in a minimum of three Availability Zones (AZs), S3 One Zone-IA stores data in a single AZ and costs 20% less than S3 Standard-IA.
       So it is not resistant against any natural disaster like earthquakes or cyclones as the data is not replicated .
       Data stored in this storage class will be lost in the event of Availability Zone destruction.

  It is ideal for customers who want a lower-cost option for infrequently accessed data but do not require the availability and resilience of S3 Standard or S3 Standard-IA.
  It’s a good choice for storing secondary backup copies of on-premises data or easily re-creatable data.
  --> Designed for durability of 99.999999999% of objects in a single Availability Zone.
  --> Designed for 99.5% availability over a given year.


4. FOR ARCHIVING OBJECTS :
   --> S3 GLACIER : Has a minimum storage duration period of 90 days and can be accessed in as little as 1-5 minutes using expedited retrieval.
       If you have deleted, overwritten, or transitioned to a different storage class an object before the 90-day minimum, you are charged for 90 days.
       S3 Glacier is a secure, durable, and low-cost storage class for data archiving.
       You can reliably store any amount of data at costs that are competitive with or cheaper than on-premises solutions.
       To keep costs low yet suitable for varying needs, S3 Glacier provides three retrieval options that range from a few minutes to hours.

   Ideal for long-term archive.
   --> Designed for durability of 99.999999999% of objects across multiple Availability Zones
   --> Configurable retrieval times, from minutes to hours.

  --> S3 GLACIER DEEP ARCHIVE : Has a minimum storage duration period of 180 days and a default retrieval time of 12 hours.
      S3 Glacier Deep Archive is the lowest cost storage option in AWS and supports long-term retention and digital preservation for data that may be accessed once or twice in a year.
      S3 Glacier Deep Archive complements Amazon S3 Glacier, which is ideal for archives where data is regularly retrieved and some of the data may be needed in minutes.

  Ideal for those in highly-regulated industries, such as the Financial Services, Healthcare, and Public Sectors — that retain data sets for 7-10 years or longer to meet regulatory compliance requirements.
  --> Designed for durability of 99.999999999% of objects across multiple Availability Zones.
  --> Retrieval time within 12 hours.
  --> Lowest cost storage class designed for long-term retention of data that will be retained for 7-10 years.

5. OTHER - OUTPOSTS :
    Amazon S3 on Outposts delivers object storage to your on-premises AWS Outposts environment.
    If you have data residency requirements that can’t be met by an existing AWS Region, you can use the S3 OUTPOSTS storage class to store your S3 data on-premises.
    S3 on Outposts provides a single Amazon S3 storage class, named S3 Outposts, which uses the S3 APIs, and is designed to durably and redundantly store data across multiple devices and servers on your Outposts.
    S3 Outposts storage class is ideal for workloads with local data residency requirements, and to satisfy demanding performance needs by keeping data close to on-premises applications.

KEY POINTS FOR STORAGE CLASSES :
S3 Intelligent-Tiering charges a small tiering fee and has a minimum eligible object size of 128KB for auto-tiering.
Smaller objects may be stored but will always be charged at the Frequent Access tier rates.

Amazon S3 also offers capabilities to manage your data throughout its lifecycle.
Once an S3 Lifecycle policy is set, your data will automatically transfer to a different storage class without any changes to your application.

Standard retrievals in archive access tier and deep archive access tier are free.
Using the S3 console, you can pay for expedited retrievals if you need faster access to your data from the archive access tiers.

S3 Intelligent-Tiering first byte latency for frequent and infrequent access tier is milliseconds access time, and
the archive access and deep archive access tiers first byte latency is minutes or hours.

-------------------------------------------------------------------------------------------------------------------------------------------------------------
                                S3 Standard 	      S3_Intelligent-      	S3_Standard-IA 	       S3_One_Zone-IA	        S3_Glacier     	    S3_Glacier-
                                                       Tiering                                                                              Deep_Archive
-------------------------------------------------------------------------------------------------------------------------------------------------------------

Designed for durability 	      11 9’s                11 9’s                  11 9’s                11 9’s                11 9’s               11 9’s
   (99.999999999%)

Designed for availability     	99.99% 	              99.9% 	                99.9% 	               99.5% 	              99.99% 	             99.99%

Availability SLA 	              99.9% 	              99% 	                  99% 	                 99% 	                99.9%                99.9%

Availability Zones 	            ≥3 	                  ≥3 	                    ≥3                    	1                 	≥3                    ≥3

Minimum capacity  	            N/A                 	N/A                   	128KB 	               128KB 	              40KB 	               40KB
charge per object

Minimum storage  	              N/A               	30 days                 	30 days              	30 days               90 days             180 days
duration charge

Retrieval fee 	                N/A	                 N/A               	per GB retrieved      	per GB retrieved 	     per GB retrieved  	 per GB retrieved

First byte latency 	       milliseconds         	milliseconds           	milliseconds 	           milliseconds   	select minutes or hours 	select hours

Storage Type &                 YES                   YES                       YES                   YES                    YES                   YES
Life Cycle Transition
---------------------------------------------------------------------------------------------------------------------------------------------------------------------

STORAGE PRICES :
-------------------
S3 Standard - General purpose storage for any type of data, typically used for frequently accessed data	 :
First 50 TB / Month	                                                              $0.023 per GB
Next 450 TB / Month                                                             	$0.022 per GB
Over 500 TB / Month	                                                              $0.021 per GB

S3 Intelligent - Tiering * - Automatic cost savings for data with unknown or changing access patterns	:
Frequent Access Tier, First 50 TB / Month	                                        $0.023 per GB
Frequent Access Tier, Next 450 TB / Month	                                        $0.022 per GB
Frequent Access Tier, Over 500 TB / Month	                                        $0.021 per GB
Infrequent Access Tier, All Storage / Month	                                      $0.0125 per GB
Archive Access Tier, All Storage / Month	                                        $0.004 per GB
Deep Archive Access Tier, All Storage / Month	                                    $0.00099 per GB
Monitoring and Automation, All Storage / Month	                                  $0.0025 per 1,000 objects

S3 Standard - Infrequent Access * - For long lived but infrequently accessed data that needs millisecond access	:
All Storage / Month	                                                              $0.0125 per GB

S3 One Zone - Infrequent Access * - For re-createable infrequently accessed data that needs millisecond access	:
All Storage / Month                                                             	$0.01 per GB

S3 Glacier ** - For long-term backups and archives with retrieval option from 1 minute to 12 hours	:
All Storage / Month                                                             	$0.004 per GB

S3 Glacier Deep Archive ** - For long-term data archiving that is accessed once or twice in a year and can be restored within 12 hours :
All Storage / Month	                                                              $0.00099 per GB



STATIC WEBSITE HOSTING :
-------------------------
STEP 1 : In route53 , register for a domain , say example.com

STEP 2 :

Create a Bucket : example.com

  Bucket name should be similar to the domain name as in example.common
  Select the bucket and then check for PROPERTIES .
  Move to bottom of the page and enable static website hosting .
  --> IN the options , select  HOST A STATIC WEBSITE
  We should make this bucket public.
  Specify the index and error pages .
  You'll create an HTML file and upload it to your bucket for an index page.

Create another Bucket : www.example.com

  If you also want your users to be able to use www.your-domain-name, such as www.example.com, toaccess your sample website, you create a second S3 bucket.
  You then configure the second bucket toroute traffic to the first bucket.

  Once the bucket www.example.com is created ,
  go to permissions -> enable static website HOSTING
  --> IN the options , select REDIRECT REQUESTS FOR AN OBJECT
  Enter the target bucket [above bucket example.com]
  Select http protocol as S3 do not support https.

STEP 3:
  create a html page and save it as index.html .
  upload the page into the bucket.

STEP 4 :
  Route DNS traffic for your domain to yourwebsite bucket
  Goto Route53 -> Hosted zones -> if you register a domain , a hosted zone is created automatically.
  chose your domain in hosted zones
  chose create record  -> record contains information about how you want to route traffic for one domain
  Choose Simple routing and choose Next
  Choose Define simple record
    --> specify RECORD NAME --> Accept the default value, which is the name of your hostedzone and your domain. This will route internet traffic to the bucket that has the same name asyour domain.
    --> Repeat the process for www.example.com and other sub domains.
    --> specify VALUE/ROUTE to Choose Alias to S3 website endpoint, then choose the AWS Region that the bucket was createdin
    For the first record that you create, choose the bucket that has the same name as your hostedzone and your domain.
    For the second record, choose the bucket that has the name www.your-domain-name.
    --> specify RECORD TYPE --> Accept the default value of A – Routes traffic to an IPv4 address and some AWS resources
  Chose define simple records
  chose create record.

  STEP 5 :
    To verify that the website is working correctly, open a web browser and browse to the following URLs:
      •  http://your-domain-name – Displays the index document in the your-domain-name bucket
      •  http://www.your-domain-name – Redirects your request to the your-domain-name bucket


AWS S3 TRANSFER ACCELERATION :
If you’re transferring your data to popular AWS storage platform S3 over long distances, AWS S3 Transfer Acceleration helps you do it faster 171% faster on average, according to AWS.
Amazon S3 Transfer Acceleration is a service that allows you to upload data to an S3 bucket quickly and securely over the public Internet.
If you’re uploading to a centralized bucket from different locations across the globe, S3 Transfer Acceleration can save a lot of transfer time.
Data is routed to S3 on optimized network paths via Amazon CloudFront edge locations, which are spread across the globe.
This helps maximize available bandwidth no matter how far the data is travelling or how much the latency varies.
There are no special clients or proprietary network protocols involved either.
Just turn on Transfer Acceleration for an S3 bucket using the Amazon S3 console, set your S3 endpoint to one of two TA options, and the acceleration is applied automatically.
The service is available for both reading and writing data to Amazon S3, making it useful for recurring jobs like media uploads, backups, and local data processing.

For your bucket to work with transfer acceleration, the bucket name must conform to DNS naming requirements and must not contain periods (".").
The service is a paid service .

  WHY USE --> You might want to use Transfer Acceleration on a bucket for various reasons, including the following:
          --> You have customers that upload to a centralized bucket from all over the world.
          -->You transfer gigabytes to terabytes of data on a regular basis across continents.
          -->You are unable to utilize all of your available bandwidth over the Internet when uploading to Amazon S3.



==============================
OTHER STORAGE OPTIONS IN AWS
==============================

AWS STORAGE GATEWAY : https://www.youtube.com/watch?v=Spzdj1NUJbA -> VV.IMP --> Watch it
-----------------------
The AWS Storage Gateway is a service connecting an on-premises software appliance with cloud-based storage to provide seamless and secure integration
between an organization's on-premises IT environment and AWS's storage infrastructure.

Storage Gateway offers three different types of gateways – File Gateway, Tape Gateway, and Volume Gateway
that seamlessly connect on-premises applications to cloud storage, caching data locally for low-latency access.
Your applications connect to the service through a virtual machine or gateway hardware appliance using standard storage protocols, such as NFS, SMB, and iSCSI.
The gateway connects to AWS storage services, such as Amazon S3, Amazon S3 Glacier, Amazon S3 Glacier Deep Archive, Amazon EBS, and AWS Backup, providing storage for files, volumes, snapshots, and virtual tapes in AWS.

               -----------                                                       -----------
              |            |                         --                         |           |
              |    AWS     | <--------------------> |  | <--------------------> |   SITE    |
              |            |                         --                         |           |
               -----------                                                       -----------
                            <------------ TAKES FREQUENT SNAPSHOTS ----------->

    File Gateway :  A file gateway supports a file interface into Amazon Simple Storage Service (AmazonS3) and combines a service and a virtual software appliance.
    By using this combination, you can storeand retrieve objects in Amazon S3 using industry-standard file protocols such as Network File System(NFS) and Server Message Block (SMB).
    The software appliance, or gateway, is deployed into your on-premises environment as a virtual machine (VM) running on VMware ESXi, Microsoft Hyper-V, or LinuxKernel-based Virtual Machine (KVM) hypervisor.
    The gateway provides access to objects in S3 as files or file share mount points.

    Volume Gateway : A volume gateway provides cloud-backed storage volumes that you can mount as Internet Small Computer System Interface (iSCSI) devices from your on-premises application servers.
    The volume gateway is deployed into your on-premises environment as a VM running on VMware ESXi,KVM, or Microsoft Hyper-V hypervisor.
    Your on-premises applications can access these as Internet Small Computer System Interface (iSCSI)targets
    The gateway supports the following volume configurations -
      • Cached volumes : You store your data in Amazon Simple Storage Service (Amazon S3) and retain a copy of frequently accessed data subsets locally.
      • Stored volumes : If you need low-latency access to your entire dataset, first configure your on-premises gateway to store all your data locally.
                         Then asynchronously back up point-in-time snapshots of this data to Amazon S3.

   Tape Gateway : A tape gateway provides cloud-backed virtual tape storage.
   The tape gateway is deployed into your on-premises environment as a VM running on VMware ESXi, KVM, or Microsoft Hyper-V hypervisor.
   With a tape gateway, you can cost-effectively and durably archive backup data in GLACIER orDEEP_ARCHIVE.
   The tapes are moved to aws by trucks and are converted to objects to be stored in glacier storage / S3.

You can run AWS Storage Gateway either on-premises as a VM appliance, as a hardware appliance, or in AWS as an Amazon EC2 instance.


AWS SNOW-FAMILY:
-----------------
  -> Snow cone        - https://www.youtube.com/watch?v=X_8LM7E_hiE      -- 8 TB storage   --> 1 SMALL BOX [ uses opshub application for data transfer ]
     Snow Ball        - https://www.youtube.com/watch?v=9uc2DSZ1wL8&t=9s -- 80 TB storage  --> 1 SUITCASE
     Snow Ball edge   - https://www.youtube.com/watch?v=bxSD1Nha2k8      -- 100 TB storage --> 1 SUITCASE  [ Embedded Ec2 - Lambda for running jobs in remote areas like ships , hills for an immediate results. ]
     Snow Mobile      - https://www.youtube.com/watch?v=8vQmTZTq7nw      -- 1 Exabyte      --> 1 Truck

    1 Exabyte = 1000 petabytes
    Snow ball can be connected in series to have a Petabyte's storage  = 1 million GB = 1024 Terabytes


AWS SNOWCONE : -  (8.94” x 5.85” x 3.25” / 227 mm x 148.6 mm x 82.65 mm),  WEighs about 4.5 lbs. (2 kg) ,run on a battery for up to approximately 6 hours
----------------
AWS Snowcone is a portable, rugged, and secure device for edge computing and data transfer.
You canuse Snowcone to collect, process, and move data to AWS, either offline by shipping the device to AWS, or online by using AWS DataSync.
The Snowcone device supports data transfer fromon-premises Windows, Linux, and macOS servers and file-based applications through the NFS interface.
Supports upto Dozens of TB. It can be used in space-constrained environments where Snowball Edge devices don't fit.
    •  For edge computing applications, to collect data, process the data to gain immediate insight, and thentransfer the data online to AWS.
    •  To transfer data that is continuously generated by sensors or machines online to AWS in a factory or atother edge locations.
    •  To distribute media, scientific, or other content from AWS storage services to your partners andcustomers.
    •  To aggregate content by transferring media, scientific, or other content from your edge locations toAWS.
    •  For one-time data migration scenarios where your data is ready to be transferred, where Snowconeoffers a simple, quick, and low-cost way to transfer up to 8 TB of data into AWS by shipping the deviceback to AWS.

 The Amazon Simple Storage Service(Amazon S3) buckets, data, and Amazon Elastic Compute Cloud (Amazon EC2) AMIs that you choose areautomatically configured, encrypted, and pre-installed on your devices.
 The AWS DataSync agent is alsopre-installed before your devices are shipped to you.
 you connect it to your on-premises network and set the IP address eithermanually or automatically with DHCP.
 You must download and install AWS OpsHub for Snow Family, on a windows or mac lappy.
 When data is transferred , you can ship it back .


AWS SNOWBALL :
---------------
Snowball is a petabyte-scale data transport solution that uses secure appliances to transfer large amounts of data into and out of the AWS cloud.
Using Snowball addresses common challenges with large-scale data transfers including high network costs, long transfer times, and security concerns.
Snowball is limited to 50 & 80Tb, Snowball Edge is limited to 100Tb but support a clustering feature.

How Pricing Works: Snowball pricing has four main cost components:
    (1) a service fee for each job you run,
    (2) data transfer fees from Amazon S3,
    (3) the shipping costs to transport a Snowball appliance to and from your address, and
    (4) the number of days you keep Snowball onsite.


GLACIER :
------------
Explained above .
Used for data archiving.

CLOUD FRONT :
---------------
Its a content delivery network . Similar to a cache.
Its stores the static pages that are frequently requested buy users .

Amazon EC2 instance store :
--------------------------------
Many instances can access storage from disks that are physically attached to the host computer.
Thisdisk storage is referred to as instance store.
Instance store provides temporary block-level storage forinstances.
The data on an instance store volume persists only during the life of the associated instance.
if you stop, hibernate, or terminate an instance, any data on instance store volumes is lost.


ELASTIC BLOCK STORE : AWS -> EC2 -> Elastic Block Store -> create a volume -> if you specify snapshot , you create EBS from a snapshot else a normal EBS .
-----------------------
Its a hard drive to EC2 instances .
Persists its data even after termination of an instance .
EBS can be connected to only one Instance at a time .
But one instance can be connected more than one EBS .

             -----------
            |            | <-------------------->   /_\   EBS-1
            |    EC2     |
            |            | <-------------------->   /_\   EBS-2
             -----------
Amazon EBS provides durable, block-level storage volumes that you can attach to a running instance.
You can use Amazon EBS as a primary storage device for data that requires frequent and granularupdates.
For example, Amazon EBS is the recommended storage option when you run a database on aninstance.
The volume persists independently from the running life of an instance. After an EBS volumeis attached to an instance, you can use it like any other physical hard drive.
You can also detach an EBS volume from oneinstance and attach it to another instance. Y
ou can dynamically change the configuration of a volumeattached to an instance.
EBS volumes can also be created as encrypted volumes using the Amazon EBSencryption feature.
To keep a backup copy of your data, you can create a snapshot of an EBS volume, which is stored inAmazon S3.

EBS volumes are created in a specific Availability Zone, and can then be attached to any instancesin that same Availability Zone.
To make a volume available outside of the Availability Zone, you cancreate a snapshot and restore that snapshot to a new volume anywhere in that Region.
You can copysnapshots to other Regions and then restore them to new volumes there.
You can increase the volumes of an instance at any time by attaching more EBS to it.

        Amazon EBS provides the following volume types: DEPENDS ON IPOS / THROUGHPUT / HDD / SDD
        A hard disk drive (HDD) is an old-school storage device that uses mechanical platters and a moving read/write head to access data.
        A solid-state drive (SSD) is a newer, faster type of device that stores data on instantly-accessible memory chips.

        IOPS measures the number of read and write operations per second, while throughput measures the number of bits read or written per second.
        You have 4 buckets (Disk blocks) of the same size that you want to fill or empty water.
        You'll be using a jug to transfer the water into the buckets. Now your question will be ,
            At a given time (per second), how many jugs of water can you pour(write) or withdraw (read)? This is IOPS
            At a given time (per second) what's the amount(bit, kb, mb, ect) of water the jug can transfer into/out of the bucket continuously? This is throughput.

          SOLID STATE DRIVES SSD : costly --> Low latency - High IOPS
              Used for BOOT volumes , transactional work loads [ enteprise apps , relational data bases, NOSQL data bases ]

              General Purpose SSD :
                General Purpose SSD volumes (gp2 and gp3) balance price and performance for a wide variety oftransactional workloads.
                These volumes are ideal for a use cases such as boot volumes, medium-size single instance databases, and development and test environments.

              Provisioned IOPS SSD :
                Provides high performance for mission-critical, low-latency, or high-throughput workloads.
                Provisioned IOPS SSD volumes (io1 and io2) support up to 64,000 IOPS and 1,000 MiB/s ofthroughput.
                This enables you to predictably scale to tens of thousands of IOPS per EC2 instance.

         HARD DISK DRIVES HDD: Cheap --> Higest Throughput
              For Straming IO , Big data , data ware housing , log processing applications
              Throughput Optimized HDD :
                Throughput Optimized HDD volumes (st1) provide low-cost magnetic storage that defines performance in terms of throughput rather than IOPS.
                These volumes are ideal for large, sequential workloads such as Amazon EMR, ETL, data warehouses, and log processing.

              Cold HDD :
                Cold HDD volumes (sc1) provide low-cost magnetic storage that defines performance in terms ofthroughput rather than IOPS.
                These volumes are ideal for large, sequential, cold-data workloads.
                If you require infrequent access to your data and are looking to save costs, these volumes provides inexpensive block storage.

        USES OF EBS :
          Data availability :
             These are  automatically replicated within its Availability Zone to prevent dataloss due to failure of any single hardware component.
             You can attach an EBS volume to any EC2 instancein the same Availability Zone.

          Data persistence : --> Delete on Termination checkbox
            EBS volumes that are attached to a running instance can automatically detach from the instance withtheir data intact when the instance is terminated.
            if you uncheck the Delete on Termination checkbox when you configure EBS volumes for your instance on the EC2 console

          Data encryption :
            Amazon EBS encryption uses AWS Key Management Service (AWS KMS) .
            The snapshots created from this encrypted EBS is also encrypted.

          Snapshots : use Amazon Data Lifecycle Manager
            Ebs -> snapshots -> create ->
                                          either from a volume  --> Single volume snapshot
                                          or instance --> Multi volume snapshot [ as instance contain more than 1 volume attached ]

            Thevolume does not need to be attached to a running instance in order to take a snapshot.
            Snapshots of encrypted EBS volumes are automatically encrypted.
            When you create a new volume from a snapshot, it's an exact copy of the original volume at the timethe snapshot was taken.
            Snapshots are incremental backups, meaning that only the blocks on the volume that have changedafter your most recent snapshot are saved.
            If you have a volume with 100 GiB of data, but only 5 GiB ofdata have changed since your last snapshot, only the 5 GiB of modified data is written to Amazon S3.

          Flexibility : ebs -> select a volume-> actions -> attach/detach
            EBS volumes support live configuration changes while in production. You can modify volume type,volume size, and IOPS capacity without service interruptions.

      Amazon Data Lifecycle Manager : SNAPSHOTS AUTOMATION  AWS -> Ebs -> life cycle manager

        You can use Amazon Data Lifecycle Manager to automate the creation, retention, and deletion of EBSsnapshots and EBS-backed AMIs.
        When you automate snapshot and AMI management, it helps you to ,
          •Protect valuable data by enforcing a regular backup schedule.
          •Create standardized AMIs that can be refreshed at regular intervals.
          •Retain backups as required by auditors or internal compliance.
          •Reduce storage costs by deleting outdated backups.
          •Create disaster recovery backup policies that back up data to isolated accounts.


=============================================
=============================================
  --> NETWORK BASED SERVICES : Private Network Connectors to AWS            - AWS DIRECT CONNECT
                               Edge Locations for S3 Enabled applications   - AMAZON S3 TRANSFER ACCELERATION

  --> ONLINE  DATA TRANSFER :  Load stream Data into S3                     - AMAZON KINESIS
                               Online transfer of active data               - AWS DATA SYNC
                               SFTP transfer to S3                          - AWS TRANSFER FOR SFTP
                               Database Transfer                            - AWS DATABSE MIGRATION
                               VM Image transfer - Live applications        - AWS SERVER MIGRATION SERVICES // AWS ENDURE


  --> OFFLINE DATA TRANSFER :  Ship Static data in and out of AWS           - AWS SNOW FAMILY [ Snow cone / Snow Ball / Snow Mobile ]

  --> HYBRID DATA STORAGE   :  Access AWS storage from On-premisis          - AWS STORAGE GATEWAY
