------------------------------
AMAZON SIMPLE STORAGE SERVICE :
------------------------------
#### S3 does not require region selection. --> S3 GLOBAL by default
#### But BUCKETS are region specific. Select the closest region available for faster access.

Amazon S3 provides a simple web service interface that you can use to store and retrieve any amount of data, at any time, from anywhere on the web.
Using this web service, you can easily build applications that make use of Internet storage.
Since Amazon S3 is highly scalable and you only pay for what you use, you can start small and grow your application as you wish, with no compromise on performance or reliability.
Amazon S3 is also designed to be highly flexible.
Store any type and amount of data that you want; read the same piece of data a million times or only for emergency disaster recovery; build a simple FTP application, or a sophisticated web application such as the Amazon.com retail web site.
Amazon S3 frees developers to focus on innovation instead of figuring out how to store their data.
Amazon S3 event notifications can be sent in response to actions in Amazon S3 like PUTs, POSTs, COPYs, or DELETEs. Notification messages can be sent through either Amazon SNS, Amazon SQS, or directly to AWS Lambda.

In S3, we can only host static websites, or static assets of a dynamic website (such as images, audio files, video files...etc).
A dynamic website relies on server-side processing and it uses server-side scripts such as PHP, JSP, or ASP.NET.
Amazon S3 does not support server-side scripting and cannot be used to host dynamic websites.
AWS has computing resources for hosting dynamic websites such as Amazon EC2 or Lambda.
Amazon S3 and Amazon EFS are storage services that scale automatically in storage capacity without any intervention to meet increased demand.


CHEAPER       :     lifecycle policies, reduced redundancy, Infrequent Access, Glacier
RESILIENT     :     versioning, s3 replication, regional replication
PERFORMANT    :     Cloud Front (for web), regional S3 endpoints, hashing key names
SECURE        :     VPC endpoints, encryption at rest

Bits          :     8 bits     = 1 byte
Bytes         : 		1024 bytes = 1 KB (1 to 3 digits)
Kilobytes     : 		1024 KB    = 1 MB (4 to 6 digits)
Megabytes     : 		1024 MB    = 1 GB (7 to 9 digits)
Gigabytes     : 		1024 GB    = 1 TB (10 to 12 digits)
Terabytes     : 		1024 TB    = 1 PB (13 to 15 digits)
Petabytes     : 		1024 PB    = 1 EB (16 to 18 digits)
Exabytes      : 		1024 EB    = 1 ZB (19 to 21 digits)
Zettabytes    : 		1024 ZB    = 1 YB (22 to 24 digits)

Amazon Simple Storage Service (Amazon S3) is storage for the internet.
You can use Amazon S3 to store and retrieve any amount of data at any time, from anywhere on the web.
Amazon S3 stores data as objects within buckets. An object is a file and any optional metadata that describes the file.
To store a file in Amazon S3, you upload it to a bucket. When you upload a file as an object, you can set permissions on the object and any metadata.
Buckets are containers for objects. You can have one or more buckets.
You can control access for each bucket, deciding who can create, delete, and list objects in it.
You can also choose the geographical Region where Amazon S3 will store the bucket and its contents and view access logs for the bucket and its objects.
Amazon S3 is a simple key-based object store. When you store data, you assign a unique object key that can later be used to retrieve the data. Keys can be any string

          Create a Bucket -> Upload a file -> Aws Adds metadata -> Aws creates an OBJECT [ file + metadata ]
          --> Before you can store data in Amazon S3, you must create a bucket.
          --> You are not charged for creating a bucket.
          --> You are charged only for storing objects in the bucket and for transferring objects in and out of the bucket.

          BUCKET = OBJECT 1 + OBJECT 2 + . . . . .
          --> customers can provision up to 100 buckets per AWS account.
          --> You can have a bucket that has different objects stored in S3 Standard, S3 Intelligent-Tiering, S3 Standard-IA, and S3 One Zone-IA.

          OBJECT = FILE + METADATA
          --> 1 object can range in size from a minimum of 0 bytes to a maximum of 5 terabytes
          --> But the number of objects per bucket is unlimited.
          --> The largest object that can be uploaded in a single PUT is 5 gigabytes.
          --> your objects are automatically stored across multiple devices spanning a minimum of three Availability Zones. (Except one zone IA)
          --> For S3 on Outposts, your data is stored in your Outpost on-premises environment, unless you manually choose to transfer it to an AWS Region.


S3 USES :
-----------
• Backup and storage  – Provide data backup and storage services for others.
• Application hosting – Provide services that deploy, install, and manage web applications.
• Media hosting       – Build a redundant, scalable, and highly available infrastructure that hosts video, photo, or music uploads and downloads.
• Software delivery   – Host your software applications that customers can download.

BUCKET :
-----------
Data is stored in key-value pairs .
KEY   : Sequence of UTF-8 chars up to 1024bytes long , it is the name of the file
VALUE : It is the actual data that you store .

You can use buckets to group related objects in the same way that you use a directory to group files in a file system.
By default, you can create up to 100 buckets in each of your AWS accounts.
If you need additional buckets, you can increase your account bucket quota to a maximum of 1,000 buckets by submitting a service quota increase.
    •  Bucket names must be between 3 and 63 characters long.
    •  After you create the bucket, you can't change its name.
    •  Bucket names can consist only of lowercase letters, numbers, dots (.), and hyphens (-).
    •  Bucket names must begin and end with a letter or number.
    •  Bucket names must not be formatted as an IP address (for example, 192.168.5.4).
    •  Bucket names can't begin with xn-- (for buckets created after February 2020).
    •  Bucket names must be unique within a partition.
       --> A partition is a grouping of Regions. AWS currently has three partitions.
           •   aws         (Standard Regions)
           •   aws-cn      (China Regions)
           •   aws-us-gov  (AWS GovCloud [US] Regions).

CREATE A BUCKET :
  --> Specify a unique name -->  region --> Public access --> Bucket versioning --> Encryption --> Object lock [ write once , read many , available if versioning is enabled]

UPLOAD AN OBJECT :
  --> Select Bucket --> Upload [ drag/drop or Add files/folders ] --> storage class -> Encryption --> ACL [ Permissions for other users / Accounts ] --> Upload

ACTIONS ON OBJECTS :
  --> Select Bucket --> Select Object / Objects --> Actions [ open , copy , move, delete, download , rename, edit meta data/tags/encryption/storage class ]

DELETE BUCKET :
  --> If you plan to delete your bucket, you must first empty your bucket, which deletes all the objects in the bucket.
  --> Buckets --> select Bucket --> top --> first EMPTY --> then DELETE

SERVER SIDE ENCRYPTION :
  --> Amazon Simple storage Service key (SSE-S3 ) : encryption key that Amazon S3 creates, manages, and uses for you.
  --> Amazon Key Management Service key (SSE-KMS) : An encryption key protected by AWS Key Management Service (AWS KMS).

--> Select a bucket and check all options , Under MANAGEMENT we can write rules to replicate bucket to another region/account etc;

OBJECT :
----------
                    --------------------------------------------------------------
                    |                   GLOBLALLY UNIQUE KEY                     |
                    --------------------------------------------------------------
                    |  META DATA                |       DATA                     |
                    --------------------------------------------------------------
                    |  ACCESS CONTROL           |       HTML                     |
                    |  FILE TYPE                |       CSS                      |
                    |  TAGS                     |       IMAGES                   |
                    |  SIZE                     |       VEDIOS                   |
                    |  CEATION DATE             |       EXECUTABLES              |
                    |                           |                                |
                    --------------------------------------------------------------

OBJECT : Photos/puppy.jpg
BUCKET : Dileep
URL    : http://Dileep.s3.aws.com/photos/puppy.jpg

PRODUCT DATA AT REST :
--------------------------
Amazon S3 provides a number of security features for the protection of data at rest, which you can use or not depending on your threat profile.

1- Permissions:
   Use bucket-level or object-level permissions alongside IAM policies to protect resources from unauthorized access and to prevent information disclosure, data integrity compromise or deletion.

2- Versioning:
  Amazon S3 supports object versions. Versioning is disabled by default.
  Enable versioning to store a new version for every modified or deleted object from which you can restore compromised objects if necessary.

3- Replication:
   Although Amazon S3 stores your data across multiple geographically diverse Availability Zones by default, compliance requirements might dictate that you store data at even greater distances.
   Cross-region replication (CRR) allows you to replicate data between distant AWS Regions to help satisfy these requirements.
   CRR enables automatic, asynchronous copying of objects across buckets in different AWS Regions.

4- Encryption – server side:
  Amazon S3 supports server-side encryption of user data. Server-side encryption is transparent to the end user.
  AWS generates a unique encryption key for each object, and then encrypts the object using AES-256.

5- Encryption – client side:
  With client-side encryption you create and manage your own encryption keys. Keys you create are not exported to AWS in clear text.
  Your applications encrypt data before submitting it to Amazon S3, and decrypt data after receiving it from Amazon S3.
  Data is stored in an encrypted form, with keys and algorithms only known to you.

6- AWS MACIE :
  AWS also provides a fully managed security service called AWS Macie to help protect your sensitive data in Amazon S3.
  Amazon Macie uses machine learning to automatically discover, classify, and protect sensitive data in Amazon S3.
  Amazon Macie recognizes sensitive data such as personally identifiable information (PII) or intellectual property, and provides you with dashboards and alerts that give visibility into how this data is being accessed or moved.
  The fully managed service continuously monitors data access activity for anomalies, and generates detailed alerts when it detects risk of unauthorized access or inadvertent data leaks.
  Today, Amazon Macie is available to protect data stored in Amazon S3, with support for additional AWS data stores coming later this year.


REPLICATION IN S3 :
-------------------------
Replication enables automatic, asynchronous copying of objects across Amazon S3 buckets.
Buckets that are configured for object replication can be owned by the same AWS account or by different accounts.
Object may be replicated to a single destination bucket or multiple destination buckets.
Destination buckets can be in different AWS Regions or within the same Region as the source bucket.

To enable object replication, you add a replication configuration to your source bucket. The minimum configuration must provide the following:
--> The destination bucket or buckets where you want Amazon S3 to replicate objects
--> An AWS Identity and Access Management (IAM) role that Amazon S3 can assume to replicate objects on your behalf

Replication can help you do the following:
------------------------------------------
   --> Replicate objects while retaining metadata :
   You can use replication to make copies of your objects that retain all metadata, such as the original object creation time and version IDs.
   This capability is important if you need to ensure that your replica is identical to the source object.

   --> Replicate objects into different storage classes :
   You can use replication to directly put objects into S3 Glacier, S3 Glacier Deep Archive, or another storage class in the destination buckets.
   You can also replicate your data to the same storage class and use lifecycle policies on the destination buckets to move your objects to a colder storage class as it ages.

   --> Maintain object copies under different ownership :
   Regardless of who owns the source object, you can tell Amazon S3 to change replica ownership to the AWS account that owns the destination bucket.
   This is referred to as the owner override option. You can use this option to restrict access to object replicas.

   --> Keep objects stored over multiple AWS Regions :
   You can set multiple destination buckets across different AWS Regions to ensure geographic differences in where your data is kept.
   This could be useful in meeting certain compliance requirements.

   --> Replicate objects within 15 minutes :
   You can use S3 Replication Time Control (S3 RTC) to replicate your data in the same AWS Region or across different Regions in a predictable time frame.
   S3 RTC replicates 99.99 percent of new objects stored in Amazon S3 within 15 minutes (backed by a service level agreement).

 WHEN TO USE CROSS_REGION REPLICATION :
 -----------------------------------------
   S3 Cross-Region Replication (CRR) is used to copy objects across Amazon S3 buckets in different AWS Regions. CRR can help you do the following-
     --> Meet compliance requirements :
         Although Amazon S3 stores your data across multiple geographically distant Availability Zones by default, compliance requirements might dictate that you store data at even greater distances. Cross-Region Replication allows you to replicate data between distant AWS Regions to satisfy these requirements.
     --> Minimize latency :
         If your customers are in two geographic locations, you can minimize latency in accessing objects by maintaining object copies in AWS Regions that are geographically closer to your users.
     --> Increase operational efficiency :
         If you have compute clusters in two different AWS Regions that analyze the same set of objects, you might choose to maintain object copies in those Regions.

 WHEN TO USE SAME_REGION REPLICATION :
 -----------------------------------------
   Same-Region Replication (SRR) is used to copy objects across Amazon S3 buckets in the same AWS Region. SRR can help you do the following-
     --> Aggregate logs into a single bucket :
         If you store logs in multiple buckets or across multiple accounts, you can easily replicate logs into a single, in-Region bucket. This allows for simpler processing of logs in a single location.
     --> Configure live replication between production and test accounts :
         If you or your customers have production and test accounts that use the same data, you can replicate objects between those multiple accounts, while maintaining  object metadata.
     --> Abide by data sovereignty laws :
         You might be required to store multiple copies of your data in separate AWS accounts within a certain Region. Same-Region Replication can help you automatically replicate critical data when compliance regulations don't allow the data to leave your country.

 REQUIREMENTS FOR REPLICATION :
 ------------------------------
 --> The source bucket owner must have the source and destination AWS Regions enabled for their account.
 --> The destination bucket owner must have the destination Region-enabled for their account.
 --> Both source and destination buckets must have versioning enabled.
 --> Amazon S3 must have permissions to replicate objects from the source bucket to the destination bucket or buckets on your behalf.
 --> If the owner of the source bucket doesn't own the object in the bucket, the object owner must grant the bucket owner READ and READ_ACP permissions with the object access control list (ACL).
 --> If the source bucket has S3 Object Lock enabled, the destination buckets must also have S3 Object Lock enabled.
 --> To enable replication on a bucket that has Object Lock enabled, contact AWS Support.

 If you are setting the replication configuration in a cross-account scenario, where source and destination buckets are owned by different AWS accounts, the following additional requirement applies:
  --> The owner of the destination buckets must grant the owner of the source bucket permissions to replicate objects with a bucket policy.
  --> The destination buckets cannot be configured as Requester Pays buckets.


STORAGE CLASSES : https://aws.amazon.com/s3/storage-classes/
-------------------
1. FOR FREQUENTLY ACCESSED OBJECTS :
   For performance-sensitive use cases (those that require millisecond access time) and frequently accessed data, Amazon S3 provides the following storage classes:

    --> S3 STANDARD         —  The default storage class If you don't specify the storage class when you upload an object, Amazon S3 assigns the S3 Standard storage class.
    --> REDUCED REDUNDANCY  —  The Reduced Redundancy Storage (RRS) storage class is designed for noncritical, reproducible data that can be stored with less redundancy than the S3 Standard storage class.
                               The reduced redundancy storage class has the lowest durability of all the storage classes.
                               This means object stored in this storage class have the highest probability of being lost.
                               So you should only store objects in this storage class if they can be easily reproduced.
                               In exchange for the lower durability, the cost is lower than the standard storage class.

    It is appropriate for cloud applications, dynamic websites, content distribution, mobile and gaming applications, and big data analytics.
    --> Low latency and high throughput performance
    --> Designed for durability of 99.999999999% of objects across multiple Availability Zones
    --> Designed for 99.99% availability over a given year.

2. FOR AUTOMATICALLY OPTIMIZING DATA WITH CHANGING PATTERNS :
   The only cloud storage class that delivers automatic cost savings by moving objects to the most cost-effective access tier between four access tiers when access patterns change.
   S3 Intelligent Tiering is an Amazon S3 storage class designed to optimize storage costs by automatically moving data to the most cost-effective access tier, without operational overhead.
   There are no retrieval fees when using the S3 Intelligent-Tiering storage class, and no additional tiering fees when objects are moved between access tiers within S3 Intelligent-Tiering.

   It works by storing objects in the remaining four access tiers.
    --> two low latency access tiers optimized for frequent and infrequent access, and
    --> two opt-in archive access tiers designed for asynchronous access that are optimized for rare access.

   S3 Intelligent-Tiering works by monitoring access patterns and then moving the objects that have not been accessed in " N " consecutive days
    --> Initially Objects that are uploaded or transitioned to S3 Intelligent-Tiering are automatically stored in the Frequent Access tier.
    --> 30 DAYS  - to the Infrequent Access tier.
    --> 90 DAYS  - to the Archive Access tier,
    --> 180 DAYS - to the Deep Archive Access tier.
    --> If the objects are accessed later, the objects are moved back to the Frequent Access tier.

   It is ideal for new applications and Data lakes.
    --> Designed for durability of 99.999999999% of objects across multiple Availability Zones
    --> Designed for 99.9% availability over a given year

3. FOR INFREQUENTLY ACCESSED OBJECTS :
   The S3 Standard-IA and S3 One Zone-IA storage classes are designed for long-lived and infrequently accessed data. (IA stands for infrequent access.)
   S3 Standard-IA is for data that is accessed less frequently, but requires rapid access when needed.
   S3 Standard-IA offers the high durability, high throughput, and low latency of S3 Standard, with a low per GB storage price and per GB retrieval fee.

   ## IA means  INFREQUENT ACCESS .
   --> S3 STANDARD-IA : Amazon S3 stores the object data redundantly across multiple geographically separated Availability Zones (similar to the S3 Standard storage class).
       S3 Standard-IA objects are resilient to the loss of an Availability Zone.
       This storage class offers greater availability and resiliency than the S3 One Zone-IA class.

   It is ideal for long-term storage, backups, and as a data store for disaster recovery files.
   --> Designed for durability of 99.999999999% of objects across multiple Availability Zones
   --> Designed for 99.9% availability over a given year

   ## IA means  INFREQUENT ACCESS .
   --> S3 ONE ZONE-IA : It is for data that is accessed less frequently, but requires rapid access when needed.
       Unlike other S3 Storage Classes which store data in a minimum of three Availability Zones (AZs), S3 One Zone-IA stores data in a single AZ and costs 20% less than S3 Standard-IA.
       So it is not resistant against any natural disaster like earthquakes or cyclones as the data is not replicated .
       Data stored in this storage class will be lost in the event of Availability Zone destruction.

  It is ideal for customers who want a lower-cost option for infrequently accessed data but do not require the availability and resilience of S3 Standard or S3 Standard-IA.
  It’s a good choice for storing secondary backup copies of on-premises data or easily re-creatable data.
  --> Designed for durability of 99.999999999% of objects in a single Availability Zone.
  --> Designed for 99.5% availability over a given year.


4. FOR ARCHIVING OBJECTS :
   --> S3 GLACIER : Has a minimum storage duration period of 90 days and can be accessed in as little as 1-5 minutes using expedited retrieval.
       If you have deleted, overwritten, or transitioned to a different storage class an object before the 90-day minimum, you are charged for 90 days.
       S3 Glacier is a secure, durable, and low-cost storage class for data archiving.
       You can reliably store any amount of data at costs that are competitive with or cheaper than on-premises solutions.
       To keep costs low yet suitable for varying needs, S3 Glacier provides three retrieval options that range from a few minutes to hours.
       With Amazon Glacier, customers can reliably store their data for as little as $0.004 per gigabyte per month.

   Ideal for long-term archive.
   --> Designed for durability of 99.999999999% of objects across multiple Availability Zones
   --> Configurable retrieval times, from minutes to hours.

  --> S3 GLACIER DEEP ARCHIVE : Has a minimum storage duration period of 180 days and a default retrieval time of 12 hours.
      S3 Glacier Deep Archive is the lowest cost storage option in AWS and supports long-term retention and digital preservation for data that may be accessed once or twice in a year.
      S3 Glacier Deep Archive complements Amazon S3 Glacier, which is ideal for archives where data is regularly retrieved and some of the data may be needed in minutes.

  Ideal for those in highly-regulated industries, such as the Financial Services, Healthcare, and Public Sectors — that retain data sets for 7-10 years or longer to meet regulatory compliance requirements.
  --> Designed for durability of 99.999999999% of objects across multiple Availability Zones.
  --> Retrieval time within 12 hours.
  --> Lowest cost storage class designed for long-term retention of data that will be retained for 7-10 years.

5. OTHER - OUTPOSTS :
    Amazon S3 on Outposts delivers object storage to your on-premises AWS Outposts environment.
    If you have data residency requirements that can’t be met by an existing AWS Region, you can use the S3 OUTPOSTS storage class to store your S3 data on-premises.
    S3 on Outposts provides a single Amazon S3 storage class, named S3 Outposts, which uses the S3 APIs, and is designed to durably and redundantly store data across multiple devices and servers on your Outposts.
    S3 Outposts storage class is ideal for workloads with local data residency requirements, and to satisfy demanding performance needs by keeping data close to on-premises applications.

KEY POINTS FOR STORAGE CLASSES :
S3 Intelligent-Tiering charges a small tiering fee and has a minimum eligible object size of 128KB for auto-tiering.
Smaller objects may be stored but will always be charged at the Frequent Access tier rates.

Amazon S3 also offers capabilities to manage your data throughout its lifecycle.
Once an S3 Lifecycle policy is set, your data will automatically transfer to a different storage class without any changes to your application.

Standard retrievals in archive access tier and deep archive access tier are free.
Using the S3 console, you can pay for expedited retrievals if you need faster access to your data from the archive access tiers.

S3 Intelligent-Tiering first byte latency for frequent and infrequent access tier is milliseconds access time, and
the archive access and deep archive access tiers first byte latency is minutes or hours.

-------------------------------------------------------------------------------------------------------------------------------------------------------------
                                S3 Standard 	      S3_Intelligent-      	S3_Standard-IA 	       S3_One_Zone-IA	        S3_Glacier     	    S3_Glacier-
                                                       Tiering                                                                              Deep_Archive
-------------------------------------------------------------------------------------------------------------------------------------------------------------

Designed for durability 	      11 9’s | 99.99%       11 9’s                  11 9’s                11 9’s                11 9’s               11 9’s
   (99.999999999%)                     | Reduced Redundancy

Designed for availability     	99.99% 	              99.9% 	                99.9% 	               99.5% 	              99.99% 	             99.99%

Availability SLA 	              99.9% 	              99% 	                  99% 	                 99% 	                99.9%                99.9%

Availability Zones 	            ≥3 	                  ≥3 	                    ≥3                    	1                 	≥3                    ≥3

Minimum capacity  	            N/A                 	N/A                   	128KB 	               128KB 	              40KB 	               40KB
charge per object

Minimum storage  	              N/A               	30 days                 	30 days              	30 days               90 days             180 days
duration charge

Retrieval fee 	                N/A	                 N/A               	per GB retrieved      	per GB retrieved 	     per GB retrieved  	 per GB retrieved

First byte latency 	       milliseconds         	milliseconds           	milliseconds 	           milliseconds   	select minutes or hours 	select hours

Storage Type &                 YES                   YES                       YES                   YES                    YES                   YES
Life Cycle Transition
---------------------------------------------------------------------------------------------------------------------------------------------------------------------

STORAGE PRICES : https://aws.amazon.com/s3/pricing/
======================
There is no Data Transfer charge for data transferred within an Amazon S3 Region via a COPY request.
Data transferred via a COPY request between AWS Regions is charged at rates specified in the pricing section of the Amazon S3 detail page.
There is no Data Transfer charge for data transferred between Amazon EC2 and Amazon S3 within the same region, for example, data transferred within the US East (Northern Virginia) Region.
However, data transferred between Amazon EC2 and Amazon S3 across all other regions is charged at rates specified on the Amazon S3 pricing page

S3 Standard - General purpose storage for any type of data, typically used for frequently accessed data	 :
First 50 TB / Month	                                                              $0.023 per GB
Next 450 TB / Month                                                             	$0.022 per GB
Over 500 TB / Month	                                                              $0.021 per GB

S3 Intelligent - Tiering * - Automatic cost savings for data with unknown or changing access patterns	:
Frequent Access Tier, First 50 TB / Month	                                        $0.023 per GB
Frequent Access Tier, Next 450 TB / Month	                                        $0.022 per GB
Frequent Access Tier, Over 500 TB / Month	                                        $0.021 per GB
Infrequent Access Tier, All Storage / Month	                                      $0.0125 per GB
Archive Access Tier, All Storage / Month	                                        $0.004 per GB
Deep Archive Access Tier, All Storage / Month	                                    $0.00099 per GB
Monitoring and Automation, All Storage / Month	                                  $0.0025 per 1,000 objects

S3 Standard - Infrequent Access * - For long lived but infrequently accessed data that needs millisecond access	:
All Storage / Month	                                                              $0.0125 per GB

S3 One Zone - Infrequent Access * - For re-creatable infrequently accessed data that needs millisecond access	:
All Storage / Month                                                             	$0.01 per GB

S3 Glacier ** - For long-term backups and archives with retrieval option from 1 minute to 12 hours	:
All Storage / Month                                                             	$0.004 per GB

S3 Glacier Deep Archive ** - For long-term data archiving that is accessed once or twice in a year and can be restored within 12 hours :
All Storage / Month	                                                              $0.00099 per GB

EXAMPLE PRICING FOR STORAGE :
--------------------------------
Assume you store 100GB (107,374,182,400 bytes) of data in Amazon S3 Standard in your bucket for 15 days in March, and 100TB (109,951,162,777,600 bytes) of data in Amazon S3 Standard for the final 16 days in March.
At the end of March, you would have the following usage in Byte-Hours:
Total Byte-Hour usage = [107,374,182,400 bytes x 15 days x (24 hours / day)] + [109,951,162,777,600 bytes x 16 days x (24 hours / day)]
                      = 42,259,901,212,262,400 Byte-Hours.

Let's convert this to GB-Months: 42,259,901,212,262,400 Byte-Hours / 1,073,741,824 bytes per GB / 744 hours per month = 52,900 GB-Months

This usage volume crosses two different volume tiers.
The monthly storage price is calculated below assuming the data is stored in the US East (Northern Virginia) Region:
-->  50 TB Tier: 51,200 GB x $0.023 = $1,177.60 50 TB to 450 TB Tier: 1,700 GB x $0.022 = $37.40
-->  Total Storage Fee = $1,177.60 + $37.40 = $1,215.00

REQUEST & DATA RETRIEVAL PRICING :  https://aws.amazon.com/s3/pricing/
-------------------------------------
You pay for requests made against your S3 buckets and objects.
S3 request costs are based on the request type, and are charged on the quantity of requests.
When you use the Amazon S3 console to browse your storage, you incur charges for GET, LIST, and other requests that are made to facilitate browsing.
Charges are accrued at the same rate as requests that are made using the API/SDK.
Reference the S3 developer guide for technical details on the following request types: PUT, COPY, POST, LIST, GET, SELECT, Lifecycle Transition, and Data Retrievals.
DELETE and CANCEL requests are free.
LIST requests for any storage class are charged at the same rate as S3 Standard PUT, COPY, and POST requests.
You pay for retrieving objects that are stored in S3 Standard – Infrequent Access, S3 One Zone – Infrequent Access, S3 Glacier, and S3 Glacier Deep Archive storage.
Reference the S3 developer guide for technical details on Data Retrievals.

DATA TRANSFER PRICING : https://aws.amazon.com/s3/pricing/
-------------------------
You pay for all bandwidth into and out of Amazon S3, except for the following:
• Data transferred in from the internet.
• Data transferred out to an Amazon Elastic Compute Cloud (Amazon EC2) instance, when the instance is in the same AWS Region as the S3 bucket (including to a different account in the same AWS region).
• Data transferred out to Amazon CloudFront (CloudFront).

The pricing below is based on data transferred "in" and "out" of Amazon S3 (over the public Internet).
Transfers between S3 buckets or from Amazon S3 to any service(s) within the same AWS Region are free.
You also pay a fee for any data transferred using Amazon S3 Transfer Acceleration

MANAGEMENT & ANALYTICS PRICING : https://aws.amazon.com/s3/pricing/
---------------------------------
You pay for the storage management features and analytics (Amazon S3 Inventory, S3 Storage Class Analysis, S3 Storage Lens, and S3 Object Tagging) that are enabled on your account’s buckets.
For pricing on Amazon CloudWatch Metrics, visit the Amazon CloudWatch pricing page.

S3 REPLICATION PRICING : https://aws.amazon.com/s3/pricing/
-----------------------------
For S3 Replication (Cross-Region Replication and Same Region Replication), you pay the S3 charges for storage in the selected destination S3 storage classes, the storage charges for the primary copy, replication PUT requests, and applicable infrequent access storage retrieval fees.
For CRR, you also pay for inter-region Data Transfer OUT from S3 to each destination region.
When you use S3 Replication Time Control, you also pay a Replication Time Control Data Transfer fee and S3 Replication Metrics charges that are billed at the same rate as Amazon CloudWatch custom metrics.
Storage and PUT request pricing for the replicated copy is based on the selected destination AWS Regions, while pricing for inter-region data transfers is based on the source AWS Region.

              S3 Replication Time Control data transfer -	$0.015 per GB



STATIC WEBSITE HOSTING :
=============================
STEP 1 : In route53 , register for a domain , say example.com

STEP 2 :

Create a Bucket : example.com

  Bucket name should be similar to the domain name as in example.common
  Select the bucket and then check for PROPERTIES .
  Move to bottom of the page and enable static website hosting .
  --> IN the options , select  HOST A STATIC WEBSITE
  We should make this bucket public.
  Specify the index and error pages .
  You'll create an HTML file and upload it to your bucket for an index page.

Create another Bucket : www.example.com

  If you also want your users to be able to use www.your-domain-name, such as www.example.com, toaccess your sample website, you create a second S3 bucket.
  You then configure the second bucket toroute traffic to the first bucket.

  Once the bucket www.example.com is created ,
  go to permissions -> enable static website HOSTING
  --> IN the options , select REDIRECT REQUESTS FOR AN OBJECT
  Enter the target bucket [above bucket example.com]
  Select http protocol as S3 do not support https.

STEP 3:
  create a html page and save it as index.html .
  upload the page into the bucket.

STEP 4 :
  Route DNS traffic for your domain to yourwebsite bucket
  Goto Route53 -> Hosted zones -> if you register a domain , a hosted zone is created automatically.
  chose your domain in hosted zones
  chose create record  -> record contains information about how you want to route traffic for one domain
  Choose Simple routing and choose Next
  Choose Define simple record
    --> specify RECORD NAME --> Accept the default value, which is the name of your hostedzone and your domain. This will route internet traffic to the bucket that has the same name asyour domain.
    --> Repeat the process for www.example.com and other sub domains.
    --> specify VALUE/ROUTE to Choose Alias to S3 website endpoint, then choose the AWS Region that the bucket was createdin
    For the first record that you create, choose the bucket that has the same name as your hostedzone and your domain.
    For the second record, choose the bucket that has the name www.your-domain-name.
    --> specify RECORD TYPE --> Accept the default value of A – Routes traffic to an IPv4 address and some AWS resources
  Chose define simple records
  chose create record.

  STEP 5 :
    To verify that the website is working correctly, open a web browser and browse to the following URLs:
      •  http://your-domain-name – Displays the index document in the your-domain-name bucket
      •  http://www.your-domain-name – Redirects your request to the your-domain-name bucket


AWS S3 TRANSFER ACCELERATION : // The service is a paid service.
-----------------------------------
If you’re transferring your data to popular AWS storage platform S3 over long distances, AWS S3 Transfer Acceleration helps you do it faster 171% faster on average, according to AWS.
Amazon S3 Transfer Acceleration is a service that allows you to upload data to an S3 bucket quickly and securely over the public Internet.
If you’re uploading to a centralized bucket from different locations across the globe, S3 Transfer Acceleration can save a lot of transfer time.
Data is routed to S3 on optimized network paths via Amazon CloudFront edge locations, which are spread across the globe.
This helps maximize available bandwidth no matter how far the data is travelling or how much the latency varies.
There are no special clients or proprietary network protocols involved either.
Just turn on Transfer Acceleration for an S3 bucket using the Amazon S3 console, set your S3 endpoint to one of two TA options, and the acceleration is applied automatically.
The service is available for both reading and writing data to Amazon S3, making it useful for recurring jobs like media uploads, backups, and local data processing.

For your bucket to work with transfer acceleration, the bucket name must conform to DNS naming requirements and must not contain periods (".").

  WHY USE --> You might want to use Transfer Acceleration on a bucket for various reasons, including the following:
          --> You have customers that upload to a centralized bucket from all over the world.
          --> You transfer gigabytes to terabytes of data on a regular basis across continents.
          --> You are unable to utilize all of your available bandwidth over the Internet when uploading to Amazon S3.



==============================
OTHER STORAGE OPTIONS IN AWS
==============================

AWS STORAGE GATEWAY : https://www.youtube.com/watch?v=Spzdj1NUJbA -> VV.IMP --> Watch it
-----------------------
The AWS Storage Gateway is a service connecting an on-premises software appliance with cloud-based storage to provide seamless and secure integration
between an organization's on-premises IT environment and AWS's storage infrastructure.

AWS Storage Gateway is a hybrid cloud storage service that gives you on-premises access to virtually unlimited cloud storage.
Storage Gateway provides a standard set of storage protocols such as iSCSI, SMB, and NFS, which allow you to use AWS storage without rewriting your existing applications.
Storage Gateway enables you to reduce your on-premises storage footprint and associated costs by leveraging Amazon S3 cloud storage.

    USECASE :
    -----------
    (1) Move backups and archives to the cloud,
    (2) Reduce on-premises storage with cloud-backed file shares, and
    (3) Provide on-premises applications low latency access to data stored in AWS.

ACTIVATION :
--------------
You use the AWS Management Console to download the virtual appliance gateway or purchase the hardware appliance, configure storage, and manage and monitor the service.
The gateway connects your applications to AWS storage by providing standard storage interfaces.
It provides transparent caching, efficient data transfer, and integration with AWS monitoring and security services.
Once you’ve installed your gateway, you associate it with your AWS Account through our activation process.
After activation, you configure the gateway to connect to the appropriate storage type.
For File Gateway, you configure file shares that are mapped to selected S3 buckets, using IAM roles.
For Volume Gateway, you create and mount volumes as iSCSI devices.
For Tape Gateway, you connect your backup application to create and manage tapes.
Once configured, you start using the gateway to write and read data to and from AWS storage. You can monitor the status of your data transfer and your storage interfaces through the AWS Management Console. Additionally, you can use the API or SDK to programmatically manage your application’s interaction with the gateway.

Storage Gateway offers three different types of gateways – File Gateway, Tape Gateway, and Volume Gateway
that seamlessly connect on-premises applications to cloud storage, caching data locally for low-latency access.
Your applications connect to the service through a virtual machine or gateway hardware appliance using standard storage protocols, such as NFS, SMB, and iSCSI.
The gateway connects to AWS storage services, such as Amazon S3, Amazon S3 Glacier, Amazon S3 Glacier Deep Archive, Amazon EBS, and AWS Backup, providing storage for files, volumes, snapshots, and virtual tapes in AWS.

                                                                 ---------------------------------------------------> CAN BE :
                                                                |                                          |          1. An on-premises VM
                          -----------                       STORAGE-GW                      -----------    |          2. A hardware appliance
          S3             |            |                         --                         |           |   |          3. An EC2 instance in EC2
          Glacier        |    AWS     | <--------------------> |  | <--------------------> |   SITE    |   |
          Glacier Deep   |            |                         --                         |           |   ---------> CAN BE :
          EBS, Backup     -----------                                                       -----------               1. FILE GW -> NFS / SMB , GW deployed on a VM running on Hyper-V / ESXi / KVM
                       AWS SSTORAGE  <------------ TAKES FREQUENT SNAPSHOTS -----------> ON PREMISES IT SERVICES      2. VOLUME GW -> Cached - Stored
                                                                                                                      3. TAPE GW -> GW deployed on a VM running on Hyper-V / ESXi / KVM


    File Gateway :  A file gateway supports a file interface into Amazon Simple Storage Service (AmazonS3) and combines a service and a virtual software appliance.
    By using this combination, you can store and retrieve objects in Amazon S3 using industry-standard file protocols such as Network File System(NFS) and Server Message Block (SMB).
    The software appliance, or gateway, is deployed into your on-premises environment as a virtual machine (VM) running on VMware ESXi, Microsoft Hyper-V, or Linux Kernel-based Virtual Machine (KVM) hypervisor.
    The gateway provides access to objects in S3 as files or file share mount points.

    // Server Messaging protocol (SMB) is the native file sharing protocol implemented in Windows systems.
       The Network File System (NFS) protocol is used by Linux systems to share files and folders.

    Volume Gateway : A volume gateway provides cloud-backed storage volumes that you can mount as Internet Small Computer System Interface (iSCSI) devices from your on-premises application servers.
    The volume gateway is deployed into your on-premises environment as a VM running on VMware ESXi,KVM, or Microsoft Hyper-V hypervisor.
    Your on-premises applications can access these as Internet Small Computer System Interface (iSCSI)targets
    The gateway supports the following volume configurations -
      • Cached volumes : You store your data in Amazon Simple Storage Service (Amazon S3) and retain a copy of frequently accessed data subsets locally.
      • Stored volumes : If you need low-latency access to your entire dataset, first configure your on-premises gateway to store all your data locally.
                         Then asynchronously back up point-in-time snapshots of this data to Amazon S3.

      **In either mode, you can take point-in-time snapshots of your volumes, which are stored as Amazon EBS Snapshots in AWS, enabling you to make space-efficient versioned copies of your volumes.
        Used for data protection, recovery, migration and various other copy data needs.

   Tape Gateway : A tape gateway provides cloud-backed virtual tape storage.
   The tape gateway is deployed into your on-premises environment as a VM running on VMware ESXi, KVM, or Microsoft Hyper-V hypervisor.
   With a tape gateway, you can cost-effectively and durably archive backup data in GLACIER or DEEP_ARCHIVE.

You can run AWS Storage Gateway either on-premises as a VM appliance, as a hardware appliance, or in AWS as an Amazon EC2 instance.


AWS SNOW-FAMILY:
-----------------
  -> Snow cone        - https://www.youtube.com/watch?v=X_8LM7E_hiE      -- 8 TB storage   --> 1 SMALL BOX [ uses opshub application for data transfer ]
     Snow Ball        - https://www.youtube.com/watch?v=9uc2DSZ1wL8&t=9s -- 80 TB storage  --> 1 SUITCASE  [ Snowball is a petabyte-scale data transport solution by cascading snow balls ]
     Snow Ball edge   - https://www.youtube.com/watch?v=bxSD1Nha2k8      -- 100 TB storage --> 1 SUITCASE  [ Embedded Ec2 - Lambda for running jobs in remote areas like ships , hills for an immediate results. ]
     Snow Mobile      - https://www.youtube.com/watch?v=8vQmTZTq7nw      -- 1 Exabyte      --> 1 Truck     [ AWS Snowmobile is the exabyte-scale data migration service ]
                                                                            [100Pb per snow mobile]

    1 Exabyte = 1000 petabytes
    Snow ball can be connected in series to have a Petabyte's storage  = 1 million GB = 1024 Terabytes


AWS SNOWCONE : -  (8.94” x 5.85” x 3.25” / 227 mm x 148.6 mm x 82.65 mm),  Weighs about 4.5 lbs. (2 kg) ,run on a battery for up to approximately 6 hours
----------------
AWS Snowcone is a portable, rugged, and secure device for edge computing and data transfer.
You can use Snowcone to collect, process, and move data to AWS, either offline by shipping the device to AWS, or online by using AWS DataSync.
The Snowcone device supports data transfer from on-premises Windows, Linux, and macOS servers and file-based applications through the NFS interface.
Supports up to Dozens of TB. It can be used in space-constrained environments where Snowball Edge devices don't fit.
    •  For edge computing applications, to collect data, process the data to gain immediate insight, and then transfer the data online to AWS.
    •  To transfer data that is continuously generated by sensors or machines online to AWS in a factory or at other edge locations.
    •  To distribute media, scientific, or other content from AWS storage services to your partners and customers.
    •  To aggregate content by transferring media, scientific, or other content from your edge locations to AWS.
    •  For one-time data migration scenarios where your data is ready to be transferred, where Snowcone offers a simple, quick, and low-cost way to transfer up to 8 TB of data into AWS by shipping the deviceback to AWS.


AWS SNOWBALL & EDGE : Edge is an extension to snowball
-----------------------
Snowball is a petabyte-scale data transport solution that uses secure appliances to transfer large amounts of data into and out of the AWS cloud.
Using Snowball addresses common challenges with large-scale data transfers including high network costs, long transfer times, and security concerns.
AWS Snowball is a service that provides secure, rugged devices, so you can bring AWS computing and storage capabilities to your edge environments, and transfer data into and out of AWS.
Now the AWS Snowball service operates with Snowball Edge devices, which include on-board computing capabilities as well as storage.

You can use Snowball Edge Storage Optimized if you have a large backlog of data to transfer or if you frequently collect data that needs to be transferred to AWS and your storage is in an area where high-bandwidth internet connections are not available or cost-prohibitive.
You can also use Snowball Edge to run edge computing workloads, such as performing local analysis of data on a Snowball Edge cluster and writing it to the S3-compatible endpoint. You can streamline it into existing workflows leveraging built-in capabilities such as the NFS file interface and migrate files to the device while maintaining file metadata.
Snowball Edge can operate in remote locations or harsh operating environments, such as factory floors, oil and gas rigs, mining sites, hospitals, and on moving vehicles. Snowball Edge is pre-configured and does not have to be connected to the internet, so processing and data collection can take place within isolated operating environments.
Snowball Edge is intended to serve as a data transport solution for moving high volumes of data into and out of a designated AWS Region. For use cases that require data transfer between AWS Regions, we recommend using S3 Cross-Region Replication as an alternative.

Snowball Edge is an edge computing and data transfer device provided by the AWS Snowball service.
It has on-board storage and compute power that provides select AWS services for use in edge locations. Snowball Edge comes in two options, Storage Optimized and Compute Optimized, to support local data processing and collection in disconnected environments such as ships, windmills, and remote factories.
Snowball is limited to 50 & 80Tb, Snowball Edge is limited to 100Tb but support a clustering feature.
consider it for large-scale data transfers and migrations when bandwidth is not available for use of a high-speed online transfer service, such as AWS DataSync.

The original Snowball devices were transitioned out of service and Snowball Edge Storage Optimized are now the primary devices used for data transfer. Snowballs are now out of order.
The Amazon Simple Storage Service(Amazon S3) buckets, data, and Amazon Elastic Compute Cloud (Amazon EC2) AMIs that you choose are automatically configured, encrypted, and pre-installed on your devices.
The AWS DataSync agent is also pre-installed before your devices are shipped to you.
you connect it to your on-premises network and set the IP address either manually or automatically with DHCP.
You must download and install AWS OpsHub for Snow Family, on a windows or mac laptop.
When data is transferred , you can ship it back .

How Pricing Works: Snowball pricing has four main cost components:
    (1) a service fee for each job you run,
    (2) data transfer fees from Amazon S3,
    (3) the shipping costs to transport a Snowball appliance to and from your address, and
    (4) the number of days you keep Snowball onsite.


GLACIER :
------------
Explained above .
Used for data archiving.

CLOUD FRONT :
---------------
Its a content delivery network . Similar to a cache.
Its stores the static pages that are frequently requested buy users .

Amazon EC2 instance store :
--------------------------------
Many instances can access storage from disks that are physically attached to the host computer.
Thisdisk storage is referred to as instance store.
Instance store provides temporary block-level storage forinstances.
The data on an instance store volume persists only during the life of the associated instance.
if you stop, hibernate, or terminate an instance, any data on instance store volumes is lost.


ELASTIC BLOCK STORE : AWS -> EC2 -> Elastic Block Store -> create a volume -> if you specify snapshot , you create EBS from a snapshot else a normal EBS .
-----------------------
Its a hard drive to EC2 instances .
Persists its data even after termination of an instance .
EBS can be connected to only one Instance at a time .
But one instance can be connected more than one EBS .

             -----------
            |            | <-------------------->   /_\   EBS-1
            |    EC2     |
            |            | <-------------------->   /_\   EBS-2
             -----------
Amazon EBS provides durable, block-level storage volumes that you can attach to a running instance.
You can use Amazon EBS as a primary storage device for data that requires frequent and granular updates.
For example, Amazon EBS is the recommended storage option when you run a database on an instance.
The volume persists independently from the running life of an instance. After an EBS volume is attached to an instance, you can use it like any other physical hard drive.
You can also detach an EBS volume from one instance and attach it to another instance. Y
You can dynamically change the configuration of a volume attached to an instance.
EBS volumes can also be created as encrypted volumes using the Amazon EBS encryption feature.
To keep a backup copy of your data, you can create a snapshot of an EBS volume, which is stored in Amazon S3.

EBS volumes are created in a specific Availability Zone, and can then be attached to any instances in that same Availability Zone.
To make a volume available outside of the Availability Zone, you can create a snapshot and restore that snapshot to a new volume anywhere in that Region.
You can copy snapshots to other Regions and then restore them to new volumes there.
You can increase the volumes of an instance at any time by attaching more EBS to it.

        Amazon EBS provides the following volume types: DEPENDS ON IPOS / THROUGHPUT / HDD / SDD
        --------------------------------------------------------------------------------------------
        A hard disk drive (HDD) is an old-school storage device that uses mechanical platters and a moving read/write head to access data.
        A solid-state drive (SSD) is a newer, faster type of device that stores data on instantly-accessible memory chips.

        IOPS measures the number of read and write operations per second, while throughput measures the number of bits read or written per second.
        You have 4 buckets (Disk blocks) of the same size that you want to fill or empty water.
        You'll be using a jug to transfer the water into the buckets. Now your question will be ,
            At a given time (per second), how many jugs of water can you pour(write) or withdraw (read)? This is IOPS
            At a given time (per second) what's the amount(bit, kb, mb, ect) of water the jug can transfer into/out of the bucket continuously? This is throughput.

          SOLID STATE DRIVES SSD : costly --> Low latency - High IOPS
              Used for BOOT volumes , transactional work loads [ enterprise apps , relational data bases, NOSQL data bases ]

              General Purpose SSD :
                General Purpose SSD volumes (gp2 and gp3) balance price and performance for a wide variety of transactional workloads.
                These volumes are ideal for a use cases such as boot volumes, medium-size single instance databases, and development and test environments.

              Provisioned IOPS SSD :
                Provides high performance for mission-critical, low-latency, or high-throughput workloads.
                Provisioned IOPS SSD volumes (io1 and io2) support up to 64,000 IOPS and 1,000 MiB/s of throughput.
                This enables you to predictably scale to tens of thousands of IOPS per EC2 instance.

         HARD DISK DRIVES HDD: Cheap --> Highest Throughput
              For Streaming IO , Big data , data ware housing , log processing applications
              Throughput Optimized HDD :
                Throughput Optimized HDD volumes (st1) provide low-cost magnetic storage that defines performance in terms of throughput rather than IOPS.
                These volumes are ideal for large, sequential workloads such as Amazon EMR, ETL, data warehouses, and log processing.

              Cold HDD :
                Cold HDD volumes (sc1) provide low-cost magnetic storage that defines performance in terms of throughput rather than IOPS.
                These volumes are ideal for large, sequential, cold-data workloads.
                If you require infrequent access to your data and are looking to save costs, these volumes provides inexpensive block storage.

        USES OF EBS :
        ---------------
          Data availability :
             These are  automatically replicated within its Availability Zone to prevent data loss due to failure of any single hardware component.
             You can attach an EBS volume to any EC2 instance in the same Availability Zone.

          Data persistence : --> Delete on Termination checkbox
            EBS volumes that are attached to a running instance can automatically detach from the instance with their data intact when the instance is terminated.
            if you uncheck the Delete on Termination checkbox when you configure EBS volumes for your instance on the EC2 console

          Data encryption :
            Amazon EBS encryption uses AWS Key Management Service (AWS KMS) .
            The snapshots created from this encrypted EBS is also encrypted.

          Snapshots : use Amazon Data Lifecycle Manager
            Ebs -> snapshots -> create ->
                                          either from a volume  --> Single volume snapshot
                                          or instance           --> Multi volume snapshot [ as instance contain more than 1 volume attached ]

            The volume does not need to be attached to a running instance in order to take a snapshot.
            Snapshots of encrypted EBS volumes are automatically encrypted.
            When you create a new volume from a snapshot, it's an exact copy of the original volume at the time the snapshot was taken.
            Snapshots are incremental backups, meaning that only the blocks on the volume that have changed after your most recent snapshot are saved.
            If you have a volume with 100 GiB of data, but only 5 GiB of data have changed since your last snapshot, only the 5 GiB of modified data is written to Amazon S3.

          Flexibility : ebs -> select a volume-> actions -> attach/detach
            EBS volumes support live configuration changes while in production. You can modify volume type, volume size, and IOPS capacity without service interruptions.

      Amazon Data Lifecycle Manager : SNAPSHOTS AUTOMATION  AWS -> Ebs -> life cycle manager

 DATA LIFECYCLE MANAGER :
 -------------------------
        You can use Amazon Data Lifecycle Manager to automate the creation, retention, and deletion of EBS snapshots and EBS-backed AMIs.
        When you automate snapshot and AMI management, it helps you to ,
          • Protect valuable data by enforcing a regular backup schedule.
          • Create standardized AMIs that can be refreshed at regular intervals.
          • Retain backups as required by auditors or internal compliance.
          • Reduce storage costs by deleting outdated backups.
          • Create disaster recovery backup policies that back up data to isolated accounts.


=============================================
=============================================
  --> NETWORK BASED SERVICES : Private Network Connectors to AWS            - AWS DIRECT CONNECT
                               Edge Locations for S3 Enabled applications   - AMAZON S3 TRANSFER ACCELERATION

  --> ONLINE  DATA TRANSFER :  Load stream Data into S3                     - AMAZON KINESIS
                               Online transfer of active data               - AWS DATA SYNC
                               SFTP transfer to S3                          - AWS TRANSFER FOR SFTP
                               Database Transfer                            - AWS DATABSE MIGRATION
                               VM Image transfer - Live applications        - AWS SERVER MIGRATION SERVICES // AWS ENDURE


  --> OFFLINE DATA TRANSFER :  Ship Static data in and out of AWS           - AWS SNOW FAMILY [ Snow cone / Snow Ball - Edge / Snow Mobile ]

  --> HYBRID DATA STORAGE   :  Access AWS storage from On-premisis          - AWS STORAGE GATEWAY
