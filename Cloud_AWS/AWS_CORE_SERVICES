behaviour====================================================================
TOTAL AVAILABLE SERVICE CATEGORIES  IN AWS : (As of January 2021)
====================================================================

NOTE :
########
1. This page contains a detailed description of what the services do in each of the categories .
2. Topics having a higher importance have individual chapters written after them. Marked with < Has a separate detailed chapter > after the header.
3. Recommended to View the YouTube Video , link provided beside the topic heading before going through the material.
4. Also view the reference pictures attached if any at cloud_AWS/0_Reference_pics.
5. If we have 2 or more services performing a similar activity , the likely chances are that the basic one is a free service while others are PAID services .
     --> AWS Cloud Watch (free) - AWS Detective (30 days free)- AWS Guard Duty  (30 days free)
6. Also a Resource  may be a combination of one or more resources or a subset of a single resource providing much more detailed infrastructure .


-------------------------------------------------------------------------------------------------------------------------------------------------
SERVICES COVERED :
-------------------------------------------------------------------------------------------------------------------------------------------------
Analytics                     Compute                  Data Base                Management & Governance         Satellite
Application Integration       Containers               Developer Tools          Migration & Transfer            Security, Identity, & Compliance
AR & VR                       Cryptography & PKI       End User Computing       Networking & Content Delivery   Storage
Billing & Cost Management     Customer Enablement      Front-End Web & Mobile   Quantum Computing
Business Applications         Customer Engagement      Game Development         Robotics
Blockchain

-------------------------------------------------------------------------------------------------------------------------------------------------
SERVICES NOT COVERED :
-------------------------------------------------------------------------------------------------------------------------------------------------
Internet of Things (IoT)
Machine Learning
Media Services


##################################################################################################################
##################################################################################################################
ANALYTICS :
##################################################################################################################
##################################################################################################################

1. AMAZON APP-FLOW :  https://www.youtube.com/watch?v=USzaWjjjOJI
// We may have SAAS apps such as sales force - slack etc; . Sharing data b/w them and AWS for analytics and archiving and automatic workflow isimportant.
// But managing the transfer is difficult which is handled by APP-FLOW.
Bidirectional data transfer from SAAS Applications To AWS
Amazon App Flow is a fully-managed integration service that enables you to securely exchange data between software as a service (SaaS) applications, such as Salesforce, Slack and AWS services, such as Amazon Simple Storage Service (Amazon S3) and Amazon Redshift.
For example, you can ingest contact records from Salesforce to Amazon Redshift or pull support tickets from Zendesk to an Amazon S3 bucket.

2. AMAZON ATHENA :  https://www.youtube.com/watch?v=M5ptG0YaqAs&t=1s
Amazon Athena is an interactive query service that makes it easy to analyze data directly in Amazon Simple Storage Service (Amazon S3) using standard SQL.
With a few actions in the AWS Management Console, you can point Athena at your data stored in Amazon S3 and begin using standard SQL to run ad-hoc queries and get results in seconds.
Athena is serverless, so there is no infrastructure to set up or manage, and you pay only for the queries you run.
Athena scales automatically—running queries in parallel—so results are fast, even with large datasets and complex queries.
You can query structured , unstructured and semi structured data as well. Examples include CSV, JSON, or columnar data formats such as Apache Parquet and Apache ORC.
You can then store results from queries directly into another bucket in S3 or download then to local.

3. AMAZON CLOUD-SEARCH :  https://www.youtube.com/watch?v=gpG16MFnEH8&t=14s
With Amazon CloudSearch, you can quickly add rich search capabilities to your website or application.
You don't need to become a search expert or worry about hardware provisioning, setup, and maintenance.
With a few clicks in the AWS Management Console, you can create a search domain and upload the data that you want to make searchable, and Amazon CloudSearch will automatically provision the required resources and deploy a highly tuned search index.
You can easily change your search parameters, fine tune search relevance, and apply new settings at any time.
      Free text, Boolean, and Faceted search - Autocomplete suggestions - Customizable relevance ranking and query-time rank expressions - Field weighting
      Geospatial search - Highlighting - Support for 34 languages

4. AWS DATA-EXCHANGE : https://www.youtube.com/watch?v=Lu9QVJ0Rml4&t=26s
AWS Data Exchange is a service that makes it easy for AWS customers to securely exchange file-based data sets in the AWS Cloud.
As a subscriber, you can find and subscribe to hundreds of products from qualified data providers.
Then, you can quickly download the data set or copy it to Amazon S3 for use across a variety of AWS analytics and machine learning service.
Providers in AWS Data Exchange have a secure, transparent, and reliable channel to reach AWS customers and grant existing customers their subscriptions more efficiently.
To promote a safe, secure, and trustworthy service for everyone, AWS Data Exchange scans all data published by providers before it is made available to subscribers. If AWS detects malware, the affected asset is removed.
Used by colleges, hospitals , scientists to get data from various data lakes .

5. AWS DATA-PIPELINE :
AWS Data Pipeline is a web service that you can use to automate the movement and transformation of data.
With AWS Data Pipeline, you can define data-driven workflows, so that tasks can be dependent on the successful completion of previous tasks.
You define the parameters of your data transformations and AWS Data Pipeline enforces the logic that you've set up.
A pipeline schedules and runs tasks by creating Amazon EC2 instances to perform the defined work activities.
You upload your pipeline definition to the pipeline, and then activate the pipeline.
Task Runner polls for tasks and then performs those tasks. For example, Task Runner could copy logfiles to Amazon S3 and launch Amazon EMR clusters.
Task Runner is installed and runs automatically on resources created by your pipeline definitions.

6. AMAZON ELASTIC-SEARCH SERVICE : https://www.youtube.com/watch?v=4Zw1IOxW-oA&t=26s
Used to analyse and get real time  insights on machine generated data at a peta byte scale. Supports Log stash , Kibana etc ;
Amazon Elasticsearch Service (Amazon ES) is a managed service that makes it easy to deploy, operate, and scale Elasticsearch clusters in the AWS Cloud.
Elasticsearch is a popular open-source search and analytics engine for use cases such as log analytics, real-time application monitoring, and click stream analysis.
With Amazon ES, you get direct access to the Elasticsearch APIs; existing code and applications work seamlessly with the service.

7. AMAZON ELASTIC MAP REDUCE - EMR : https://www.youtube.com/watch?v=QuwaBOESGiU
APACHE SPARK - APACHE HADOOP - CLUSTERS - SPARK  --> Maintaining Hadoop and Spark is costly so EMR does this for  us .
Amazon EMR is a managed cluster platform that simplifies running big data frameworks, such as Apache Hadoop and Apache Spark, on AWS to process and analyze vast amounts of data.
By using these frameworks and related open-source projects, such as Apache Hive and Apache Pig, you can process data for analytics purposes and business intelligence workloads.
The central component of Amazon EMR is the cluster. A cluster is a collection of Amazon Elastic Compute Cloud (Amazon EC2) instances.
Each instance in the cluster is called a node. Each node has a role within the cluster, referred to as the node type.
Amazon EMR also installs different software components on each node type, giving each node a role in a distributed application like Apache Hadoop.
Master node - Core Node - Task Node

Amazon EMR launches clusters in minutes.
You don’t need to worry about node provisioning, infrastructure setup, Hadoop configuration, or cluster tuning.
Amazon EMR takes care of these tasks so you can focus on analysis.
DynamoDB is serverless with no servers to provision, patch, or manage and no software to install, maintain, or operate.
DynamoDB automatically scales tables up and down to adjust for capacity and maintain performance.
Availability and fault tolerance are built in, eliminating the need to architect your applications for these capabilities.


8. AWS GLUE : https://www.youtube.com/watch?v=oAxvd547kMU&t=28s
AWS Glue is a fully managed ETL (extract, transform, and load) service that makes it simple and cost-effective to categorize your data, clean it, enrich it, and move it reliably between various data stores and data streams.
It is used to combine data [ Prepare a data lake ] from various sources and clean , normalize and prepare data to one common syntax and save to S3.
It uses other AWS services to orchestrate your ETL jobs to build data warehouses and data lakes and generate output streams.
AWS Glue calls API operations to transform your data, create runtime logs, store your job logic, and create notifications to help you monitor your job runs.

9. AMAZON KINESIS : https://www.youtube.com/watch?v=MbEfiX4sMXc&t=19s      &&    https://www.youtube.com/watch?v=07iZOEl0knc&t=29s
// Collect - storing - analyse high throughput information. -> Take action
// Not used to STREAM FEED TO mobiles etc ; it is used to STREAM FEED FROM from mobiles, cameras, IOT's and store and analyse them . DO not get confused.

Amazon Kinesis makes it easy to collect, process, and analyze real-time, streaming data.
This data includes video, audio, application logs, website clickstreams, and IoT telemetry data for machine learning, analytics, and other applications.
Amazon Kinesis enables you to process and analyze data as it arrives. You're able to respond instantly instead of having to wait until all your data is collected before the processing can begin.
It can take the below forms :
    Kinesis Streams   -> Collect and store data streams -- Collect gigabytes of data per second and make it available for processing and analyzing in real time.
    Real-time data capture : Ingest and store data streams from hundreds of thousands of data sources:
    --> Log and event data collection
    --> IoT device data capture
    --> Mobile data collection
    --> Gaming data feed

    Kinesis Firehose  -> Process and deliver data streams -- Prepare and load real-time data streams into data stores and analytics tools.
    Load real-time data : Load streaming data into data lakes, data stores, and analytics tools for:
    --> Log and event analytics
    --> IoT data analytics
    --> Clickstream analytics
    --> Security monitoring

    Kinesis Analytics -> Analyze streaming data -- Get actionable insights from streaming data in real time.
    Get insights in real time : Analyze streaming data and gain actionable insights in real time:
    --> Real-time streaming ETL
    --> Real-time log analytics
    --> Ad tech and digital marketing analytics
    --> Real-time IoT device monitoring

Amazon Kinesis Video Streams enables you to securely stream video from connected devices (IoT devices) to AWS for analytics, machine learning (ML), playback, and other processing.
Kinesis Video Streams automatically provisions and elastically scales all the infrastructure needed to ingest streaming video data from millions of devices. It durably stores, encrypts, and indexes video data in your streams, and allows you to access your data through easy-to-use APIs.

Amazon Kinesis Video Streams is a fully managed AWS service that you can use to stream live video from devices to the AWS Cloud, or build applications for real-time video processing or batch-oriented video analytics.
Kinesis Video Streams isn't just storage for video data. You can use it to watch your video streams in real time as they are received in the cloud.
You can use Kinesis Video Streams to capture massive amounts of live video data from millions of sources, including smartphones, security cameras, webcams, cameras embedded in cars, drones, and other sources.
You can also send non-video time-serialized data such as audio data, thermal imagery, depth data, RADAR data, and use kinesis to process these at a later Time.
you can stream video from a computer's webcam using the GStreamer  library, or from a camera on your network using RTSP.
You can also configure your Kinesis video stream to durably store media data for the specified retention period.
Kinesis Video Streams automatically stores this data and encrypts it at rest.
--> logs from mobile applications - logs from e-commers purchases - information from social media - satellite data - audio - video - spy cams - security cams etc;

  USECASES :
  --------------
  The AWS Streaming Data Solution for Amazon Kinesis helps developers more easily configure the core AWS services necessary to capture, store, process, and deliver streaming data by offering two AWS CloudFormation templates.
  One option is designed to capture data from non-AWS environments such as mobile clients, enable throttling at the API level, and leverage AWS Lambda to handle errors for Amazon Kinesis streams.
  The second option leverages Apache Flink and provides a fully managed service to handle backups automatically. This option also supports the Amazon Kinesis Producer Library (KPL).

  With Amazon Kinesis services, you can perform real-time analytics on data that has been traditionally analyzed using batch processing.
  Common streaming use cases include sharing data between different applications, streaming extract-transform-load, and real-time analytics.
  For example, you can use Kinesis Data Firehose to continuously load streaming data into your S3 data lake or analytics services.

  You can use Amazon Kinesis services for real-time application monitoring, fraud detection, and live leader-boards.
  You can ingest streaming data using Kinesis Data Streams, process it using Kinesis Data Analytics, and emit the results to any data store or application using Kinesis Data Streams with millisecond end-to-end latency.


10. AWS LAKE-FORMATION :
The data lake is your persistent data that is stored in Amazon S3 and managed by Lake Formation using a Data Catalog.
The Data Catalog is your persistent metadata store. It is a managed service that lets you store, annotate, and share metadata in the AWS Cloud.
Metadata about data sources and targets is in the form of databases and tables.
AWS Lake Formation is a fully managed service that makes it easier for you to build, secure, and manage data lakes.
Lake Formation simplifies and automates many of the complex manual steps that are usually required to create data lakes.
These steps include collecting, cleansing, moving, and cataloging data, and securely making that data available for analytics and machine learning.
You point Lake Formation at your data sources, and Lake Formation crawls those sources and moves the data into your new Amazon Simple Storage Service (Amazon S3) data lake.
After the data is securely stored in the data lake, users can access the data through their choice of analytics services, including Amazon Athena, Amazon Redshift, and Amazon EMR.

11. AMAZON  MANAGED STREAMING FOR KAFKA - MSK : Managed Streaming for Apache Kafka
Amazon Managed Streaming for Apache Kafka (Amazon MSK) is a fully managed service that enables you to build and run applications that use Apache Kafka to process streaming data.
Amazon MSK provides the control-plane operations, such as those for creating, updating, and deleting clusters.
It lets you use Apache Kafka data-plane operations, such as those for producing and consuming data. It runs open-source versions of Apache Kafka.

12. AMAZON QUICK-SIGHT : https://www.youtube.com/watch?v=2V1bHRLRG-w&t=8s
Amazon QuickSight is a cloud-scale business intelligence (BI) service that you can use to deliver easy-to-understand insights to the people who you work with, wherever they are.
It connects to your data in the cloud and combines data from many different sources.
In a single data dashboard, QuickSight can include AWS data, third-party data, big data, spreadsheet data, SaaS data, B2B data, and more.
Securely push these dashboards to customers. Used to forecast future results and take decisions.

13. AMAZON REDSHIFT : // Documented in Database https://www.youtube.com/watch?v=_qKm6o1zK3U&t=9s
Amazon Redshift is a fully managed, petabyte-scale data warehouse service in the cloud. An Amazon Redshift data warehouse is a collection of computing resources called nodes, which are organized into a group called a cluster.
Each cluster runs an Amazon Redshift engine and contains one or more databases.
Redshift lets you easily save the results of your queries back to your S3 data lake using open formats, like Apache Parquet, so that you can do additional analytics from other analytics services like Amazon EMR, Amazon Athena, and Amazon SageMaker.

Amazon Redshift is a data warehouse service that only supports relational data, NOT key-value data.
Amazon Redshift is a fast, fully managed data warehouse service that is specifically designed for online analytic processing (OLAP) and business intelligence (BI) applications, which require complex queries against large datasets.



##################################################################################################################
##################################################################################################################
COMPUTE  :
##################################################################################################################
##################################################################################################################

1. ELASTIC COMPUTE CLOUD :  < Has a separate detailed chapter >
Amazon Elastic Compute Cloud (Amazon EC2) provides scalable computing capacity in the Amazon Web Services (AWS) Cloud.
Using Amazon EC2 eliminates your need to invest in hardware up front, so you can develop and deploy applications faster.
You can use Amazon EC2 to launch as many or as few virtual servers as you need, configure security and networking, and manage storage.
Amazon EC2 enables you to scale up or down to handle changes in requirements or spikes in popularity, reducing your need to forecast traffic.

2. AWS BATCH : https://www.youtube.com/watch?v=j_iI1DzSi5g      &&   https://www.youtube.com/watch?v=T4aAWrGHmxQ&t=29s
AWS Batch enables you to run batch computing workloads on the AWS Cloud.
Batch computing is a common way for developers, scientists, and engineers to access large amounts of compute resources.
AWS Batch removes the undifferentiated heavy lifting of configuring and managing the required infrastructure.
As a fully managed service, AWS Batch helps you to run batch computing workloads of any scale.
AWS Batch automatically provisions compute resources and optimizes the workload distribution based on the quantity and scale of the workloads.
With AWS Batch, there is no need to install or manage batch computing software, which allows you to focus on analysing results and solving problems.
After a compute environment is up and associated with a job queue, you can define job definitions that specify which Docker container images to run your jobs.
  --> Break work into bathes and tell AWS which batch needs which resources are required and when to execute a batch

3. AWS ELASTI BEANSTALK :  < Has a separate detailed chapter > https://www.youtube.com/watch?v=SrwxAScdyT0&t=28s
--> Focus on your code --> Write it --> Upload it -->  AWS takes care of required resources .
With AWS Elastic Beanstalk, you can quickly deploy and manage applications in the AWS Cloud without worrying about the infrastructure that runs those applications.
AWS Elastic Beanstalk reduces management complexity without restricting choice or control.
You simply upload your application, and AWS Elastic Beanstalk automatically handles the details of capacity provisioning, load balancing, scaling, and application health monitoring.

4. AMAZON EC2 IMAGE BUILDER : https://www.youtube.com/watch?v=K5d2VdByohs&t=151s
It's a fully-managed AWS service that makes it easier to automate the creation, management, and deployment of customized, secure, and up-to-date “golden” server images that are pre-installed and pre-configured with software and settings to meet specific IT standards.
Crete a custom image based on required parameters and share the AMI across accounts or for instances in your account .

5. AWS LAMBDA :  < Has a separate detailed chapter >
--> Do something in response to events such as an api call / SNS / a data change in S3 /etc ;
With AWS Lambda, you can run code without provisioning or managing servers.
You pay only for the compute time that you consume—there’s no charge when your code isn’t running.
You can run code for virtually any type of application or backend service—all with zero administration.
Just upload your code and Lambda takes care of everything required to run and scale your code with high availability.
You can set up your code to automatically trigger from other AWS services or call it directly from any web or mobile app.
The Lambda free tier includes 1M free requests per month and 400,000 GB-seconds of compute time per month.
Past this, it costs $0.20 per 1 million requests and $0.00001667 for every GB-seconds. The GB-seconds is based on the memory consumption of the Lambda function.


6. AWS LAUNCH WIZARD :
--> Used by enterprise applications .
AWS Launch Wizard reduces the time it takes to deploy application and domain-controller solutions to the cloud by providing easy step-by-step guidance.
You input your application or domain controller requirements, and AWS Launch Wizard identifies the right AWS resources to deploy and run your solution.
AWS Launch Wizard provides an estimated cost of deployment, and gives you the ability to modify your resources and instantly view the updated cost assessment.
When you approve, AWS Launch Wizard provisions and configures the selected resources in a few hours to create fully-functioning, production-ready applications or domain controllers. It also creates custom AWS CloudFormation templates, which can be reused and customized for subsequent deployments.

7. AMAZON LIGHT SAIL : https://www.youtube.com/watch?v=wzhTAwRbdXw
--> Deploying a word press site etc ;
Amazon Lightsail helps developers get started using AWS to build websites or web applications.
It includes the features that you need to launch your project: instances (virtual private servers), managed databases, SSD-based block storage, static IP addresses, load balancers, content delivery network (CDN) distributions, DNS management of registered domains, and snapshots (backups).
These features are all available for a low, predictable monthly price.

8. AWS OUTPOSTS : https://www.youtube.com/watch?v=ppG2FFB0mMQ&t=9s
--> Even though we can use cloud , some applications still need to be on premises for latency issues . Hence OUTPOSTS.
    So instead of changing applications to meet both cloud and on premises apps , we can use same API's to access resources in both.
    AWS Technician comes and set up all the resources needed to connect resources in cloud and resources in on-premises.
An Outpost is a pool of AWS compute and storage capacity deployed at a customer site.
AWS Outposts brings native AWS services, infrastructure, and operating models to virtually any data centre, co-location space, or on-premises facility.
You can use the same services, tools, and partner solutions to develop for the cloud and on premises .

9. AWS PARALLEL CLUSTER : HPC // 1000's of CPU or GPU.   https://www.youtube.com/watch?v=r4RxT-IMtFY
AWS Parallel Cluster is an AWS supported open source cluster management tool that helps you to deploy and manage high performance computing (HPC) clusters in the AWS Cloud.
You can use AWSParallelCluster with batch schedulers, such as AWS Batch and Slurm.
AWS Parallel Cluster facilitates quick start proof of concept deployments and production deployments.
You can also build higher level workflows, such as a genomics portal that automates an entire DNA sequencing workflow, on top of AWSParallelCluster.

10. AWS SAM : [ SERVERLESS APPLICATION MODEL ]
The AWS Serverless Application Model (AWS SAM) is an open-source framework that you can use to buildserverless applications on AWS.
You can use AWS SAM to define your serverless applications.
It is an extension of CLOUD FORMATION

11. AWS SERVERLESS APPLICATION REPOSITORY :
The AWS Serverless Application Repository is a managed repository for serverless applications.
It enables teams, organizations, and individual developers to find, deploy, publish, share, store, and easily assemble serverless architectures.
Using SAM from above , we can publish and deploy applications to the public or to our team.

12. AWS WAVE LENGTH : To fully utilize 5G capabilities . // https://www.youtube.com/watch?v=EhMqwPqPzcY&t=6s
AWS Wavelength allows developers to build applications that deliver ultra-low latencies to mobile devices and end users.
Wavelength deploys standard AWS compute and storage services to the edge of telecommunication carriers' 5G networks.
Developers can extend an Amazon Virtual Private Cloud (VPC) to one or more Wavelength Zones.
Then use AWS resources like Amazon Elastic Compute Cloud (EC2) instances to run applications that require ultra-low latency and a connection to AWS services in the Region.
To help 5G --> Gaming experience - smart cities - robotics - autonomous driving cars



##################################################################################################################
##################################################################################################################
CONTAINERS   :
##################################################################################################################
##################################################################################################################

// DOCKER FILE  is a text document that contains commands that are used to assemble an image.
   We can use any command that call on the command line.
   Docker builds images automatically by reading the instructions from the Dockerfile.

    $ docker build  /path/to/a/Dockerfile  --> To build an image.

// DOCKER image is a read-only template with instructions for creating a Docker container.
   A docker image is described in text file called a Dockerfile, which has a simple, well-defined syntax.

// Docker container is a running instance of an image.
   You can use Command Line Interface (CLI) commands to run, start, stop, move, or delete a container.
   You can also provide configuration for the network and environment variables.
   Docker container is an isolated and secure application platform, but it can share and access to resources running in a different host or container.

// DOCKER packages software into standardized units called CONTAINERS .
   They have everything your software needs to run including Libraries , system tools , code and run time .
   It lets you quickly deploy and scale applications into any environment and know your code will run .

    $ docker run hello-world   // $ docker run <image> --> To create a container from an image and run it.
    https://www.javatpoint.com/docker-java-example

1. AMAZON ECR :  [ Amazon Elastic Container Registry ] // https://www.youtube.com/watch?v=H73uX0TOX9g&t=2s
It is a fully managed Docker container registry that makes it easy for developers to store, manage, and deploy DOCKER CONTAINER IMAGES.
Supports private container image repositories with resource-based permissions using AWS IAM. This is so that specified users or Amazon EC2 instances can access your container repositories and images.
Sports public container image repositories as well.
A repository is where you store your Docker or Open Container Initiative (OCI) images in Amazon ECR.
Each time you push or pull an image from Amazon ECR, you specify the repository and the registry location which informs where to push the image to or where to pull it from.

// Once an image is registered in the ECR , we can run that image as a container in the ECS service. -- ECS is a logical group of EC2 instances.
   ECS CLUSTER is a logical group of EC2 instances that you can place containers into .
   Ec2 instances can be of different configurations , types , and in diff. AZ's

2.AMAZON ECS : [ Amazon Elastic Container Service ] // https://www.youtube.com/watch?v=zBqjh61QcB4&t=4s && https://www.youtube.com/watch?v=eq4wL2MiNqo
Amazon Elastic Container Service (Amazon ECS) is a highly scalable, fast container management service that makes it easy to run, stop, and manage containers on a cluster.
Amazon ECS enables you to launch and stop your container-based applications by using simple API calls.
You can create Amazon ECS clusters within a new or existing VPC.
After a cluster is up and running, you can create task definitions that define which container images run across your clusters.
Your task definitions are used to run tasks or create services. Container images are stored in and pulled from container registries, for example, the Amazon Elastic Container Registry.

To prepare your application to run on Amazon ECS, you must create a task definition.
The task definition is a text file (in JSON format) that describes one or more containers (up to a maximum of ten) that form your application.
The task definition can be thought of as a blueprint for your application. It specifies various parameters for your application.
For example, these parameters can be used to indicate which containers should be used, which ports should be opened for your application, and what data volumes should be used with the containers in the task.
The specific parameters available for your task definition depend on the needs of your specific application.

A task is the instantiation of a task definition within a cluster.
After you have created a task definition for your application within Amazon ECS, you can specify the number of tasks to run on your cluster.
The Amazon ECS task scheduler is responsible for placing tasks within your cluster. There are several different scheduling options available.

// ECS AGENT manages the state of containers on an EC2 instance.
   Manages how ECS communicates with the docker daemon on the ec2.
   Is present on every ec2 instance.
   Is included with an ECS-Optimised Amazon Machine Image - AMI

// KUBERNETIS also known as K8s, is an open-source system for automating deployment, scaling, and management of containerized applications.
   It groups containers that make up an application into logical units for easy management and discovery.

3. AMAZON EKS : [ Amazon Elastic Kubernetes Service  ]
It s a managed service that you can use to run Kubernetes on AWS without needing to install, operate, and maintain your own Kubernetes control plane or nodes.
Kubernetes is an open-source system for automating the deployment, scaling, and management of containerized applications.
Amazon EKS automatically detects and replaces unhealthy control plane instances, and it provides automated version upgrades and patching for them.
Amazon EKS runs up-to-date versions of the open-source Kubernetes software, so you can use all of the existing plugins and tooling from the Kubernetes community.

4. AWS FARGATE :
AWS Fargate is a serverless compute engine for containers that works with both Amazon Elastic Container Service (ECS) and Amazon Elastic Kubernetes Service (EKS).
Fargate makes it easy for you to focus on building your applications.
Fargate removes the need to provision and manage servers, lets you specify and pay for resources per application, and improves security through application isolation by design.
Fargate allocates the right amount of compute, eliminating the need to choose instances and scale cluster capacity.
You only pay for the resources required to run your containers, so there is no over-provisioning and paying for additional servers.
Fargate runs each task or pod in its own kernel providing the tasks and pods their own isolated compute environment.
This enables your application to have workload isolation and improved security by design.
This is why customers such as Vanguard, Accenture, Foursquare, and Ancestry have chosen to run their mission critical applications on Fargate.

5. AWS APP2CONTAINER :
It is a command-line tool for modernizing .NET and Java applications into containerized applications.
Using A2C simplifies your migration tasks by performing inventory and analysis of your existing applications, creating Docker containers that include your application dependencies, and generating deployment templates based on AWS best practices with known values filled in for you.
After you have reviewed your templates, A2C helps you register your containers to Amazon ECR, deploy to Amazon ECS or Amazon EKS, and build CI/CD pipelines using AWS CodeStar.
It is also a command line tool to help you lift and shift applications that run in your on-premises data centres or on virtual machines, so that they run in containers that are managed by Amazon ECS or Amazon EKS. Thus providing a step towards modernization .



##################################################################################################################
##################################################################################################################
STORAGE   :
##################################################################################################################
##################################################################################################################

1. AMAZON SIMPLE STORAGE SERVICE - S3 :  < Has a separate detailed chapter >
--> Has 11 9's of durability . 99.999999999
Amazon S3 stores any number of objects, but each object does have a size limitation. Individual Amazon S3 objects can range in size from a minimum of 0 bytes to a maximum of 5 terabytes.
It is storage for the internet. You can use Amazon S3 to store and retrieve any amount of data at any time, from anywhere on the web.
You can accomplish these tasks using the simple and intuitive web interface of the AWS Management Console.
A bucket is a container for objects stored in Amazon S3. Every object is contained in a bucket.

2. AWS BACKUP : https://www.youtube.com/watch?v=QDiXzFx2iMU&t=17s
AWS Backup is a fully managed backup service that makes it easy to centralize and automate the backup of data across AWS services in the cloud as well as on premises [ uses AWS STORAGE GATEWAY for on premises].
Using AWS Backup, you can configure backup policies and monitor backup activity for your AWS resources in one place.
AWS Backup automates and consolidates backup tasks that were previously performed service-by-service, and removes the need to create custom scripts and manual processes.
It provides a fully managed backup service and a policy-based backup solution .
These features include Amazon Elastic Block Store (Amazon EBS) snapshots, Amazon Relational Database Service (Amazon RDS)snapshots, Amazon DynamoDB backups, AWS Storage Gateway snapshots, and others.
AWS Backup implements its backup features using the existing capabilities of these AWS services.
You can write backup PLANS to tell when to back up and when to move backup to Glacier etc;

3. AMAZON ELASTIC BLOCK STORE - EBS :  < Has a separate detailed chapter > // https://www.youtube.com/watch?v=77qLAl-lRpo
Amazon Elastic Block Store (Amazon EBS) is a storage service, NOT a database service.
High performance and low latency block storage - has 4 types.
Amazon Elastic Block Store (Amazon EBS) is a web service that provides block level storage volumes for use with EC2 instances.
EBS volumes are highly available and reliable storage volumes that can be attached to any running instance and used like a hard drive.

4. AMAZON ELASTIC FILE SYSTEM - EFS : -> Its a Cloud File  https://www.youtube.com/watch?v=6ZIPBC78U0s&t=8s
Amazon Elastic File System (Amazon EFS) provides a simple, scalable, fully managed elastic NFS file system for use with AWS Cloud services and on-premises resources.
It provides file storage for your Amazon EC2 instances.
With Amazon EFS, you can create a file system, mount the file system on your EC2 instances, and then read and write data from your EC2 instances to and from your file system.
It provides a simple, scalable, fully managed elastic NFS filesystem for use with AWS Cloud services and on-premises resources.
Amazon EFS has a simple web services interface that allows you to create and configure file systems quickly and easily.
Multiple Amazon EC2instances can access an Amazon EFS file system at the same time, providing a common data source for workloads and applications running on more than one instance or server.
Using Amazon EFS with Microsoft Windows–based Amazon EC2 instances is not supported.

                        EC2-a --
                        EC2-b   | <---------->  |||||||||  -> EFS
                        EC2-c --       NFS
            So multiple instances can access NFS at the same time using a standard NFSv4 Protocol .
            Once EFS is create d, we can mount EC2 instances on EFS.
            Useful when application runs on multiple EC2.

When you first create your file system, there is only one root directory at /.
By default, only the root user (UID0) has read-write-execute permissions. For other users to modify the file system, the root user must explicitly grant them access.
You use EFS access points to provision directories that are writable from a specific application

Amazon EFS oﬀers two storage classes, Standard and Infrequent Access.
The Standard storage class is used to store frequently accessed files.
The Infrequent Access (IA) storage class is a lower-cost storage class that's designed for storing long-lived, infrequently accessed ﬁles cost-eﬀectively.


// EFS for Linux
// FSx for Windows

5. AMAZON FSx : https://www.youtube.com/watch?v=4v08-CzjH1U
Amazon FSx provides fully managed third-party file systems with the native compatibility and feature sets for workloads such as Microsoft Windows–based storage, high-performance computing, machine learning, and electronic design automation.
Amazon FSx supports two file system types: Lustre and Windows File Server.
With file storage on Amazon FSx, the code, applications, and tools that Windows developers and administrators use today can continue to work unchanged.
Windows applications and workloads ideal for Amazon FSx include business applications, home directories, web serving, content management, data analytics, software build setups, and media processing workloads.

6. AMAZON SNOW FAMILY : < Has a separate detailed chapter - Storage_s3 >
 -> Snow cone        - https://www.youtube.com/watch?v=X_8LM7E_hiE      -- 8 TB storage   --> 1 SMALL BOX [ uses opshub application for data transfer ]
    Snow Ball        - https://www.youtube.com/watch?v=9uc2DSZ1wL8&t=9s -- 80 TB storage  --> 1 SUITCASE
    Snow Ball edge   - https://www.youtube.com/watch?v=bxSD1Nha2k8      -- 100 TB storage --> 1 SUITCASE  [ Embedded Ec2 - Lambda for running jobs in remote areas like ships , hills for an immediate results. ]
    Snow Mobile      - https://www.youtube.com/watch?v=8vQmTZTq7nw      -- 1 Exabyte      --> 1 Truck

   1 Exabyte = 1000 petabytes
   Snow ball can be connected in series to have a Petabyte's storage  = 1 million GB = 1024 Terabytes

7. AMAZON STORAGE GATEWAY : < Has a separate detailed chapter - Storage_s3 > -- https://www.youtube.com/watch?v=DPyc0q4MYsM -- https://www.youtube.com/watch?v=Spzdj1NUJbA -- https://www.youtube.com/watch?v=2I4CKdNESoQ&t=8s --
--> HYBRID CLOUD STORAGE SERVICE WITH A LOCAL CACHE . -- FILE GATEWAY -- VOLUME GATEWAY -- TAPE GATEWAY .
AWS Storage Gateway is a service that connects an on-premises software appliance with cloud-based storage to provide seamless and secure integration between your on-premises IT environment and the AWS storage infrastructure in the cloud.
Securely takes data from on premises data centre and uploads in S3 using SSL. We can get back data when ever we need it , for example if a hard disk crashed in on-premisi then we need to restore it back .
Storage gateway keeps regular backups from S3 so that it can restore to on-premises when needed.

The above process is needed until you are ready to move completely to cloud .
Slowly we can also launch an EC2 and attach an EBS with data from S3 and migrate to cloud directly.



##################################################################################################################
##################################################################################################################
DATABASE   :
##################################################################################################################
##################################################################################################################

DATABASE TYPE     USE CASE                                                      AWS SERVICE
================================================================================================================================================
Relational        Traditional applications, ERP, CRM, e-commerce                Amazon Aurora - Amazon RDS -  Amazon Redshift
================================================================================================================================================
Key-value         High-traffic web apps, e-commerce systems,                    Amazon DynamoDB
                  gaming applications
================================================================================================================================================
In-memory         Caching, session management, gaming leader boards,            Amazon ElastiCache for Memcached -  Amazon ElastiCache for Redis
                  geospatial applications
================================================================================================================================================
Document          Content management, catalogs, user profiles                   Amazon DocumentDB (with MongoDB compatibility)
================================================================================================================================================
Wide column       High scale industrial apps for equipment maintenance,         Amazon Keyspaces (for Apache Cassandra)
                  fleet management, and route optimization
================================================================================================================================================
Graph             Fraud detection, social networking, recommendation engines    Amazon Neptune
================================================================================================================================================
Time series       IoT applications, DevOps, industrial telemetry                Amazon Timestream
================================================================================================================================================
Ledger            Systems of record, supply chain, registrations,               Amazon QLDB
                  banking transactions
================================================================================================================================================

1. AMAZON AURORA : < Has a separate detailed chapter - Database >  https://www.youtube.com/watch?v=eMzCI7S1P9M&t=19s
Amazon Aurora (Aurora) is a fully managed relational database engine that's compatible with MySQL and PostgreSQL.
With some workloads, Aurora can deliver up to five times the throughput of MySQL and up to three times the throughput of PostgreSQL without requiring changes to most of your existing applications.
Aurora is part of the managed database service Amazon Relational Database Service (Amazon RDS).
Aurora management operations typically involve entire clusters of database servers that are synchronized through replication, instead of individual database instances.
The automatic clustering, replication, and storage allocation make it simple and cost-effective to set up, operate, and scale your largest MySQL and PostgreSQL deployments.

Amazon Aurora is a MySQL and PostgreSQL-compatible relational database built for the cloud.
Amazon Aurora combines the performance and availability of traditional enterprise databases with the simplicity and cost-effectiveness of open source databases.
It delivers up to five times the throughput of standard MySQL and up to three times the throughput of standard PostgreSQL.
Amazon Aurora is designed to be compatible with MySQL and with PostgreSQL, so that existing applications and tools can run without requiring modification.
It is available through Amazon Relational Database Service (RDS), freeing you from time-consuming administrative tasks such as provisioning, patching, backup, recovery, failure detection, and repair.


2. AMAZON DOCUMENT-DB :  https://www.youtube.com/watch?v=tkzDp9T8V-k
Amazon DocumentDB (with MongoDB compatibility) is a fast, reliable, and fully managed database service that makes it easy for you to set up, operate, and scale MongoDB-compatible databases.
Typically, in the application tier, data is represented as a JSON document because it is more intuitive for developers to think of their data model as a document.
The popularity of document databases has grown because they let you persist data in a database by using the same document model format that you use in your application code.
Document databases are a practical solution to online profiles in which different users provide different types of information. Using a document database, you can store eachuser's profile efficiently by storing only the attributes that are specific to each user.
Suppose that a user elects to add or remove information from their profile. In this case, their document could be easily replaced with an updated version that contains any recently added attributes anddata or omits any newly omitted attributes and data.

Document databases are used for storing semi structured data as a document—rather than normalizing data across multiple tables, each with a unique and fixed structure, as in a relational database.
Different types of documents can be stored in the same document database, thus meeting the requirement for processing similar data that is in different formats.

3. AMAZON DYNAMO-DB : < Has a separate detailed chapter - Database >   https://www.youtube.com/watch?v=sI-zciHAh-4
Amazon DynamoDB is a fully managed NoSQL database service that provides fast and predictable performance with seamless scalability.
You can use Amazon DynamoDB to create a database table that can store and retrieve any amount of data, and serve any level of request traffic.
Amazon DynamoDB automatically spreads the data and traffic for the table over a sufficient number of servers to handle the request capacity specified by the customer and the amount of data stored, while maintaining consistent and fast performance.
DynamoDB lets you offload the administrative burdens of operating and scaling a distributed database so that you don't have to worry about hardware provisioning, setup and configuration, replication, software patching, or cluster scaling.
You can create on-demand backups and enable point-in-time recovery for your Amazon DynamoDB tables.
Point-in-time recovery helps protect your tables from accidental write or delete operations.
With point-in-time recovery, you can restore a table to any point in time during the last 35 days
Allows you to delete expired items from tables automatically to help you reduce storage usage and the cost of storing data that is no longer relevant.

4. AMAZON ELASTIC CACHE FOR REDIS: -> https://www.youtube.com/watch?v=GoNsuTqeMto&t=18s
--> Caching data and providing it to users quickly rather than getting it from Disks. // example for applications in mobile .
Amazon ElastiCache makes it easy to set up, manage, and scale distributed in-memory cache environments in the AWS Cloud.
It provides a high performance, resizable, and cost-effective in-memory cache, while removing complexity associated with deploying and managing a distributed cache environment.
ElastiCache works with both the Redis and Memcached engines.
Amazon ElastiCache is a web service that makes it easy to deploy, operate, and scale an in-memory data store or cache in the cloud.
The service improves the performance of web applications by allowing you to retrieve information from fast, managed, in-memory data stores, instead of relying entirely on slower disk-based databases.

  IN MEMEMORY vs ELASTICACHE :
  --------------------------------
  The most frequently accessed data be stored in elasticache so that the application’s response time is optimal.
  The primary purpose of an in-memory data store is to provide ultrafast (submillisecond latency) and inexpensive access to copies of data.
    Querying a database is always slower and more expensive than locating a copy of that data in a cache.
    Some database queries are especially expensive to perform.
    An example is queries that involve joins across multiple tables or queries with intensive calculations.
    By caching (storing) such query results, you pay the price of the query only once.
    Then you can quickly retrieve the data multiple times without having to re-execute the query.

5. AMAZON KEYSPACES [APAHE ASSANDRA] :

6. AMAZON NEPTUNE: Used for fraud detection // https://www.youtube.com/watch?v=Rl6UwE7kLio
Amazon Neptune is a fast, reliable, fully managed graph database service that makes it easy to build and run applications that work with highly connected datasets.
The core of Neptune is a purpose-built, high-performance graph database engine that is optimized for storing billions of relationships and querying the graph with milliseconds latency.
Neptune supports the popular graph query languages Apache TinkerPop Gremlin and W3C’s SPARQL, allowing you to build queries that efficiently navigate highly connected datasets.
Neptune powers graph use cases such as recommendation engines, fraud detection, knowledge graphs, drug discovery, and network security.

Each node represents an entity (a person, place, thing, category or other piece of data), and each relationship represents how two nodes are associated.
This general-purpose structure allows you to model all kinds of scenarios – from a system of roads, to a network of devices, to a population’s medical history or anything else defined by relationships.

7. AMAZON QUANTUM LEDGER DATABASE - QLDB :  https://www.youtube.com/watch?v=jcZ_rsLJrqk
Amazon Quantum Ledger Database (Amazon QLDB) is a fully managed ledger database that provides a transparent, immutable, and cryptographically verifiable transaction log owned by a central trusted authority.
You can use Amazon QLDB to track all application data changes, and maintain a complete and verifiable history of changes over time.
Ex : Banks data -> keeps full history of changes. -- Immutable and Transparent.

8. AMAZON RELSTIONAL DATABASE SERVICE -RDS : < Has a separate detailed chapter - Database >   https://www.youtube.com/watch?v=eMzCI7S1P9M&t=19s
It is a web service that makes it easier to set up, operate, and scale a relational database in the cloud.
It provides cost-efficient, resizable capacity for an industry-standard relational database and manages common database administration tasks.
Amazon RDS can be used to host Amazon Aurora, PostgreSQL, MySQL, MariaDB, Oracle, and SQL Server databases.

  • When you buy a server, you get CPU, memory, storage, and IOPS, all bundled together. With AmazonRDS, these are split apart so that you can scale them independently.
    If you need more CPU, less IOPS, or more storage, you can easily allocate them.
  • Amazon RDS manages backups, software patching, automatic failure detection, and recovery.
  • To deliver a managed service experience, Amazon RDS doesn't provide shell access to DB instances. It also restricts access to certain system procedures and tables that require advanced privileges.
  • You can have automated backups performed when you need them, or manually create your own backup snapshot. You can use these backups to restore a database. The Amazon RDS restore processworks reliably and efficiently.
  • You can use the database products you are already familiar with: MySQL, MariaDB, PostgreSQL, Oracle, Microsoft SQL Server.
  • In addition to the security in your database package, you can help control who can access your RDS databases by using AWS Identity and Access Management (IAM) to define users and permissions.

9. AMAZON REDSHIFT :  https://www.youtube.com/watch?v=_qKm6o1zK3U&t=22s
Amazon Redshift is not a MySQL database service. Amazon Redshift is a fully managed data warehouse.
For data from BIG-DATA , enterprise applications //  used mainly for data analytics by quickly mining data .
Amazon Redshift is a fast, fully managed, petabyte-scale data warehouse service that makes it simple and cost-effective to efficiently analyze all your data using your existing business intelligence tools.
It is optimized for datasets ranging from a few hundred gigabytes to a petabyte or more and costs less than $1,000 per terabyte per year, a tenth the cost of most traditional data warehousing solutions.
Amazon Redshift doesn’t support non-relational data. Amazon Redshift is a fully managed data warehouse service that allows you to run complex analytic queries against petabytes of structured data using standard SQL and your existing Business Intelligence (BI) tools.

Amazon Redshift is a data warehouse service that only supports relational data, NOT key-value data.
Amazon Redshift is a fast, fully managed data warehouse service that is specifically designed for online analytic processing (OLAP) and business intelligence (BI) applications, which require complex queries against large datasets.


10. AMAZON TIMESTREAM :
With Amazon Timestream, you can easily store and analyze sensor data for IoT applications, metrics for DevOps use cases, and telemetry for application monitoring scenarios such as clickstream data analysis.
Used to store and analyze trillions of time series data points per day.
Timestream saves you time and cost in managing the lifecycle of time series data by keeping recent data in memory and moving historical data to a cost optimized storage tier based upon user defined policies.
Timestream’s purpose-built query engine lets you access and analyze recent and historical data together, without having to specify its location.
Examples of a growing list of use cases for Timestream include:
--> Monitoring metrics to improve the performance and availability of your applications.
--> Storage and analysis of industrial telemetry to streamline equipment management and maintenance.
--> Tracking user interaction with an application over time.
--> Storage and analysis of IoT sensor data



##################################################################################################################
##################################################################################################################
SECURITY - IDENTITY - COMPLIANCE   :
##################################################################################################################
##################################################################################################################

1. AWS IDENTITY & ACCESS MANAGEMENT : < Has a separate detailed chapter - IAM >  https://www.youtube.com/watch?v=Ul6FW4UANGc
It is a web service for securely controlling access to AWS services. You use IAM to control who is authenticated (signed in) and authorized (has permissions) to use resources.
With IAM, you can centrally manage users, security credentials such as access keys, and permissions that control which AWS resources users and applications can access.
You can use IAM to control all the available ~180 services available in AWS. And using cloudtrail you can keep track of the activities .
Users can focus on developing applications and IAM handles the security. And it is free of cost .

2. AWS ARTIFACT : https://www.youtube.com/watch?v=ILEoLqpbfXM&t=5s
No cost, self-service portal for on-demand access to AWS compliance reports and for entering into select online agreements.
Use AWS Artifact reports to access several compliance reports from third-party auditors who have tested and verified our compliance with a variety of global, regional, and industry specific security standards and regulations.
Use AWS Artifact agreements to review, accept, and terminate agreements with AWS for an individual account, and for all accounts that are a part of your organization in AWS Organizations.

AWS Artifact is a web service that enables you to download AWS security and compliance documents such as ISO certifications, , Payment Card Industry (PCI), and Service Organization Control (SOC) reports.
You can submit the security and compliance documents (also known as audit artifacts) to your auditors or regulators to demonstrate the security and compliance of the AWS infrastructure and services that you use.
You can also use these documents as guidelines to evaluate your own cloud architecture and assess the effectiveness of your company's internal controls.
AWS Artifact provides documents about AWS only. You can also use AWS Artifact to review, accept, and track the status of AWS agreements such as theBusiness Associate Addendum (BAA).
AWS customers are responsible for developing or obtaining documents that demonstrate the security and compliance of their companies.

AWS Artifact is a self-service audit artifact retrieval portal that provides customers with on-demand access to AWS’ compliance documentation and AWS agreements. You can use AWS Artifact Agreements to review, accept, and track the status of AWS agreements such as the Business Associate Addendum (BAA).
You can also use AWS Artifact Reports to download AWS security and compliance documents, such as AWS ISO certifications, Payment Card Industry (PCI), and System and Organization Control (SOC) reports.

3. AWS AUDIT MANAGER :
When it is time for an audit, AWS Audit Manager helps you manage stakeholder reviews of your controls, which means you can build audit-ready reports with much less manual effort.
AWS Audit Manager helps you continuously audit your AWS usage to simplify how you manage risk and compliance with regulations and industry standards.
AWS Audit Manager makes it easier to evaluate whether your policies, procedures, and activities—also known as controls—are operating as intended.
The service offers prebuilt frameworks with controls that are mapped to well-known industry standards and regulations, full customization of frameworks and controls, and automated collection and organization of evidence as designed by each control requirement.

4. AWS COGNITO :
Amazon Cognito handles user authentication and authorization for your web and mobile apps.
With user pools, you can easily and securely add sign-up and sign-in functionality to your apps.
Your users can sign in directly with a user name and password, or through a third party such asFacebook, Amazon, Google or Apple.
The two main components of Amazon Cognito are user pools and identity pools.
  --> User pools are user directories that provide sign-up and sign-in options for your app users.
  --> Identity pools enable you to grant your users access to other AWS services.
  You can use identity pools and user pools separately or together.

5. AWS DIRETORY SERVICE : -> To use Microsoft Active Directory - MAD // https://www.youtube.com/watch?v=XNTsmRe8k7Q&t=9s
--> MAD is  used to manage computers and other devices on a network.
--> It is a primary feature of Windows Server, an operating system that runs both local and Internet-based servers.
--> So a similar functionality is provided by AWS Directory service , used to handle unlimited number of servers and their credentials and let users login through single sign on with existing corporate credentials on premises.
--> As an admin you  can manage both windows and linux instances and no more needed to manage credentials to individual servers.
AWS Directory Service provides multiple ways to set up and run Microsoft Active Directory with other AWS services such as Amazon EC2, Amazon RDS for SQL Server, Amazon FSx for Windows File Server, and AWS Single-Sign On.
AWS Directory Service for Microsoft Active Directory, also known as AWS Managed Microsoft AD, enables your directory-aware workloads and AWS resources to use a managed Active Directory in the AWS Cloud.

 // The below three services AWS WAF - AWS SHIELD - AWS FIREWALL MANAGER are interlinked. So which one to use ??
    --> It all starts with AWS WAF. You can automate and then simplify AWS WAF management using AWSFirewall Manager.
    --> Shield Advanced adds additional features on top of AWS WAF, such as dedicated support from the DDoS Response Team (DRT) and advanced reporting.
    --> To use the services of the DRT, you must be subscribed to the Business Support plan or theEnterprise Support plan.
    --> If you want granular control over the protection that is added to your resources, AWS WAF alone is the right choice.
    --> If you want to use AWS WAF across accounts, accelerate your AWS WAF configuration, or automate protection of new resources, use Firewall Manager with AWS WAF.
    --> Finally, if you own high visibility websites or are otherwise prone to frequent DDoS attacks, you should consider purchasing the additional features that Shield Advanced provides.

6. AWS WEB APPLICATION FIREWALL - WAF :
A web application firewall (WAF) helps protect your web applications from common web exploits that could affect application availability, compromise security, or consume excessive resources.
AWS WAF is a web application firewall that lets you monitor the HTTP and HTTPS requests that are forwarded to an Amazon Cloud Front distribution, an Amazon API Gateway REST API, an Application Load Balancer, or an AWS AppSync GraphQL API.
AWS WAF also lets you control access to your content.
Based on conditions that you specify, such as the IP addresses that requests originate from or the values of query strings, Amazon Cloud Front, Amazon API Gateway, Application Load Balancer, or AWS AppSync responds to requests either with the requested content or with an HTTP 403 status code (Forbidden).
You also can configure CloudFront to return a custom error page when a request is blocked.
You can use AWS WAF web access control lists (web ACLs) to help minimize the effects of a distributed denial of service (DDoS) attack
You can also use AWS WAF to protect your applications that are hosted in Amazon Elastic Container Service (Amazon ECS) containers.
AWS WAF is available today anywhere CloudFront is available. Pricing is $5 per web ACL, $1 per rule, and $0.60 per million HTTP requests.

You can chose to :
  • Allow all requests except the ones that you specify
  • Block all requests except the ones that you specify
  • Count the requests that match the properties that you specify
Setting up WAF :
  • Create a web access control list (web ACL) using the wizard in the AWS WAF console.
  • Choose the AWS resources that you want AWS WAF to inspect web requests for.
  • Add the rules and rule groups that you want to use to filter web requests.
    --> A string match rule statement identifies strings that you want AWS WAF to search for in a request, such as a specified value in a header or in a query string.
    --> Attackers attack with a particular string and when you write a string matching rule , those requests are blocked .
  • Specify a default action for the web ACL, either block or allow.

7. AWS SHIELD : --> Mainly for the DDOS attacks.
You can use AWS WAF web access control lists (web ACLs) to help minimize the effects of a distributed denial of service (DDoS) attack.
For additional protection against DDoS attacks, AWS also provides AWS Shield Standard and AWS Shield Advanced.
AWS Shield Standard is automatically included at no extra cost beyond what you already pay for AWS WAF and your other AWS services.
AWS Shield Advanced provides expanded DDoS attack protection for your Amazon EC2 instances, Elastic Load Balancing load balancers, CloudFront distributions, Route 53 hosted zones, and AWS Global Accelerator accelerators.
AWS Shield Advanced incurs additional charges. You can add protection for any of the following resource types:
  • Amazon CloudFront distributions
  • Amazon Route 53 hosted zones
  • AWS Global Accelerator accelerators
  • Application load balancers
  • Elastic Load Balancing (ELB) load balancers
  • Amazon Elastic Compute Cloud (Amazon EC2) Elastic IP addresses

8. AWS FIREWALL MANAGER :  --> Used to manage the above WAF and SHIELD .
AWS Firewall Manager simplifies your administration and maintenance tasks across multiple accounts and resources for AWS WAF, AWS Shield Advanced, Amazon VPC security groups, and AWS NetworkFirewall.
With Firewall Manager, you set up your AWS WAF firewall rules, Shield Advanced protections, Amazon VPC security groups, and Network Firewall firewalls just once.
The service automatically applies your rules across your accounts and resources, even as you add new resources.

  • Helps to protect resources across accounts
  • Helps to protect all resources of a particular type, such as all Amazon CloudFront distributions
  • Helps to protect all resources with specific tags
  • Automatically adds protection to resources that are added to your account
  • Allows you to subscribe all member accounts in an AWS Organizations organization to AWS Shield Advanced, and automatically subscribes new in-scope accounts that join the organization
  • Allows you to apply security group rules to all member accounts or specific subsets of accounts in an AWS Organizations organization, and automatically applies the rules to new in-scope accounts that join the organization
  • Lets you use your own rules, or purchase managed rules from AWS Marketplace

// Once configured, Firewall Manager will automatically create a Network Firewall with the sets of rules, deploying an endpoint in a dedicated subnet for every availability zone containing public subnets, in the accounts and VPCs you specify.
   At the same time, any changes to the centrally configured set of rules are automatically updated downstream on the deployed Network Firewalls.
   Network firewall manager manages all netwrok firewalls , vpc's , subnets etc;
   Network firewall manages a particular VPC . You can create one for each VPC .

9. AWS NETWORK FIREWALL : --> Supported by NETWORK FIREWALL MANAGER // https://aws.amazon.com/about-aws/whats-new/2020/11/aws-firewall-manager-supports-centralized-management-aws-network-firewall/
AWS Network Firewall is a stateful, managed, network firewall and intrusion detection and prevention service for your virtual private cloud (VPC).
With Network Firewall, you can filter traffic at the perimeter of your VPC. This includes filtering traffic going to and coming from an internet gateway, NAT gateway, or over VPN or AWS Direct Connect.
  • Pass traffic through only from known AWS service domains or IP address endpoints, such as AmazonS3.
  • Use custom lists of known bad domains to limit the types of domain names that your applications can access.
  • Perform deep packet inspection on traffic entering or leaving your VPC.
  • Use stateful protocol detection to filter protocols like HTTPS, independent of the port used.
To enable Network Firewall for your VPC, you perform steps in both Amazon VPC and in Network Firewall.
The firewall protects the subnets within your VPC by filtering traffic going between the subnets and locations outside of your VPC.
To enable the firewall's protection, you modify your Amazon VPC route tables to send your networktraffic through the Network Firewall firewall endpoint.


10. AWS CLOUD DIRECTORY :  https://www.youtube.com/watch?v=OhaGbCeNLs0&t=27s
--> Graph-based directory -- Different from AWS DIRECTORY SERVICE above.
--> Cloud Directory is NOT a directory service for IT Administrators who want to manage or migrate their directory infrastructure and do authentication and authorization.
--> The concept is to generate a conceptual relationship between the objects .
Amazon Cloud Directory is a cloud-native directory that can store hundreds of millions of application-specific objects with multiple relationships and schemas.
Use Cloud Directory when you need a cloud-scale directory to share and control access to hierarchical data between your applications.
With Cloud Directory, you can organize application data into multiple hierarchies to support many organizational pivots and relationships across directory information.
For example, a directory of users may provide a hierarchical view based on reporting structure, location, and project affiliation.
Similarly, a directory of devices may have multiple hierarchical views based on its manufacturer, current owner, and physical location.

/// The below AWS DETECTIVE - AWS GUARD DUTY - AWS INSPECTOR - AWS MACIE are similar in concepts.
    --> AWS DETECTIVE is mainly for the security findings in login attempts , API calls.
    --> AWS GUARD DUTY does DETETIVE duty plus reports compromised EC2 instanes , compromised s3 instanes .
    --> AWS INSPECTOR does an vulnerability assessment based on common security standards and vulnerability definitions updated frequently by AWS .
    --> AWS MACIE is for protecting personal and financial data stored  in S3 across your organization.
    --> All the services are free for 30 days .Pay from the next month!! During the 30-day free trial period, the cost estimation projects what your estimated costs will be after the free trial period.


11. AMAZON DETECTIVE :
Amazon Detective makes it easy to analyze, investigate, and quickly identify the root cause of security findings or suspicious activities.
Detective automatically collects log data from your AWS resources and uses machine learning, statistical analysis, and graph theory to help you visualize and conduct faster and more efficient security investigations.
Detective automatically extracts time-based events such as login attempts, API calls, and network traffic from AWS CloudTrail and Amazon VPC flow logs.
It also ingests findings detected by GuardDuty. You can explore this behavior graph to examine disparate actions such as failed logon attempts or suspicious API calls.
Free for 30days. Pay from the next month!! During the 30-day free trial period, the cost estimation projects what your estimated costs will be after the free trial period.

12. AWS GUARD DUTY :  https://www.youtube.com/watch?v=ocZjGirQT9A&t=28s
Amazon Guard Duty is a continuous security monitoring service.
Amazon Guard Duty can help to identify unexpected and potentially unauthorized or malicious activity in your AWS environment.
It a continuous security monitoring service that analyzes and processes the following Data sources : VPC Flow Logs, AWS CloudTrail management event logs, Cloudtrail S3 data event logs, and DNS logs.
It uses threat intelligence feeds, such as lists of malicious IP addresses and domains, and machine learning to identify unexpected and potentially unauthorized and malicious activity within your AWS environment.
This can include issues like escalations of privileges, uses of exposed credentials, or communication with malicious IP addresses, or domains.

    •   compromised EC2 instance
    •   Compromised S3 Bucket
    •   compromised AWS credentials

For example, GuardDuty can detect compromised EC2 instances serving malware or mining bitcoin.
It also monitors AWS account access behaviour for signs of compromise, such as unauthorized infrastructure deployments, like instances deployed in a Region that has never been used, or unusual API calls, like a password policy change to reduce password strength.
AWS DETECTIVE checks only security findings but guard duty can do additional stuff stated above .
Free for 30days. Pay from the next month!! During the 30-day free trial period, the cost estimation projects what your estimated costs will be after the free trial period.

13. AWS INSPECTOR : https://www.youtube.com/watch?v=xxvrmBPbNPs
Amazon Inspector is a security vulnerability assessment service that helps improve the security and compliance of your AWS resources.
It tests the network accessibility of your Amazon EC2 instances and the security state of your applications that run on those instances. Amazon Inspector assesses applications for exposure, vulnerabilities, and deviations from best practices.
Amazon Inspector also offers predefined software called an agent that you can optionally install in the operating system of the EC2 instances that you want to assess.
Amazon Inspector automatically assesses resources for vulnerabilities or deviations from best practices, and then produces a detailed list of security findings prioritized by level of severity.
Amazon Inspector includes a knowledge base of hundreds of rules mapped to common security standards and vulnerability definitions that are regularly updated by AWS security researchers.

Amazon Inspector is an automated security assessment service that helps improve the security and compliance of applications deployed on AWS.
Amazon Inspector automatically assesses applications for vulnerabilities or deviations from best practices. After performing an assessment, Amazon Inspector produces a detailed list of security findings prioritized by level of severity.
These findings can be reviewed directly or as part of a detailed assessment report which is available via the Amazon Inspector console or API.
To help get started quickly, Amazon Inspector includes a knowledge base of hundreds of rules mapped to common security best practices and vulnerability definitions.
Examples of built-in rules include checking for remote root login being enabled, or vulnerable software versions installed.
These rules are regularly updated by AWS security researchers.


14. AWS MACIE :
Amazon Macie is a fully managed data security and data privacy service that uses machine learning and pattern matching to discover, classify, and help you protect your sensitive data in Amazon S3.
Macie automates the discovery of sensitive data, such as personally identifiable information (PII) and financial data, to provide you with a better understanding of the data that your organization storesin Amazon Simple Storage Service (Amazon S3).
Macie also provides you with an inventory of your S3buckets, and it automatically evaluates and monitors those buckets for security and access control.
Within minutes, Macie can identify and report overly permissive or unencrypted buckets for your organization.

15. AWS RESOURCE ACCESS MANAGER - RAM : < Has a separate detailed chapter - Resource_groups_&_RAM >
AWS Resource Access Manager (AWS RAM) enables you to share your resources with any AWS account or organization in AWS Organizations.
Customers who operate multiple accounts can create resources centrally and use AWS RAM to share them with all of their accounts to reduce operational overhead.
AWS RAM is available at no additional charge.
You can share resources that you own by creating a resource share.
When you create a resource share, you specify a name, the resources to share, and the principals with whom to share.
Principals can be AWS accounts, organizational units, or an entire organization from AWS Organizations.
Your account retains full ownership of the resources that you share.

  • Reduces operational overhead
  • Provides security and consistency
  • Provides visibility and auditability

The following sections list the services that integrate with AWS RAM, and the resources that support sharing.
    • AWS App Mesh     • Amazon Aurora        • AWS Certificate Manager Private Certificate Authority
    • AWS Code Build   • Amazon EC2           • EC2 Image Builder
    • AWS Glue         • AWS License Manager  • AWS Network Firewall
    • AWS Outposts     • AWS Resource Groups  • Amazon Route 53
    • Amazon VPC

16. AWS RESOURCE GROUPS & TAG EDITOR : < Has a separate detailed chapter - Resource_groups_&_RAM >
AWS Resource Groups lets you organize AWS resources into groups, tag resources using virtually any criteria, and manage, monitor, and automate tasks on grouped resources.
A resource group is a collection of AWS resources that are all in the same AWS region, and that match criteria provided in a query.
Tags are words or phrases that act as metadata that you can use to identify and organize your AWS resources.
With Tag Editor, you build a query to find resources in one or more AWS Regions that are available for tagging.
You can choose up to 20 individual resource types, or build a query on All resource types.

17. AWS SECRETS MANAGER :
In the past, when you created a custom application to retrieve information from a database, you typically embedded the credentials, the secret, for accessing the database directly in the application.
When the time came to rotate the credentials, you had to do more than just create new credentials. You had to invest time to update the application to use the new credentials.
Then you distributed the updated application. If you had multiple applications with shared credentials and you missed updating one of them, the application failed.

AWS Secrets Manager helps you to securely encrypt, store, and retrieve credentials for your databases and other services.
This helps ensure the secret can't be compromised by someone examining your code, because the secret no longer exists in the code.
Instead of hardcoding credentials in your apps, you can make calls to Secrets Manager to retrieve your credentials whenever needed.
Secrets Manager helps you protect access to your IT resources and data by enabling you to rotate and manage access to your secrets.
You create roles and grant permissions to apps to use that role to access secret manager.

18. AWS SECURITY HUB :
AWS Security Hub provides you with a comprehensive view of the security state of your AWS resources against security industry standards and best practices.
Security Hub collects security data from across AWS accounts and services, and helps you analyze your security trends to identify and prioritize the security issues across your AWS environment.

SINGLE-SIGN-ON :
------------------
Security Assertion Markup Language (SAML) is an open standard that allows identity providers (IdP) to pass authorization credentials to service providers (SP).
What that jargon means is that you can use one set of credentials to log into many different websites.
Single Sign On uses SAML .

Single sign-on (SSO) is an authentication method that enables users to securely authenticate with multiple applications and websites by using just one set of credentials.
SSO works based upon a trust relationship set up between an application, known as the service provider, and an identity provider.
This trust relationship is often based upon a certificate that is exchanged between the identity provider and the service provider.
This certificate can be used to sign identity information that is being sent from the identity provider to the service provider so that the service provider knows it is coming from a trusted source.
In SSO, this identity data takes the form of tokens which contain identifying bits of information about the user like a user’s email address or a username.

The login flow usually looks like this:
  --> A user browses to the application or website they want access to, aka, the Service Provider.
  --> The Service Provider sends a token that contains some information about the user, like their email address, to the SSO system, aka, the Identity Provider, as part of a request to authenticate the user.
  --> The Identity Provider first checks to see whether the user has already been authenticated, in which case it will grant the user access to the Service Provider application and skip to step 5.
  --> If the user hasn’t logged in, they will be prompted to do so by providing the credentials required by the Identity Provider. This could simply be a username and password or it might include some other form of authentication like a One-Time Password (OTP).
  --> Once the Identity Provider validates the credentials provided, it will send a token back to the Service Provider confirming a successful authentication.
  --> This token is passed through the user’s browser to the Service Provider.
  --> The token that is received by the Service Provider is validated according to the trust relationship that was set up between the Service Provider and the Identity Provider during the initial configuration.
  --> The user is granted access to the Service Provider.

  The use of SSO is a very popular method of allowing access with just a single sign in. LDAP, on the other hand, is the protocol used in authentication of the SSO systems.
  The Acronym LDAP refers to Lightweight Directory Access Protocol.
  LDAP is an Identity repository.
  SAML ( used by SSO ) is an Identity standard that could use LDAP as the repository. Or it could use something else like Active directory.


19. AWS SINGLE SIGN-ON :
AWS Single Sign-On (AWS SSO) is a cloud-based service that simplifies managing SSO access to AWS accounts and business applications.
You can control SSO access and user permissions across all your AWS accounts in AWS Organizations.
You can also administer access to popular business applications and custom applications that support Security Assertion Markup Language (SAML) 2.0.
In addition, AWS SSO offers a user portal where your users can find all their assigned AWS accounts, business applications, and custom applications in one place.



#########################################################################################################################
#########################################################################################################################
CRYPTOGRAPHY AND PKI [ Public Key Infrastructure ]  SERVICES : // View chapter Cryptography_KMS_CMK for complete Details.
#########################################################################################################################
#########################################################################################################################

1. AWS CLOUD-HSM :
HARDWARE SECURITY MODULE is a computing device that performs cryptographic operations and provides secure storage for cryptographic keys.
AWS CloudHSM organizes HSMs in clusters, which are automatically synchronized collections of HSMs within a given Availability Zone (AZ).
If you want a managed service for creating and controlling encryption keys, but do not want or need tooperate your own HSM, consider using AWS Key Management Service.
When you use an HSM from AWS CloudHSM, you can perform a variety of cryptographic tasks:
    • Generate, store, import, export, and manage cryptographic keys, including symmetric keys and asymmetric key pairs.
    • Use symmetric and asymmetric algorithms to encrypt and decrypt data.
    • Use cryptographic hash functions to compute message digests and hash-based message authentication codes (HMACs).
    • Cryptographically sign data (including code signing) and verify signatures.
    • Generate cryptographically secure random data.

2. AWS KEY MANAGEMENT SERVICE :
It provides tools for generating master keys and other data keys.
AWS KMS also interacts with many other AWS services to encrypt their service-specific data.
It is an AWS service that makes it easy for you to create and control the encryption keys that are used to encrypt your data.
The customer master keys (CMKs) that you create in AWS KMS are protected by FIPS 140-2 validated cryptographic modules.
They never leave AWS KMS unencrypted. To use or manage your CMKs, you interact with AWS KMS.
AWS KMS is also integrated with AWS CloudTrail to deliver detailed logs of all cryptographic operations that use your CMKs and management operations that change their configuration.

  ****VIMP :
  AWS KMS protects the customer master keys that protect your data.
  In the classic scenario, you encrypt your data using data key A. But you need to protect data key A, so you encrypt data key A by using data key B.
  Now data key B is vulnerable, so you encrypt it by using data key C. And, so on.
  This encryption technique, which is called envelope encryption, always leaves one last encryption key unencrypted so you can decrypt your encryption keys and data.
  That last unencrypted (or plaintext) key is called a master key.

AWS KMS customer master keys (CMKs) are 256-bit Advanced Encryption Standard (AES) symmetric keys that are not exportable. They spend their entire lifecycle entirely within AWS KMS.
You can also create asymmetric RSA or elliptic curve (ECC) CMKs backed by asymmetric key pairs. The public key in each asymmetric CMK is exportable, but the private key remains within AWS KMS.
You can use your CMKs to encrypt small amounts of data (up to 4096 bytes). However, CMKs are typically used to generate, encrypt, and decrypt the data keys that encrypt your data.
Unlike CMKs, data keys can encrypt data of any size and format, including streamed data.

AWS KMS does not store or manage data keys, and you cannot use KMS to encrypt or decrypt with data keys. To use data keys to encrypt and decrypt, use the AWS Encryption SDK.
AWS KMS CMKs are backed by FIPS-validated hardware service modules (HSMs) that KMS manages. Tomanage your own HSMs, use AWS CloudHSM.

3. AWS ENCRYPTION SDK :
The AWS Encryption SDK is a client-side encryption library to help you implement best-practice encryption and decryption in any application even if you're not a cryptography expert.
The AWS Encryption SDK works on all types of data. Every successful call to encrypt returns a single portable, formatted encrypted message that contains metadata and the message ciphertext.
The AWS Encryption SDK is developed as an open source project.
It is available in multiple programming languages, including a command line interface that is supported on Linux, macOS, and Windows.
All implementations are interoperable. For example, you can encrypt your data with the Java library and decrypt it with the Python library. Or you can encrypt data with the C library and decrypt it with the CLI.

4. AMAZON DYNAMO-DB ENCRYPTION CLIENT :
The Amazon DynamoDB Encryption Client is a client-side encryption library that helps you to protect your table data before you send it to Amazon DynamoDB.
Encrypting your sensitive data in transit and at rest helps ensure that your plaintext data isn’t available to any third party, including AWS.
It encrypts the attribute values in each table item using a unique encryption key.
It then signs the item to protect it against unauthorized changes, such as adding or deleting attributes or swapping encrypted values.
It also verifies and decrypts them when you retrieve them.
It is an open source project and is available in JAVA and python and is interoperable.

5. AMAZON S3 CLIENT SIDE ENCRYPTION :
  OPTION-1 : Using a CMK stored in AWS KMS :
        When uploading an object —
            Using the CMK ID, the client first sends a request to AWS KMS for a CMK that it can use to encrypt your object data.
            The client obtains a unique data key for each object that it uploads.
            AWS KMS returns two versions of a randomly generated data key:
              --> A plaintext version of the data key that the client uses to encrypt the object data.
              --> A cipher blob of the same data key that the client uploads to Amazon S3 as object metadata.

        When downloading an object —
        The client downloads the encrypted object from Amazon S3 along with the cipher blob version of the data key stored as object metadata.
        The client then sends the cipher blob to AWS KMS to get the plaintext version of the data key so that it can decrypt the object data

  OPTION-2 : Using a master key stored within your application
        When uploading an object —
            You provide a client-side master key to the Amazon S3 encryption client.
            The client uses the master key only to encrypt the data encryption key that it generates randomly.
              --> The Amazon S3 encryption client generates a one-time-use symmetric key (also known as a data encryption key or data key) locally.
                  It uses the data key to encrypt the data of a single Amazon S3 object. The client generates a separate data key for each object.
              --> The client encrypts the data encryption key using the master key that you provide.
                  The client uploads the encrypted data key and its material description as part of the object metadata.
                  The client uses the material description to determine which client-side master key to use for decryption.
              --> The client uploads the encrypted data to Amazon S3 and saves the encrypted data key as object metadata (x-amz-meta-x-amz-key) in Amazon S3.

        When downloading an object —
        The client downloads the encrypted object from Amazon S3.
        Using the material description from the object's metadata, the client determines which master key to use to decrypt the data key.
        The client uses that master key to decrypt the data key and then uses the data key to decrypt the object.

NOTE :
Your client-side master keys and your unencrypted data are never sent to AWS. It's important that you safely manage your encryption keys. If you lose them, you can't decrypt your data.
The client-side master key that you provide can be either a symmetric key or a public/private key pair. The following code examples show how to use each type of key.

5. AWS SECRET MANAGER : Documented in SECURITY-IDENTITY-COMPLIANCE
provides encryption and rotation of encrypted secrets used with AWS-supported databases.

To protect DynamoDB table items before you send them to DynamoDB, use the DynamoDB Encryption Client.
To protect Amazon S3 objects before you send them to an Amazon S3 bucket, use Amazon S3 client-side encryption. Amazon S3 also offers server-side encryption.
To protect all other types of data at their source, use the AWS Encryption SDK.

WHEN TO USE AWS-CLOUD HSM AND WHEN TO USE KMS :
Use AWS CloudHSM when you need to manage the HSMs that generate and store your encryption keys.
In AWS CloudHSM, you create and manage HSMs, including creating users and setting their permissions.
You also create the symmetric keys and asymmetric key pairs that the HSM stores.
If you need to secure your encryption keys in a service backed by FIPS-validated HSMs, but you do not need to manage the HSM, try AWS Key Management Service (p

6. AWS CERTIFICATE MANAGER - ACM :
AWS Certificate Manager is a service that lets you easily provision, manage, and deploy public and private Secure Sockets Layer/Transport Layer Security (SSL/TLS) certificates for use with AWS services and your internal connected resources.
It is used to generate, issue, and manage public and private SSL/TLS certificates for use with your AWS based websites and applications.
These public certificates verify the identity and authenticity of your web server and the ownership of your public keys.
In doing so, public certificates initiate a trusted, encrypted connection between you and your users.

7. AWS CERTIFICATE MANAGER PRIVATE CERTIFICATE AUTHORITY - ACM PCA:
is service is for enterprise customers building a public key infrastructure (PKI)inside the AWS cloud and intended for private use within an organization.
It is a managed private certificate authority (CA) service with which you can manage your CA infrastructure and your private certificates.
With private certificates you can authenticate resources inside an organization.
Private certificates allow entities like users, web servers, VPN users, internal API endpoints, and IoT devices to prove their identity and establish encrypted communications channels.

8. AWS SIGNER :
AWS Signer is a fully managed code-signing service to ensure the trust and integrity of your code.
Organizations validate code against a digital signature to confirm that the code is unaltered and from a trusted publisher.
With AWS Signer, your security administrators have a single place to define your signing environment, including what AWS Identity and Access Management (IAM) role can sign code and in what regions.
AWS Signer manages the code-signing certificate public and private keys and enables central management of the code-signing lifecycle.
  --> With Code Signing for AWS Lambda, you can ensure that only trusted code runs in your Lambda functions.
  --> With Code Signing for AWS IoT, you can sign code that you create for IoT devices supported by Amazon Free RTOS and AWS IoT device management.

9. AWS CRYPTO TOOLS :
Cryptography is hard to do safely and correctly.
The AWS Crypto Tools libraries are designed to help everyone do cryptography right, even without special expertise.
Our client-side encryption libraries help you to protect your sensitive data at its source using secure cryptographic algorithms, envelope encryption, and signing.
It is a part of AWS ENCRYPTION SDK .



##################################################################################################################
##################################################################################################################
DEVELOPER TOOLS :
##################################################################################################################
##################################################################################################################


1. AWS CLOUD-9 : --> We get an IDE to work with online.
AWS Cloud9 is a cloud-based integrated development environment (IDE) that you use to write, run, and debug code.
The AWS Cloud9 IDE offers a rich code-editing experience with support for several programming languages and runtime debuggers, and a built-in terminal.
It contains a collection of tools that you use to code, build, run, test, and debug software, and helps you release software to the cloud.
sing the AWS Cloud9 IDE, you can:
  •  Store your project's files locally on the instance or server.
  •  Clone a remote code repository — such as a repo in AWS CodeCommit — into your environment.
  •  Work with a combination of local and cloned files in the environment.

In AWS Cloud9, a development environment (or just environment) is a place where you STORE your development project's files and where you run the tools to develop your applications.
You can instruct AWS Cloud9 to create an Amazon EC2 instance, and then connect the environment to that newly created EC2 instance. This type of setup is called an EC2 environment.
You can instruct AWS Cloud9 to connect an environment to an existing cloud compute instance or to your own server. This type of setup is called an SSH environment
Free of cost but if you integrate an EC2 , compute is billed.

2. AWS CLOUD-SHELL : --> We get a shell to work with online .
AWS CloudShell is a browser-based shell that you can use to manage AWS services using the AWS Command Line Interface (AWS CLI) and a range of pre-installed development tools.
AWS CloudShell is a browser-based, pre-authenticated shell that you can launch directly from the AWS Management Console.
You can run AWS CLI commands against AWS services using your preferred shell (Bash, PowerShell, or Z shell).
And you can do this without needing to download or install command line tools.

3. AWS CODE-ARTIFACT :
AWS CodeArtifact is a secure, scalable, and cost-effective artifact management service for software development.
CodeArtifact is a fully managed artifact repository service that makes it easy for organizations to securely store, publish and share software packages used for application development.
You can use CodeArtifact with popular build tools and package managers such as NuGet, Maven, Gradle, npm, yarn, pip, and twine.

Artifacts refers to the collection of data, such as application source code, built applications, dependencies, definitions files, templates, and so on, that is worked on by pipeline actions.
Artifacts are produced by some actions and consumed by others.
In a pipeline, artifacts can be the set of files worked on by an action (input artifacts) or the updated output of a completed action (output artifacts).

4. AWS CODE-BUILD : One of the steps in CodePipeline.
AWS CodeBuild is a fully managed build service in the cloud.
CodeBuild compiles your source code, runs unit tests, and produces artifacts that are ready to deploy.
CodeBuild eliminates the need toprovision, manage, and scale your own build servers.
It provides prepackaged build environments forpopular programming languages and build tools such as Apache Maven, Gradle, and more.
You can use the AWS CodeBuild or AWS CodePipeline console to run CodeBuild.

--> As input, you must provide CodeBuild with a build project.
    A build project includes information about how to run a build, including where to get the source code, which build environment to use, which build commands to run, and where to store the build output.
    A build environment represents a combination of operating system, programming language runtime, and tools that CodeBuild uses to run a build.
--> CodeBuild uses the build project to create the build environment.
--> CodeBuild downloads the source code into the build environment and then uses the build specification ( buildspec ), as defined in the build project or included directly in the source code.
    A buildspec is a collection of build commands and related settings, in YAML format, that CodeBuild uses to run a build
--> If there is any build output, the build environment uploads its output to an S3 bucket.
    The build environment can also perform tasks that you specify in the buildspec (for example, sending build notifications to an Amazon SNS topic).
--> While the build is running, the build environment sends information to CodeBuild and Amazon CloudWatch Logs.

PROCESS :::
 Source Code + BUILD PROJECT [ BUILD SPEC -> OS + Language + Run time + Build env + where to put output ] -> CODE BUILD creates a BUILD ENVIRONMENT  -> Send output to S3 ->  Send SNS if specified .

**** If you use AWS CodePipeline to run builds, you can get limited build information from CodePipeline.
     Hence we use codebuild seperately.

5. AWS CODE-COMMIT : Similar to GITHUB and Versioning in S3.
AWS CodeCommit is a version control service that enables you to privately store and manage Git repositories in the AWS Cloud.
CodeCommit eliminates the need for you to manage your own source control system or worry about scaling its infrastructure. You can use CodeCommit to store anything from code to binaries and documents.
    1. Use the AWS CLI or the CodeCommit console to create a CodeCommit repository.
    2. From your development machine, use Git to run git clone, specifying the name of the CodeCommit repository. This creates a local repo that connects to the CodeCommit repository.
    3. Use the local repo on your development machine to modify (add, edit, and delete) files, and then run git add to stage the modified files locally. Run git commit to commit the files locally, and then run git push to send the files to the CodeCommit repository.
    4. Download changes from other users. Run git pull to synchronize the files in the CodeCommit repository with your local repo. This ensures you're working with the latest version of the files.

CodeCommit is optimized for team software development. It manages batches of changes acrossmultiple files, which can occur in parallel with changes made by other developers.
Amazon S3 versioning supports the recovery of past versions of files, but it's not focused on collaborative file tracking featuresthat software development teams need.
You can use AWS Cloud9 to make code changes in a CodeCommit repository. You need to have an AWS Cloud9 EC2 development environment running on Amazon Linux.

6. AWS CODE-DEPLOY : https://www.youtube.com/watch?v=Wx-ain8UryM&t=9s
CodeDeploy is a deployment service that automates application deployments to Amazon EC2 instances, on-premises instances, serverless Lambda functions, or Amazon ECS services.
CodeDeploy can deploy application content that runs on a server and is stored in Amazon S3 buckets, GitHub repositories, or Bitbucket repositories.
The service scales with your infrastructure so you can easily deploy to one instance or thousands.
It Make sure that the patches are deployed in all instances equally. Free of cost and pay only for EC2.

  • Rapidly release new features.
  • Update AWS Lambda function versions.
  • Avoid downtime during application deployment.
  • Handle the complexity of updating your applications, without many of the risks associated with error-prone manual deployments.

--> First, you create deployable content on your local development machine or similar environment, and then you add an application specification file (AppSpec file).
    The AppSpec file is unique to CodeDeploy. It defines the deployment actions you want CodeDeploy to execute.
    You bundle your deployable content and the AppSpec file into an archive file, and then upload it to an Amazon S3 bucket or aGitHub repository.
    This archive file is called an application revision (or simply a revision).
--> Next, you provide CodeDeploy with information about your deployment, such as which AmazonS3 bucket or GitHub repository to pull the revision from and to which set of Amazon EC2 instances to deploy its contents.
    CodeDeploy calls a set of Amazon EC2 instances a deployment group.
    A deployment group contains individually tagged Amazon EC2 instances, Amazon EC2 instances in Amazon EC2 Auto Scaling groups, or both.
--> Next, the CodeDeploy agent on each instance polls CodeDeploy to determine what and when to pull from the specified Amazon S3 bucket or GitHub repositor.
--> Finally, the CodeDeploy agent on each instance pulls the target revision from the Amazon S3 bucket or GitHub repository and, using the instructions in the AppSpec file, deploys the contents to theinstance.

PROCESS :::
 Deployable content [ from CODE BUILD ] + APPSPEC File -> S3 or GITHUB -> Tell CODE DEPLOY [ s3 url / guthub url && No; of instances ] -> CODE DEPLOY Agent in each instance pulls the revision .

7. AWS CODE-PIPELINE :
AWS CodePipeline is a continuous delivery service that enables you to model, visualize, and automate the steps required to release your software.
You can quickly model and configure the different stages of a software release process. CodePipeline automates the steps required to release your software changes continuously.
CodePipeline is a continuous delivery service that automates the building, testing, and deployment of your software into production.
  --> Continuous delivery is a software development methodology where the release process is automated.
      Every software change is automatically built, tested, and deployed to production.
      Before the final push to production, a person, an automated test, or a business rule decides when the final push should occur.
  --> Continuous integration is a software development practice where members of a team use a version control system and frequently integrate their work to the same location, such as a main branch.
      Each change is built and verified to detect integration errors as quickly as possible.
      Continuous integration is focused on automatically building and testing code, as compared to continuous delivery, which automates the entire software release process up to production.

when developers commit changes to a source repository, CodePipeline automatically detects the changes. Those changes are built, and if any tests are configured, those tests are run.
After the tests are complete, the built code is deployed to staging servers for testing.
From the staging server, CodePipeline runs more tests, such as integration or load tests.
Upon the successful completion of those tests, and after a manual approval action that was added to the pipeline is approved, CodePipeline deploys the tested and approved code to production instances.

CodePipeline can deploy applications to EC2 instances by using CodeDeploy, AWS Elastic Beanstalk, or AWS OpsWorks Stacks.
CodePipeline can also deploy container-based applications to services by using Amazon ECS.

PROCESS ::: // Check_Diagram in Refrence pics .
You can trigger an execution when you change your source code or manually start the pipeline.
You can also trigger an execution through an Amazon CloudWatch Events rule that you schedule.
For example, when a source code change is pushed to a repository configured as the pipeline's source action, the pipeline detects the change and starts an execution.

--> The application source code is maintained in a repository configured as a GitHub source action in the pipeline.
    When developers push commits to the repository, CodePipeline detects the pushed change, and a pipeline execution starts from the Source Stage.
--> The GitHub source action completes successfully (that is, the latest changes have been downloaded and stored to the artifact bucket unique to that execution).
    The output artifacts produced by the GitHub source action, which are the application files from the repository, are then used as the input artifacts to be worked on by the actions in the next stage.
--> The pipeline execution transitions from the Source Stage to the Prod Stage.
    The first action in the Prod Stage runs a build project created in CodeBuild and configured as a build action in the pipeline.
    The build task pulls a build environment image and builds the web application in a virtual container.
--> The next action in the Prod Stage is a unit test project created in CodeBuild and configured as a test action in the pipeline.
--> The unit tested code is next worked on by a deploy action in the Prod Stage that deploys the application to a production environment.
    After the deploy action completes successfully, the final action in the stage is an integration testing project created in CodeBuild and configured as a tes taction in the pipeline.
    The test action calls to shell scripts that install and run a test tool, such as a link checker, on the web application.
    After successful completion, the output is a built web application and a set of test results.

To use the console to stop a pipeline execution, you can choose Stop execution on the pipeline visualization page.
--> Stop and wait: All in-progress action executions are allowed to complete, and subsequent actions are not started.
    The pipeline execution does not continue to subsequent stages.
    You cannot use this option on an execution that is already in a Stopping state.
--> Stop and abandon: All in-progress action executions are abandoned and do not complete, and subsequent actions are not started.
    The pipeline execution does not continue to subsequent stages.
    You can use this option on an execution that is already in a Stopping state.


8. AWS CODE-STAR :
AWS CodeStar lets you quickly develop, build, and deploy applications on AWS.
An AWS CodeStar project creates and integrates AWS services for your project development toolchain.
Depending on your choice of AWS CodeStar project template, that toolchain might include source control, build, deployment, virtual servers or serverless resources, and more.
An AWS CodeStar project is a combination of source code and the resources created to deploy the code.
The collection of resources that help you build, release, and deploy your code are called tool chain resources.
At project creation, an AWS CloudFormation template provisions your toolchain resources in a continuous integration/continuous deployment (CI/CD) pipeline.
You can create a project both from CLI and console .

--> • Start new software projects on AWS in minutes using templates for web applications, web services, and more
--> • Manage project access for your team
--> • Visualize, operate, and collaborate on your projects in one place

PROCESS :::
Goto AWS -> Coe Star -> Select a Template -> Add team members -> create a repo for code -> Create a pipeline -> Deploy Project .

A developer with the AWSCodeStarFullAccess policy applied that creates a project and adds team members to it.
Together they write, build, test, and deploy code.
The project dashboard provides tools that can be used in real time to view application activity and monitor builds, the flow of code through the deployment pipeline, and more.
The team uses the team wiki tile to share information, best practices, and links.
They integrate their issue-tracking software to help them track progress and tasks.
As customers provide requests and feedback, the team adds this information to the project and integrates it into their project planning and development.
As the project grows, the team adds more team members to support their code base.

9. AWS X-RAY :
AWS X-Ray makes it easy for developers to analyze the behavior of their distributed applications by providing request tracing, exception collection, and profiling capabilities.
AWS X-Ray is a service that collects data about requests that your application serves, and provides tools you can use to view, filter, and gain insights into that data to identify issues and opportunities for optimization.
For any traced request to your application, you can see detailed information not only about the request and response, but also about calls that your application makes to downstream AWS resources, microservices, databases and HTTP web APIs.



##################################################################################################################
##################################################################################################################
MANAGEMENT & GOVERNANCE :
##################################################################################################################
##################################################################################################################

1. AWS APPCONFIG : // Different from CONFIG // Integrated DEPLOY action in AWS CODE PIPELINE .
Use AWS AppConfig, a capability of AWS Systems Manager, to create, manage, and quickly deploy application configurations.
You can use AWS AppConfig with applications hosted on Amazon Elastic Compute Cloud (Amazon EC2) instances, AWS Lambda, containers, mobile applications, or IoT devices.
AWS AppConfig supports controlled deployments and includes built-in validation checks and monitoring.
At a high level , there are three processes for working with AWSCONFIG .
  --> 1. Configure :  AWS AppConfig to work with your application.
  --> 2. Enable    :  your application code to periodically check for and receive configuration data from AWS AppConfig.
  --> 3. Deploy    :  a new or updated configuration

  APPLICATION : An application in AWS AppConfig is a logical unit of code that provides capabilities for your customers.
                For example, an application can be a microservice that runs on EC2 instances, a mobile application installed by your users and AWS Lambda, or any system you run on behalf of others.
  ENVIRONMENT : For each application, you define one or more environments.
                An environment is a logical deployment group of AWS AppConfig applications, such as applications in a Beta or Production environment.
                You can also define environments for application sub components such as the Web, Mobile, and Back-end components for your application.
  CONFIGURATION PROFILE : A configuration profile enables AWS AppConfig toaccess your configuration in its stored location.
                You can store configurations in YAML / JSON / Object in S3 / Document in SYSTEMS MANAGER.
                It can have validators which appconfig checks while deployment and rolls back if any validator constraint is not met .

AWS AppConfig helps simplify the following tasks:

  --> CONFIGURE  : Source your configurations from Amazon Simple Storage Service (Amazon S3), AWS AppConfig hosted configurations, Parameter Store, Systems Manager Document Store.
                   Use AWS CodePipeline integration to source your configurations from Bitbucket Pipelines, GitHub, and AWS CodeCommit.
  --> VALIDATE   : While deploying application configurations, a simple typo could cause an unexpected outage.
                   Prevent errors in production systems using AWS AppConfig validators.
                   AWS AppConfig validators provide asyntactic check using a JSON schema deployments proceed only when the checks pass .
  --> DEPLOY     : Define deployment criteria and rate controls to determine how your targets receive the new configuration
                   Monitor each deployment to proactively catch any errors using AWS AppConfig integration with Amazon CloudWatch Events.
                   If AWS AppConfig encounters an error, the system rolls back.


2. AWS AUTO-SCALING : < Has a separate detailed chapter >
Auto scaling is enabled by Amazon CloudWatch and is available at no additional charge beyond the service fees for CloudWatch and the other AWS resources that you use.
Amazon EC2 Auto Scaling helps you ensure that you have the correct number of Amazon EC2 instances available to handle the load for your application.
You create collections of EC2 instances, called AutoScaling groups. You can specify the minimum number of instances in each Auto Scaling group, and Amazon EC2 Auto Scaling ensures that your group never goes below this size.
You can specify the maximum number of instances in each Auto Scaling group, and Amazon EC2 Auto Scaling ensures that your group never goes above this size.
If you specify scaling policies, then Amazon EC2 Auto Scaling can launch or terminate instances as demand on your application increases or decreases.

--> Groups : Your EC2 instances are organized into groups so that they can be treated as a logical unit for the purposes of scaling and management.
             When you create a group, you can specify its minimum, maximum, and, desired number of EC2 instances.
--> Configuration templates : Your group uses a launch template, or a launch configuration , as a configuration template for its EC2 instances.
             You can specify information such as the AMI ID, instance type, key pair, security groups, and block device mapping for your instances.
--> Scaling options  : Amazon EC2 Auto Scaling provides several ways for you to scale your Auto Scaling groups.
             You can configure a group to scale based on if certain conditions are met or on a schedule .

SCALING OPTIONS :
 --> Maintain current instance levels at all times :
 --> Scale manually :
 --> Scale based on a schedule :
 --> Scale based on demand :
 --> Use predictive scaling :

// Elastic Load Balancers do not scale resources.
// Elastic Load Balancers distributes incoming application traffic across multiple targets, such as Amazon EC2 instances, containers, IP addresses, and Lambda functions.

// Scaling resources is a function of AWS AUTO SCALING.
// AWS Auto Scaling is the feature that automates the process of adding/removing server capacity (based on demand).
// Autoscaling allows you to reduce your costs by automatically turning off resources that aren’t in use.
// On the other hand, Autoscaling ensures that your application runs effectively by provisioning more server capacity if required.


3. AWS BACKINT AGENT FOR SAP HANA :
AWS Backint Agent for SAP HANA (AWS Backint Agent) is an SAP-certified backup and restore application for SAP HANA workloads running on Amazon EC2 instances in the cloud.

4. AWS CHATBOT :
AWS Chatbot is an AWS service that enables DevOps and software development teams to use Slack or Amazon Chime chat rooms to monitor and respond to operational events in their AWS Cloud.
AWS Chatbot processes AWS service notifications from Amazon Simple Notification Service (Amazon SNS), and forwards them to Slack or Amazon Chime chat rooms so teams can analyze and act on them.
Teams can respond to AWS service events from a chat room where the entire team can collaborate, regardless of location.
Slack and Amazon Chimeusers map the SNS topics to their Slack channels or Amazon Chime web hooks.
For Slack, after the Slack administrator approves AWS Chatbot support for the Slack workspace, anyone in the workspace can add AWS Chatbot to their Slack channels. For Amazon Chime, users with AWS Identity and Access Management (IAM) permissions to use Amazon Chime can add AWS Chatbot to their web hooks.

5. AWS CLOUD-FORMATION : < Has a separate detailed chapter >  https://www.youtube.com/watch?v=Omppm_YUG2g&t=8s
--> Create a JSON template with required resources and AWS does the rest.
AWS CloudFormation enables you to create and provision AWS infrastructure deployments predictably and repeatedly.
It helps you leverage AWS products such as Amazon EC2, Amazon Elastic Block Store, Amazon SNS, Elastic Load Balancing, and Auto Scaling to build highly reliable, highly scalable, cost-effective applications in the cloud without worrying about creating and configuring the underlying AWS infrastructure.
AWS CloudFormation enables you to use a template file to create and delete a collection of resources together as a single unit (a stack).

AWS CloudFormation allows you to use programming languages or a simple text file to model and provision, in an automated and secure manner, all the resources needed for your applications across all regions and accounts.
You create a template that describes all the AWS resources that you want (like Amazon EC2 instances or Amazon RDS DB instances), and AWS CloudFormation takes care of provisioning and configuring those resources for you.
You don't need to individually create and configure AWS resources and figure out what's dependent on what; AWS CloudFormation handles all that for you.


6. AWS CLOUD-TRAIL : < Has a separate detailed chapter >  https://www.youtube.com/watch?v=mXQSnbc9jMs
--> Integrates with CLOUD WATCH for a detailed monitoring and alarms.
AWS CloudTrail is an AWS service that can be used to monitor all user interactions with the AWS environment.
With AWS CloudTrail, you can monitor your AWS deployments in the cloud by getting a history of AWS API calls for your account, including API calls made via the AWS Management Console, the AWS SDKs, the command line tools, and higher-level AWS services.
You can also identify which users and accounts called AWS APIs for services that support CloudTrail, the source IP address the calls were made from, and when the calls occurred.
You can integrate CloudTrail into applications using the API, automate trail creation for your organization, check the status of your trails, and control how administrators turn CloudTrail logging on and off.

Event history allows you to view, search, and download the past 90 days of activity in your AWS account.
In addition, you can create a CloudTrail trail to archive, analyze, and respond to changes in your AWS resources.
A trail is a configuration that enables delivery of events to an Amazon S3 bucket that you specify. Subscribe to a SNS topic to receive notifications about log file delivery to your bucket.
You can also deliver and analyze events in a trail with Amazon CloudWatch Logs and Amazon CloudWatch Events.
INSIGHTS : Configure your trails to log Insights events to help you identify and respond to unusual activity associated with write management API calls.
--> While creating a trail in Console , we get the option if we want to send them to cloud watch . Enable it !!!!

// AWS CLOUD TRAIL is an AWS service that can be used to monitor all USER interactions / API CALLS with the AWS environment.
// CloudTrail provides event history of your AWS account activity, including actions taken through the AWS Management Console, AWS SDKs, command line tools, and other AWS services.
// So if you want to know who took an action on a particular resource CLOUD TRAIL is your choice.
// For a detailed info about a particular resource we need to use CLOUD WATCH.

7. AWS CLOUD-WATCH : < Has a separate detailed chapter > https://www.youtube.com/watch?v=a4dhoTQCyRA
Amazon CloudWatch is a service that monitors AWS cloud resources and the applications you run on AWS.
You can use Amazon CloudWatch to collect and track metrics, collect and monitor log files, set alarms, and automatically react to changes in your AWS resources.
Amazon CloudWatch can monitor AWS resources such as Amazon EC2 instances, Amazon DynamoDB tables, and Amazon RDS DB instances, as well as custom metrics generated by your applications and services, and any log files your applications generate.
You can use CloudWatch to detect anomalous behavior in your environments, take automated actions, troubleshoot issues, and discover insights to keep your applications running smoothly.

Amazon CloudWatch provides a reliable, scalable, and flexible monitoring solution that you can start using within minutes.
You no longer need to set up, manage, and scale your own monitoring systems and infrastructure.
It saves logs as metrics and events . You can set high resolution alarms , send SNS and take automatic actions !!

The CloudWatch home page automatically displays metrics about every AWS service you use.
You can additionally create custom dashboards to display metrics about your custom applications, and display custom collections of metrics that you choose.
You can create alarms that watch metrics and send notifications or automatically make changes to the resources you are monitoring when a threshold is breached.
For example, you can monitor the CPU usage and disk reads and writes of your Amazon EC2 instances and then use this data to determine whether you should launch additional instances to handle increased load.
You can also use this data to stop under-used instances to save money.

8. AWS COMMAND LINE INTERFACE : < Has a separate detailed chapter >
The AWS Command Line Interface (AWS CLI) is a unified tool that provides a consistent interface for interacting with all parts of AWS.
We have the latest CLI-v2.0 with more functionalities.
We download it and install and run commands in the inbuilt shell . --> cmd for windows , terminal for linux
HELP : aws help
       aws command help
       aws command subcommand help --> aws ec2 describe-instances help | more

       If any command returns errors , then use the --debug flag to see logs
       Use --debug option to see the debug logs for each step in a AWS command.

9. AWS COMPUTE OPTIMIZER :
AWS Compute Optimizer recommends optimal AWS compute resources for your workloads.
It can help you reduce costs and improve performance, by using machine learning to analyze your historical utilization metrics.
Compute Optimizer helps you choose the optimal resource configuration based on your utilization data.
It reports whether your resources are optimal, and generates optimization recommendations to reduce the cost and improve the performance of your workloads.
It also provides graphs showing recent utilization metric history data, as well as projected utilization for recommendations, which you can use to evaluate which recommendation provides the best price-performance trade-off.
The analysis and visualization of your usage patterns can help you decide when to move or resize your running resources, and still meet your performance and capacity requirements.
Compute Optimizer generates recommendations for the following resources:
    • Amazon Elastic Compute Cloud (Amazon EC2) instances
    • Amazon EC2 Auto Scaling groups
    • Amazon Elastic Block Store (Amazon EBS) volumes
    • AWS Lambda functions

10. AWS CONFIG : < Has a separate detailed chapter >  https://www.youtube.com/watch?v=MJDuAvNEv64&t=26s && https://www.youtube.com/watch?v=X_fznJtSyV8&t=8s
--> To assess , audit and evaluate the configuration of your resources . And to make sure resources are compliant with industry standards.
--> We get notified if a resource is non-compliant or matches a rule against compliance.
To configure the config , we need to make sure that the instances are managed by SYSTEMS MANAGER . So create a role and assign it to the instances.
AWS Config provides a detailed view of the resources associated with your AWS account, including how they are configured, how they are related to one another, and how the configurations and their relationships have changed over time.

11. AWS CONSOLE MOBILE APPLICATION :
The AWS Console Mobile Application, provided by Amazon Web Services, lets you view and manage a select set of resources to support incident response while on-the-go // or away from the computer.
With the Console Mobile Application, you can monitor resources and view configuration details, metrics,and alarms for a select subset of AWS services.
You can see an overview of the account status with real-time data on Amazon CloudWatch, AWS Personal Health Dashboard, and AWS Billing and Cost Management.
You can view ongoing issues and follow through to the relevant CloudWatch alarm screens for a detailed view with graphs and configuration options.
In addition, you can check on the status of specific AWS services, view detailed resource screens, and perform some actions.

12. AWS CONTROL TOWER : Organizations + SSO + Service Catalog
AWS Control Tower is a service that enables you to enforce and manage governance rules for security, operations, and compliance at scale across all your organizations and accounts in the AWS Cloud.
AWS Control Tower creates an abstraction or orchestration layer that combines and integrates the capabilities of several other AWS services, including AWS Organizations, AWS Single Sign-on, and AWS Service Catalog.
AWS Control Tower provides the easiest way to set up and govern a secure, compliant, multi-account AWS environment based on best practices established by working with thousands of enterprises.
If you are hosting more than a handful of accounts, it’s beneficial to have an orchestration layer that facilitates account deployment and account governance.
With AWS Control Tower, end users on your distributed teams can provision new AWS accounts quickly.
Meanwhile your central cloud administrators know that all accounts are aligned with centrally established, company-wide compliance policies.

  --> Landing zone – A landing zone is a well-architected, multi-account AWS environment that's based on security and compliance best practices.
      It is the enterprise-wide container that holds all of your organizational units (OUs), accounts, users, and other resources that you want to be subject to compliance regulation.
      A landing zone can scale to fit the needs of an enterprise of any size.
  --> Guardrails – A guardrail is a high-level rule that provides ongoing governance for your overall AWS environment. It's expressed in plain language.
      Two kinds of guardrails exist: preventive and detective.
      Three categories of guidance apply to the two kinds of guardrails: mandatory, strongly recommended, or elective.
  --> Account Factory – An Account Factory is a configurable account template that helps to standardize the provisioning of new accounts with pre-approved account configurations.
      AWS Control Tower offers a built-in Account Factory that helps automate the account provisioning workflow in your organization.

13. AWS DATA LIFE CYCLE MANAGER :
With Amazon Data Lifecycle Manager, you can manage the lifecycle of your AWS resources. You create lifecycle policies to automate operations on specified resources.
You can use Amazon Data Lifecycle Manager to automate the creation, retention, and deletion of EBS snapshots and EBS-backed AMIs.
When you automate snapshot and AMI management, it helps you to:
    •  Protect valuable data by enforcing a regular backup schedule.
    •  Create standardized AMIs that can be refreshed at regular intervals.
    •  Retain backups as required by auditors or internal compliance.
    •  Reduce storage costs by deleting outdated backups.
    •  Create disaster recovery backup policies that back up data to isolated accounts.
When combined with the monitoring features of Amazon CloudWatch Events and AWS CloudTrail, Amazon Data Lifecycle Manager provides a complete backup solution for Amazon EC2 instances and individual EBS volumes at no additional cost.

14. AWS HEALTH : // Personal Health Dashboard
AWS Health provides personalized information about events that can affect your AWS infrastructure, guides you through scheduled changes, and accelerates the troubleshooting of issues that affect your AWS resources and accounts.
All customers can use the Personal Health Dashboard (PHD), powered by the AWS Health API. The dashboard requires no setup, and it's ready to use for authenticated AWS users.
The Personal Health Dashboard organizes issues in three groups: open issues, scheduled changes, and other notifications.
When you choose an event in the Event log list, the Event details pane appears with information about the event and resources that are affected by the event.
--> Event logs show everything happening around the world in AWS .
--> You can view ACCOUNT SPECIFIC and also PUBLIC EVENTS [issues common for everybody in all regions say ec2 services down etc :]

AWS Personal Health Dashboard provides alerts and remediation guidance when AWS is experiencing events that may impact you.
While the Service Health Dashboard displays the general status of AWS services, Personal Health Dashboard gives you a personalized view into the performance and availability of the AWS services underlying your AWS resources.
AWS Personal Health Dashboard provides a personalized view of the health of the specific services that are powering your workloads and applications.
What’s more, Personal Health Dashboard proactively notifies you when AWS experiences any events that may affect you, helping provide quick visibility and guidance to help you minimize the impact of events in progress, and plan for any scheduled changes, such as AWS hardware maintenance.

// NOTE :::: Personal health dashboard is different from a Service health Dashboard.

You can get information about the current status and availability of the AWS services any time using the AWS Service Health Dashboard that is available at this link: https://status.aws.amazon.com/
The Service Health Dashboard is a good way to view the overall status of each AWS service, but provides little in terms of how the health of those services is impacting your resources.

BENIFITS OF PERSONAL HEALTH DASHBOARD :
-------------------------------------------
  ** PERSONALIZED VIEW OF SERVICE HEALTH :
     Personal Health Dashboard gives you a personalized view of the status of the AWS services that power your applications, enabling you to quickly see when AWS is experiencing issues that may impact you.
     For example, in the event of a lost EBS volume associated with one of your EC2 instances, you would gain quick visibility into the status of the specific service you are using, helping save precious time troubleshooting to determine root cause.

  ** PROACTIVE NOTIFICATIOPNS :
     The dashboard also provides forward looking notifications, and you can set up alerts across multiple channels, including email and mobile notifications, so you receive timely and relevant information to help plan for scheduled changes that may affect you.
     In the event of AWS hardware maintenance activities that may impact one of your EC2 instances, for example, you would receive an alert with information to help you plan for, and proactively address any issues associated with the upcoming change.

  ** DETAILED TROUBLESHOOTING GUIDANCE :
     When you get an alert, it includes remediation details and specific guidance to enable you to take immediate action to address AWS events impacting your resources.
     For example, in the event of an AWS hardware failure impacting one of your EBS volumes, your alert would include a list of your affected resources, a recommendation to restore your volume, and links to the steps to help you restore it from a snapshot.
     This targeted and actionable information reduces the time needed to resolve issues.


15. AWS LICENSE MANAGER  : Bring Your Own License - BYOL // Sends SNS if a license is expiring.
AWS License Manager streamlines the process of bringing software vendor licenses ( for example, Microsoft, SAP, Oracle, and IBM) to the cloud.
As you build out cloud infrastructure on AWS, you can save costs by using bring-your-own-license (BYOL) opportunities, that is, by repurposing your existing license inventory for use with cloud resources.
License Manager reduces the risk of licensing overages and penalties with inventory tracking that is tied directly into AWS services.
License Manager's built-in dashboards provide ongoing visibility into license usage and assistance with vendor audits.

16. AMAZON MANAGED SERVICE FOR GRAFANA - AMG :
(AMG) is a fully managed and secure data visualization service that enables you to instantly query, correlate, and visualize operational metrics, logs, and traces from multiple data sources.
AMG makes it easy to deploy, operate, and scale Grafana, a widely deployed open-source data visualization tool popular for its extensible data support.

17. AMAZON MANAGED SERVICE FOR PROMETHEUS - AMP :
It provides highly available, secure, and managed monitoring for your containers.
Amazon Managed Service for Prometheus is a serverless, Prometheus-compatible monitoring service for container metrics that makes it easier to securely monitor container environments at scale.
With AMP, you can use the same open-source Prometheus data model and query language that you use today to monitor the performance of your containerized workloads, and also enjoy improved scalability, availability, and security without having to manage the underlying infrastructure.

18. AWS MANAGEMENT CONSOLE :
Self - Explanatory .

19. AWS OPSWORKS : < Has a separate detailed chapter > https://www.youtube.com/watch?v=BhNfhHXvhhc
AWS OpsWorks provides a simple and flexible way to create and manage stacks and applications.
With AWS OpsWorks, you can provision AWS resources, manage their configuration, deploy applications to those resources, and monitor their health.

AWS OpsWorks is a configuration management service that helps you configure and operate applications in a cloud enterprise by using Puppet or Chef.
Chef and Puppet are automation platforms that allow you to use code to automate the configurations of your servers.
AWS OpsWorks Stacks and AWS OpsWorks for ChefAutomate let you use Chef cookbooks and solutions for configuration management, while OpsWorks for Puppet Enterprise lets you configure a Puppet Enterprise master server in AWS.
Puppet offers a set of tools for enforcing the desired state of your infrastructure, and automating on-demand tasks.

20. AWS ORGANIZATIONS : < Has a separate detailed chapter >
AWS Organizations is an account management service that lets you consolidate multiple AWS accounts into an organization that you create and centrally manage.
With AWS Organizations, you can create member accounts and invite existing accounts to join your organization.
You can organize those accounts and manage them as a group.
AWS Organizations includes account management and consolidated billing capabilities that enable you to better meet the budgetary, security, and compliance needs of your business.
As an administrator of an organization, you can create accounts in your organization and invite existing accounts to join the organization.

Basic organization that consists of "n" accounts can be  organized into "m" organizational units (OUs) under the root.
The organization also has several policies that are attached to some of the OUs or directly to accounts.

      ROOT ACCOUNT --> ORGANIZATIONAL UNIT - 1 --> Member - q ; Member - p
                   --> ORGANIZATIONAL UNIT - 2
                   --> ORGANIZATIONAL UNIT - 3 --> Member - a ; Member - b ; Member -c
                   -
AWS Organizations helps customers centrally govern their environments as they grow and scale their workloads on AWS.
Whether customers are a growing startup or a large enterprise, Organizations helps them to centrally manage billing; control access, compliance, and security; and share resources across their AWS accounts.
AWS Organizations has five main benefits:
  1) Centrally manage access polices across multiple AWS accounts.
  2) Automate AWS account creation and management.
  3) Control access to AWS services.
  4) Consolidate billing across multiple AWS accounts.
  5) Configure AWS services across multiple accounts.

21. AWS PROTON :
AWS Proton creates and manages standardized infrastructure and deployment tooling for developers and their serverless and container-based applications
The AWS Proton service is a two-pronged automation framework.
As an administrator, you can create service templates to provide standardized infrastructure and deployment tooling for serverless and container-based applications.
Then developers on your team, in turn, can select from the available service templates to automate their application or service deployments.
Platform teams can use AWS Proton to define and manage standard application stacks that contain the architecture, infrastructure resources, and the CI/CD software deployment pipeline.
When developers use the AWS Proton self-service interface to select a service template, they're selecting a standardized application stack definition for their code deployments.
Proton automatically provisions the resources, configures the CI/CD pipeline, and deploys the code into the defined infrastructure.

22. AWS SERVICE CATALOG :
AWS Service Catalog enables IT administrators to create, manage, and distribute portfolios of approved products to end users, who can then access the products they need in a personalized portal.
Typical products include servers, databases, websites, or applications that are deployed using AWS resources (for example, an Amazon EC2 instance or an Amazon RDS database).
You can control which users have access to specific products to enforce compliance with organizational business standards, manage product lifecycles, and help users find and launch products with confidence.

23. AWS SERVICE QUOTAS :
Service Quotas is a service for viewing and managing your quotas easily and at scale as your AWS workloads grow.
Quotas, also referred to as limits, are the maximum number of resources that you can create in an AWS account.
WS service defines its quotas and establishes default values for those quotas.
Depending on your business needs, you might need to increase your service quota values.
Service Quotas makes it easy to look up your service quotas and to request increases.

24. AWS SYSTEMS MANAGER :
Use AWS Systems Manager to organize, monitor, and automate management tasks on your AWS resources and also use to view and control your infrastructure onAWS.
AWS Systems Manager gives you visibility and control of your infrastructure on AWS.
Systems Manager provides a unified user interface so you can view operational data from multiple AWS services and enables you to automate operational tasks across your AWS resources.
AWS Systems Manager provides a unified user interface so you can view operational data from multiple AWS services and allows you to automate operational tasks across your AWS resources.

A managed instance is a machine that has been configured for use with Systems Manager.
Systems Manager also helps you configure and maintain your managed instances.
Supported machine types include Amazon Elastic Compute Cloud (Amazon EC2) instances, on-premises servers, and virtual machines (VMs), including VMs in other cloud environments.
Supported operating system types include Windows Server, macOS, Raspbian, and multiple distributions of Linux,.
Systems Manager helps you maintain security and compliance by scanning your managed instances and reporting on (or taking corrective action on)any policy violations it detects.

After tagging the required resources in a region , you can view a consolidated dashboard in Systems Manager that reports the status of all the resources that are part of the region.
If a problem arises with any of these resources, you can take corrective action immediately.

Systems Manager is comprised of individual capabilities , which are grouped into five categories:
--> Operations Management, Application Management, Change Management, Node Management, and Shared Resources
    • Group AWS resources together by any purpose or activity you choose, such as application, environment, region, project, campaign, business unit, or software lifecycle.
    • Centrally define the configuration options and policies for your managed instances.
    • Centrally view, investigate, and resolve operational work items related to AWS resources.
    • Automate or schedule a variety of maintenance and deployment tasks.• Use and create runbook-style SSM documents that define the actions to perform on your managed instances.
    • Run a command, with rate and error controls, that targets an entire fleet of managed instances.
    • Securely connect to a managed instance with a single click, without having to open an inbound port or manage SSH keys.
    • Separate your secrets and configuration data from your code by using parameters, with or without encryption, and then reference those parameters from a number of other AWS services.
    • Perform automated inventory by collecting metadata about your Amazon EC2 and on-premises managed instances. Metadata can include information about applications, network configurations, and more.
    • View consolidated inventory data from multiple AWS Regions and accounts that you manage.
    • Quickly see which resources in your account are out of compliance and take corrective action from a centralized dashboard.
    • View active summaries of metrics and alarms for your AWS resources.

25. AWS TOOLS FOR POWER-SHELL :
AWS Tools for PowerShell and AWS Tools for PowerShell Core are PowerShell modules, built on functionality exposed by the AWS SDK for .NET, that enable you to script operations on AWS resources from the PowerShell command line.
Although you use the SDK’s service clients and methods to implement the cmdlets, the cmdlets give you a PowerShell experience to specify parameters and handle results.
For example, the cmdlets in both modules support PowerShell pipelining to pipe PowerShell objects to and from the cmdlets.

26. AWS TRUSTED ADVISOR : < Has a separate detailed chapter >
AWS Support provides support for users of Amazon Web Services.
All users have access to account and billing help in the AWS Support Center.
In addition, customers with some support plans have access to additional features, including AWS Trusted Advisor and an API for programmatic access to support cases and Trusted Advisor.

AWS Trusted Advisor is an online tool that provides you real time guidance to help you provision your resources following AWS best practices.
Trusted Advisor checks help optimize your AWS infrastructure, increase security and performance, reduce your overall costs, and monitor service limits.
Whether establishing new workflows, developing applications, or as part of ongoing improvement, take advantage of the recommendations provided by Trusted Advisor on a regular basis to help keep your solutions provisioned optimally.

AWS Basic Support and AWS Developer Support customers get access to 6 security checks (S3 Bucket Permissions, Security Groups - Specific Ports Unrestricted, IAM Use, MFA on Root Account, EBS Public Snapshots, RDS Public Snapshots) and 50 service limit checks.
AWS Business Support and AWS Enterprise Support customers get access to all 115 Trusted Advisor checks (14 cost optimization, 17 security, 24 fault tolerance, 10 performance, and 50 service limits) and recommendations.

27. AWS WELL-ARCHITECTURED TOOL :
Use the AWS Well-Architected Tool to review your workloads against current AWS architectural best practices.
The AWS Well-Architected Tool measures the workload and provides recommendations on how to improve your architecture.
  • Assisting with documenting the decisions that you make
  • Providing recommendations for improving your workload based on best practices
  • Guiding you in making your workloads more reliable, secure, efficient, and cost-effective

Today, you can use AWS WA Tool to document and measure your workload using the best practices from the AWS Well-Architected Framework.
These best practices were developed by AWS Solutions Architects based on their years of experience building solutions across a wide variety of businesses.
The frame work provides a consistent approach for measuring architectures and provides guidance for implementing designs that scale with your needs over time.



##################################################################################################################
##################################################################################################################
MIGRATION & TRANSFER :
##################################################################################################################
##################################################################################################################

// Classical way of DB migration can be time consuming, difficult and expensive . Users also can not use the DB while in transfer.
// Also converting data into a new data base type is difficult .

--> NETWORK BASED SERVICES : Private Network Connectors to AWS            - AWS DIRECT CONNECT
                             Edge Locations for S3 Enabled applications   - AMAZON S3 TRANSFER ACCELERATION

--> ONLINE  DATA TRANSFER :  Load stream Data into S3                     - AMAZON KINESIS
                             Online transfer of active data               - AWS DATA SYNC
                             SFTP transfer to S3                          - AWS TRANSFER FOR SFTP
                             Database Transfer                            - AWS DATABSE MIGRATION
                             VM Image transfer - Live applications        - AWS SERVER MIGRATION SERVICES // AWS ENDURE

--> OFFLINE DATA TRANSFER :  Ship Static data in and out of AWS           - AWS SNOW FAMILY [ Snow cone / Snow Ball / Snow Mobile ]

--> HYBRID DATA STORAGE   :  Access AWS storage from On-premises          - AWS STORAGE GATEWAY


1. AWS APPLICATION DISCOVERY SERVICE :  https://www.youtube.com/watch?v=TYiqPrCA4fc
--> Deploy Discovery Agent on resources  - Migrate to cloud // Integrated with MIGRATION HUB //
It helps you plan your migration to the AWS cloud quickly and reliably plan application migration projects by automatically identifying applications running in on-premises data centres, their associated dependencies, and their performance profile.
It is integrated with AWS Migration Hub, which simplifies your migration tracking as it aggregates your migration status information into a single console.
You can view the discovered servers, group them into applications, and then track the migration status of each application from the Migration Hub console in your home region.
Application Discovery Service integrates with application discovery solutions from AWS Partner Network(APN) partners.
These third-party solutions can help you import details about your on-premises environment directly into Migration Hub, without using any discovery connector or discovery agent.

The AWS Discovery Agent is AWS software that you install on on-premises servers and VMs targeted for discovery and migration.
Agents capture system configuration, system performance, running processes, and details of the network connections between systems.
Agents support most Linux and Windows operating systems, and you can deploy them on physical on-premises servers, Amazon EC2 instances, and virtual machines.
After registration, it pings the service at 15-minute intervals for configuration information.

You can get data about your servers and applications into the AWS Migration Hub console for migration tracking in three ways:
-->  Migration Hub import, the AWS Agentless Discovery Connector, and the AWS Application Discovery Agent.

2. AWS DATABASE MIGRATION SERVICE - DMS : https://www.youtube.com/watch?v=ouia1Sc5QGo&t=16s // AWS SCHEMA CONVERSION TOOL
--> Migrate Data base to and from Cloud to on premises. // Bidirectional
AWS Database Migration Service (AWS DMS) is a cloud service that makes it easy to migrate relational databases, data warehouses, NoSQL databases, and other types of data store.
These services can include a database on Amazon RDS or a database on an Amazon EC2 instance. You can also migrate a database from an AWS service to an on-premises database.
You can migrate data between heterogeneous or homogenous database engines.

With AWS DMS, you can perform one-time migrations, and you can replicate ongoing changes to keep sources and targets in sync.
If you want to change database engines, you can use the AWS Schema Conversion Tool (AWS SCT) to translate your database schema to the new platform.
And then use AWS DMS to migrate to cloud . Any data that can not be converted is marked for review later .

AWS Database Migration Service (AWS DMS) is a web service that you can use to migrate data from a source data store to a target data store.
These two data stores are called endpoints. You can migrate between source and target endpoints that use the same database engine, such as from an Oracle database to an Oracle database.
You can also migrate between source and target endpoints that use different database engines, such as from an Oracle database to a PostgreSQL database.
The only requirement to use AWS DMS is that one of your endpoints must be on an AWS service.
You can't use AWS DMS to migrate from an on-premises database to another on-premises database.

At a high level, when using AWS DMS you do the following:
  --> • Create a replication server.
  --> • Create source and target endpoints that have connection information about your data stores.
  --> • Create one or more migration tasks to migrate data between the source and target data stores.
A task can consist of three major phases:
  --> • The full load of existing data
  --> • The application of cached changes
  --> • Ongoing replication

While the replication is going on , all the transactions are cached and the cached data is migrated at the end to not disturb the flow of business or miss any data transaction.

// DMS is for DATA BASE migration
// DATA SYNC is for DATA migration

3. AWS DATA-SYNC : Online Transfer // https://www.youtube.com/watch?v=jPRquig6Nrw
AWS DataSync is an online data transfer service that simplifies, automates, and accelerates moving data between on-premises storage systems and AWS storage services, and also among  AWS storage services themselves .
DataSync can copy data between Network File System (NFS), Server Message Block (SMB) fileservers, self-managed object storage, AWS Snow cone, Amazon Simple Storage Service (Amazon S3)buckets, Amazon EFS file systems, and Amazon FSx for Windows File Server file systems.

                     ON_PREMISIS                                                          AWS CLOUD
                                                              TLS
    Shared File system <----> AWS Data Sync agent  <========================> AWS Data Sync <----> AWS S3 / EFS

4. AWS MIGRATION-HUB :
AWS Migration Hub provides a single location to track migration tasks across multiple AWS tools and partner solutions.
With Migration Hub, you can choose the AWS and partner migration tools that best fit your needs while providing visibility into the status of your migration projects.
Migration Hub also provides key metrics and progress information for individual applications, regardless of which tools are used to migrate them.
Migration Hub supports migration status updates from the following AWS services:
  --> • AWS Database Migration Service
  --> • AWS Server Migration Service
  --> • AWS Cloud Endure Migration

5. AWS SCHEMA CONVERSION TOOL : // USED BY AWS-DMS
The AWS Schema Conversion Tool makes heterogeneous database migrations easy by automatically converting the source database schema and a majority of the custom code to a format compatible with the target database.
The custom code that the tool converts includes views, stored procedures, and functions.
Any code that the tool cannot convert automatically is clearly marked so that you can convert it yourself.
You can convert relational OLTP schema, or data warehouse schema.
Your converted schema is suitable for an Amazon Relational Database Service (Amazon RDS) MySQL, MariaDB, Oracle, SQL Server, PostgreSQL DB, an Amazon Aurora DB cluster, or an Amazon Redshift cluster.
The converted schema can also be used with a database on an Amazon EC2 instance or stored as data on an Amazon S3 bucket.

  SCT EINDOW : view Ref pic.
  1. In the left panel, the schema from your source database is presented in a tree view. Your database schema is "lazy loaded."
     In other words, when you select an item from the tree view, AWS SCT gets and displays the current schema from your source database.
  2. In the top middle panel, action items appear for schema elements from the source database engine that couldn't be converted automatically to the target database engine.
  3. In the right panel, the schema from your target DB instance is presented in a tree view. Your database schema is "lazy loaded."
     That is, at the point when you select an item from the tree view, AWS SCT gets and displays the current schema from your target database.
  4. In the lower left panel, when you choose a schema element, properties describing the source schema element and the SQL command to create that element in the source database are displayed.
  5. In the lower right panel, when you choose a schema element, properties describing the target schema element and the SQL command to create that element in the target database are displayed.
      You can edit this SQL command and save the updated command with your project.

6. AWS SERVER MIGRATION SERVICE : Microsoft or Azure to AWS
AWS Server Migration Service (AWS SMS) combines data collection tools with automated server replication to speed the migration of on-premises servers to AWS.
AWS Server Migration Service automates the migration of your on-premises VMware vSphere, Microsoft Hyper-V/SCVMM, and Azure virtual machines to the AWS Cloud.
AWS SMS incrementally replicates your server VMs as cloud-hosted Amazon Machine Images (AMIs) ready for deployment on Amazon EC2.
Working with AMIs, you can easily test and update your cloud-based images before deploying them in production.

The Server Migration Connector is a FreeBSD VM that you install in your on-premises virtualization environment.
The supported platforms are VMware vSphere, Microsoft Hyper-V/SCVMM, and Microsoft Azure. We need to install this on the on premises / virtual VM.
Now open the Server Migration Service on AWS and do the transfer.

7. AWS SNOW FAMILY :
The AWS Snow Family is a service that helps customers who need to run operations in austere, non-data centre environments, and in locations where there's no consistent network connectivity.
You can use these devices to locally and cost-effectively access the storage and compute power of the AWS Cloud in places where an internet connection might not be an option.

AWS Snowcone [2kg] is a portable, rugged, and secure device for edge computing and data transfer.
You can use Snowcone to collect, process, and move data to AWS, either offline by shipping the device to AWS, or online by using AWS DataSync.
With two CPUs and 8 TB of storage, Snowcone can run edge computing workloads that use Amazon Elastic Compute Cloud (Amazon EC2) instances, and store data securely.
you can carry one in a backpack, use it with battery-based operation, and use the Wi-Fi interface to gather sensor data.
Snowcone supports a file interface with NFS support. The Snowcone device supports data transfer from on-premises Windows, Linux, and macOS servers and file-based applications through the NFS interface.
It can be used in space-constrained environments where Snowball Edge devices don't fit.

-> Snow cone        - https://www.youtube.com/watch?v=X_8LM7E_hiE      -- 8 TB storage   --> 1 SMALL BOX [ uses opshub application for data transfer ]
   Snow Ball        - https://www.youtube.com/watch?v=9uc2DSZ1wL8&t=9s -- 80 TB storage  --> 1 SUITCASE
   Snow Ball edge   - https://www.youtube.com/watch?v=bxSD1Nha2k8      -- 100 TB storage --> 1 SUITCASE  [ Embedded Ec2 - Lambda for running jobs in remote areas like ships , hills for an immediate results. ]
   Snow Mobile      - https://www.youtube.com/watch?v=8vQmTZTq7nw      -- 1 Exabyte      --> 1 Truck

  1 Exabyte = 1000 petabytes
  Snow ball can be connected in series to have a Petabyte's storage  = 1 million GB = 1024 Terabytes

8. AWS TRANSFER FAMILY :
AWS Transfer Family is a secure transfer service that stores your data in Amazon S3 and simplifies the migration of :
  --> Secure Shell File Transfer Protocol (SFTP),
  --> File Transfer Protocol Secure (FTPS), and
  --> File Transfer Protocol (FTP) workflows to AWS.

AWS Transfer Family is a secure transfer service that enables you to transfer files into and out of AWSstorage services.
Supports transferring data from the following :
  --> Amazon Simple Storage Service (Amazon S3) storage.
  --> Amazon Elastic File System (Amazon EFS) Network File System (NFS) file system

You can get started with AWS Transfer Family by creating a file transfer protocol-enabled server and then assigning users to use the server.
To service your AWS Transfer Family users' transfer requests, you create an AWS Identity and Access Management (IAM) role to access your Amazon S3 bucket or Amazon Elastic File System.
AWS Transfer Family supports any standard file transfer protocol client. Some commonly used clients are the following:
  -->  OpenSSH – A Macintosh and Linux command line utility.
  -->  WinSCP – A Windows-only graphical client.
  -->  Cyberduck – A Linux, Macintosh, and Microsoft Windows graphical client.
  -->  FileZilla – A Linux, Macintosh, and Windows graphical client.

STEP-1 : Sign into Aws - Transfer family
STEP-2 : Create an SFTP enabled server
         Secure Shell (SSH) File Transfer Protocol (SFTP) is a network protocol used for secure transfer of data over the internet.
         The protocol supports the full security and authentication functionality of SSH.
         It is widely used to exchange data, including sensitive information between business partners in a variety of industries such as financial services, healthcare, retail, and advertising.
         create server -> SFTP
STEP-3 : Add a user to the created server and provide an S3 bucket as home folder for the transferred files to be stored.
STEP-4 : Open any client and initiate transfer.



##################################################################################################################
##################################################################################################################
NETWORKING & CONTENT DELIVERY :
##################################################################################################################
##################################################################################################################

1. AMAZON API GATEWAY :
Amazon API Gateway is an AWS service for creating, publishing, maintaining, monitoring, and securing REST, HTTP, and Web Socket APIs at any scale.
You can create robust, secure, and scalable APIs that access AWS or other web services, as well as data that’s stored in the AWS Cloud.
You can create APIs to use in your own client applications, or you can make your APIs available to third-party app developers.
API Gateway creates RESTful APIs that:
  • Are HTTP-based.
  • Enable stateless client-server communication.
  • Implement standard HTTP methods such as GET, POST, PUT, PATCH, and DELETE.

 API Gateway handles all the tasks involved in accepting and processing up to hundreds of thousands of concurrent API calls.
 These tasks include traffic management, authorization and access control, monitoring, and API version management.
 API Gateway acts as a "front door" for applications to access data, business logic, or functionality from your backend services, such as workloads running on Amazon Elastic Compute Cloud (Amazon EC2), code running on AWS Lambda, any web application, or real-time communication applications.
 <NEED TO ADD EXTRA INFORMATION , LINK IN NOTEPAD++>  -- https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-dg.pdf

2. AWS APP MESH :
AWS App Mesh makes it easy to monitor and control microservices running on AWS.
App Mesh standardizes how your services communicate, giving you end-to-end visibility and helping to ensure high availability for your applications.
App Mesh gives you consistent visibility and network traffic controls for every service in an application.

Say you have a serviceA sending requests to ServieB . Now if you create a new version of ServiceB anD ServiceB-V2 and you want to send 50% requests to ServiceB-V2 and the remaining 50% of traffic to ServiceB until you are confident that it is working good .
This can be made posssible without making any application level changes using app mesh.
// Refer to the pics NW_App-Mesh.* for the scenario.

3. AWS CLOUD MAP :
AWS Cloud Map is a fully managed service that you can use to create and maintain a map of the backend services and resources that your applications depend on.
  1. You create a namespace that identifies the name that you want to use to locate your resources and also specifies how you want to locate resources: using AWS Cloud Map DiscoverInstances API calls, DNS queries in a VPC, or public DNS queries.
     Typically, a namespace contains all the services for an application, such as a billing application.
  2. You create an AWS Cloud Map service for each type of resource for which you want to use AWS CloudMap to locate endpoints.
     For example, you might create services for web servers and database servers.
     A service is a template that AWS Cloud Map uses when your application adds another resource, such as another web server.
  3. When your application adds a resource, it can call the AWS Cloud Map RegisterInstance API action, which creates a service instance.
     The service instance contains information about how your application can locate the resource, whether using DNS or using the AWS Cloud Map DiscoverInstances API action.
  4. When your application needs to connect to a resource, it calls DiscoverInstances and specifies the namespace and service that are associated with the resource.
     AWS Cloud Map returns information about how to locate one or more resources.

  AWS Cloud Map is tightly integrated with Amazon Elastic Container Service (Amazon ECS).
  As new container tasks spin up or down, they automatically register with AWS Cloud Map.

4. AMAZON CLOUD-FRONT : < Has a separate detailed chapter >  https://www.youtube.com/watch?v=AT-nHW3_SVI&t=9s
Amazon CloudFront speeds up distribution of your static and dynamic web content, such as .html, .css, .php, image, and media files.
When users request your content, CloudFront delivers it through a worldwide network of edge locations that provide low latency and high performance.
When a user requests content that you're serving with CloudFront, the request is routed to the edge location that provides the lowest latency (time delay), so that content is delivered with the best possible performance.
If the content is not present then cloudfront stores of copy of it in cache after retrieving it from origin .
--> POP Point of Presence // another name for an edge location
--> Along with EDGE LOCATIONS , it also maintains a REGIONAL EDGE CACHES.

Amazon CloudFront is a global content delivery network (CDN) service that gives businesses and web application developers an easy and cost effective way to distribute content (such as videos, data, applications, and APIs) with low latency and high data transfer speeds.
Like other AWS services, Amazon CloudFront is a self-service, pay-per-use offering, requiring no long term commitments or minimum fees.
With CloudFront, your files are delivered to end-users using a global network of edge locations.
CloudFront is integrated with other AWS services such as AWS Shield for DDoS mitigation, Amazon S3, Elastic Load Balancing or Amazon EC2 as origins for your applications, and Lambda@Edge to run custom code close to your viewers.

CloudFront also has regional edge caches that bring more of your content closer to your viewers, even when the content is not popular enough to stay at a POP, to help improve performance for that content.
Regional edge caches are CloudFront locations that are deployed globally, close to your viewers.
They'r elocated between your origin server and the POPs—global edge locations that serve content directly to viewers. As objects become less popular, individual POPs might remove those objects to make roomfor more popular content.
Regional edge caches have a larger cache than an individual POP, so objects remain in the cache longer at the nearest regional edge cache location.
This helps keep more of your content closer to your viewers, reducing the need for CloudFront to go back to your origin server, and improving overall performance for viewers.
When a viewer makes a request on your website or through your application, DNS routes the request to the POP that can best serve the user’s request.
This location is typically the nearest CloudFront edge location in terms of latency. In the POP, CloudFront checks its cache for the requested files. If the files are in the cache, CloudFront returns them to the user.
If the files are not in the cache, the POPs go to the nearest regional edge cache to fetch the object. In the regional edge cache location, CloudFront again checks its cache for the requested files.
If the files are in the cache, CloudFront forwards the files to the POP that requested them. As soon as the first byte arrives from regional edge cache location, CloudFront begins to forward the files to the user.
CloudFront also adds the files to the cache in the POP for the next time someone requests those files.

            ORIGIN [ S3 ] <----------> [ REGIONAL EDGE CACHE ] <-------> [ POP - EDGE LOCATION ] <---------> VIEWERS

          •  Proxy methods PUT/POST/PATCH/OPTIONS/DELETE go directly to the origin from the POPs and do not proxy through the regional edge caches.
          •  Dynamic requests, as determined at request time, do not flow through regional edge caches, but go directly to the origin.

5. ELASTIC LOAD BALANCING : < Has a separate detailed chapter >
Elastic Load Balancers do not scale resources. Elastic Load Balancers distributes incoming application traffic across multiple targets, such as Amazon EC2 instances, containers, IP addresses, and Lambda functions.

Elastic Load Balancing automatically distributes your incoming traffic across multiple targets, such as EC2 instances, containers, and IP addresses, in one or more Availability Zones.
It monitors the health of its registered targets and routes traffic only to the healthy targets.
You can add and remove compute resources from your load balancer as your needs change, without disrupting the overall flow of requests to your applications.

Application Load Balancers - Network Load Balancers - Gateway Load Balancers - Classic Load Balancers.
If you need flexible application management, we recommend that you use an Application Load Balancer.
If extreme performance and static IP is needed for your application, we recommend that you use a Network Load Balancer.
If you have an existing application that was built within the EC2-Classic network, then you should use a Classic Load Balancer.
https://aws.amazon.com/elasticloadbalancing/features/#Product_comparisons

// Elastic Load Balancers do not scale resources.
// Elastic Load Balancers distributes incoming application traffic across multiple targets, such as Amazon EC2 instances, containers, IP addresses, and Lambda functions.

// Scaling resources is a function of AWS AUTO SCALING.
// AWS Auto Scaling is the feature that automates the process of adding/removing server capacity (based on demand).
// Autoscaling allows you to reduce your costs by automatically turning off resources that aren’t in use.
// On the other hand, Autoscaling ensures that your application runs effectively by provisioning more server capacity if required.


6. AWS DIRECT CONNECT : < Has a separate detailed chapter >
A Direct Connect gateway is a globally available resource. You can create the Direct Connect gateway in any Region and access it from all other Regions.
AWS Direct Connect links your internal network to an AWS Direct Connect location over a standard 1 gigabit or 10 gigabit Ethernet fiber-optic cable.
One end of the cable is connected to your router, the other to an AWS Direct Connect router.
With this connection in place, you can create virtual interfaces directly to the AWS cloud and Amazon Virtual Private Cloud, bypassing Internet service providers in your network path.
AWS Direct Connect has two billing elements: port hours and outbound data transfer. Port hour pricing is determined by capacity and connection type (dedicated connection or hosted connection).

You can therefore use a single AWS Direct Connect connection to build multi-Region services. This connection an be configured to access a VPC in your account in any other public Region
Accessing public services in a remote region :
--> To access public resources in a remote Region, you must set up a public virtual interface and establish a Border Gateway Protocol (BGP) session.
Accessing VPC's in a remote region :
--> You can create a Direct Connect gateway in any public Region. Use it to connect your AWS Direct Connect connection over a private virtual interface to VPCs in your account that are located in different Regionsor to a transit gateway.

There are two types of connections:
• Dedicated Connection: A physical Ethernet connection associated with a single customer. Customers can request a dedicated connection through the AWS Direct Connect console, the CLI, or the API.
    Speeds :  1Gbps and 10Gbps
• Hosted Connection: A physical Ethernet connection that an AWS Direct Connect Partner provisions on behalf of a customer. Customers request a hosted connection by contacting a partner in the AWSDirect Connect Partner Program, who provisions the connection.
    Speeds :  50Mbps, 100Mbps, 200Mbps, 300Mbps, 400Mbps,500Mbps, 1Gbps, 2Gbps, 5Gbps, and 10Gbps.

You must create one of the following virtual interfaces to begin using your AWS Direct Connect connection.
• Private virtual interface: Access an Amazon VPC using private IP addresses.
• Public virtual interface: Access AWS services from your on-premises data center.
  Allow AWS services, or AWS customers access to your public networks over the interface instead of traversing the internet.
• Transit virtual interface: Access one or more Amazon VPC Transit Gateways associated with DirectConnect gateways.
  You can use transit virtual interfaces with 1/2/5/10 Gbps AWS Direct Connect connections.

Use AWS Direct Connect gateway to connect your VPCs. You associate an AWS Direct Connect gateway with either of the following gateways:
• A transit gateway when you have multiple VPCs in the same Region
• A virtual private gateway


7. AWS GLOBAL ACCELERATOR :
AWS Global Accelerator is a service in which you create accelerators to improve the performance of your applications for local and global users.
• By using a standard accelerator, you can improve availability of your internet applications that are used by a global audience.
  With a standard accelerator, Global Accelerator directs traffic over the AWS global network to endpoints in the nearest Region to the client.
  Endpoints for standard accelerators can be Network Load Balancers, Application Load Balancers, Amazon EC2 instances, or Elastic IP addresses that are located in one AWS Region or multiple Regions.
• By using a custom routing accelerator, you can map one or more users to a specific destination among many destinations.
  custom routing accelerators only support virtual private cloud (VPC) subnet endpoint types and route traffic to private IP addresses in that subnet.

The static IP addresses provided by AWS Global Accelerator serve as single fixed entry points for your clients.
When you set up your accelerator with Global Accelerator, you associate the static IP addresses to regional endpoints in one or more AWS Regions.
From the edge location, traffic for your application is routed based on the type of accelerator that you configure.
  • For standard accelerators, traffic is routed to the optimal AWS endpoint based on several factors, including the user’s location, the health of the endpoint, and the endpoint weights that you configure.
  • For custom routing accelerators, each client is routed to a specific Amazon EC2 instance and port in a VPC subnet, based on the external static IP address and listener port that you provide

Traffic travels over the well-monitored, congestion-free, redundant AWS global network to the endpoint.
By maximizing the time that traffic is on the AWS network, Global Accelerator ensures that traffic is always routed over the optimum network path.
Global Accelerator terminates TCP connections from clients at AWS edge locations and, almost concurrently, establishes a new TCP connection with your endpoints. This gives clients faster response times (lower latency) and increased throughput.

In standard accelerators, Global Accelerator continuously monitors the health of all endpoints, and instantly begins directing traffic to another available endpoint when it determines that an active endpoint is unhealthy.
This allows you to create a high-availability architecture for your applications on AWS.
Health checks aren't used with custom routing accelerators and there is no failover, because you specify the destination to route traffic to.

8. AMAZON ROUTE 53 : < Has a separate detailed chapter >  https://www.youtube.com/watch?v=RGWgfhZByAI
Amazon Route 53 is a highly available and scalable Domain Name System (DNS) web service.
You can use Route 53 to perform three main functions in any combination: domain registration, DNS routing, and health checking.
  1. Register domain names : Your website needs a name, such as example.com.
    Route 53 lets you register a name for your website or web application, known as a domain name.
  2. Route internet traffic to the resources for your domain When a user opens a web browser and enters your domain name (example.com) or subdomain name(acme.example.com) in the address bar, Route 53 helps connect the browser with your website or web application
  3. Check the health of your resources Route 53 sends automated requests over the internet to a resource, such as a web server, to verify that it's reachable, available, and functional.
     You also can choose to receive notifications when a resource becomes unavailable and choose to route internet traffic away from unhealthy resources.

9. AMAZON VPC : < Has a separate detailed chapter >
Amazon Virtual Private Cloud (Amazon VPC) enables you to provision a logically isolated section of the AWS Cloud where you can launch AWS resources in a virtual network that you've defined.
A virtual private cloud (VPC) is a virtual network dedicated to your AWS account. It is logically isolated from other virtual networks in the AWS Cloud.
You can launch your AWS resources, such as Amazon EC2instances, into your VPC. You can specify an IP address range for the VPC, add subnets, associate security groups, and configure route tables.
A subnet is a range of IP addresses in your VPC. You can launch AWS resources into a specified subnet.
Use a public subnet for resources that must be connected to the internet, and a private subnet for resources that won't be connected to the internet .
To protect the AWS resources in each subnet, you can use multiple layers of security, including security groups and network access control lists (ACL).

   ***REFER THE CHAPTER FOR MORE CLEAR DETAILS .

10. AWS VPN : < Has a separate detailed chapter >
AWS Virtual Private Network (AWS VPN) lets you establish a secure and private tunnel from your network or device to the AWS Cloud.
You can extend your existing on-premises network into a VPC, or connect to other AWS resources from a client.
AWS VPN offers two types of private connectivity that feature the high availability and robust security necessary for your data.

By default, instances that you launch into an Amazon VPC can't communicate with your own (remote)network.
You can enable access to your remote network from your VPC by creating an AWS Site-to-Site VPN (Site-to-Site VPN) connection, and configuring routing to pass traffic through the connection.
A virtual private gateway is the VPN concentrator on the Amazon side of the Site-to-Site VPN connection.
You create a virtual private gateway and attach it to the VPC from which you want to create the Site-to-Site VPN connection.

To set up a Site-to-Site VPN connection, use the following steps:
  •Step 1: Create a customer gateway
  •Step 2: Create a target gateway --> Can be a virtual private gateway or transit gateway
  •Step 3: Configure routing
  •Step 4: Update your security group
  •Step 5: Create a Site-to-Site VPN connection
  •Step 6: Download the configuration file
  •Step 7: Configure the customer gateway device using the configuration file.




##################################################################################################################
##################################################################################################################
FRONT END WEB & MOBILE :
##################################################################################################################
##################################################################################################################

1. AWS AMPLIFY :
AWS Amplify enables developers to develop and deploy cloud-powered mobile and web apps.
The Amplify Framework is a comprehensive set of SDKs, libraries, tools, and documentation for client app development.
The Amplify Console provides a continuous delivery and hosting service for web applications.

The AWS Amplify Console is the control center for full stack web and mobile application deployments in AWS.
Amplify Console provides two main services, hosting and the Admin UI.
Amplify Console hosting provides a git-based workflow for hosting fullstack serverless web apps with continuous deployment.
The Admin UI is a visual interface for frontend web and mobile developers to create and manage app backends outside the AWS console.

2. AWS APP-SYNC :
AWS AppSync is an enterprise level, fully managed GraphQL service with real-time data synchronization and offline programming features.

3. AWS DEVICE FARM :
AWS Device Farm is an app testing service that enables you to test your iOS, Android and Fire OS apps on real, physical phones and tablets that are hosted by AWS.
The service allows you to upload your own tests or use built-in, script-free compatibility tests.
    • Automated testing of apps using a variety of testing frameworks.
    • Remote access of devices onto which you can load, run, and interact with apps in real time. -> Remotely work on someones phone
    Note : Device Farm is only available in the us-west-2 (Oregon) region

Device Farm allows you to upload your own tests or use built-in, script-free compatibility tests.
Because testing is performed in parallel, tests on multiple devices begin in minutes.
As tests are completed, a test report that contains high-level results, low-level logs, pixel-to-pixel screenshots, and performance data is updated.

Remote access allows you to swipe, gesture, and interact with a device through your web browser in real time.
There are a number of situations where real-time interaction with a device is useful.
For example, customer service representatives can guide customers through the use or setup of their device.
They can also walk customers through the use of apps running on a specific device.
You can install apps on a device running in a remote access session and then reproduce customer problems or reported bugs.

4. AWS LOCATION SERVICE :
With Amazon Location Service, you can easily and securely add maps, points of interest, geocoding, geofences, and tracking to your applications.
Amazon Location provides cost-effective location-based services (LBS) using high-quality data from global, trusted providers Esri and HERE.
Additionally, sensitive tracking and geofencing location information, such as facility, asset, and personnel locations, never leaves your AWS account at all.
This helps you shield sensitive information from third parties, protect user privacy, and reduce your application’s security risks.
With Amazon Location, neither Amazon nor third parties have rights to sell your data or use it for advertising.

5. AMAZON PINPOINT :
Amazon Pinpoint helps you engage your customers by sending them email, SMS and voice messages, and push notifications.
You can use Amazon Pinpoint to send targeted messages (such as promotions and retention campaigns), as well as transactional messages (such as order confirmations and password reset messages).
you have to add customer contact information into Amazon Pinpoint, and then create segments that target certain customers.
Next, you have to create your messages and schedule your campaigns.
Finally, after you send your campaigns, you can use the analytics dashboards that are built into Amazon Pinpoint to see how well the campaigns performed.


6. AMAZON SIMPLE NOTIFICATION SERVICE - SNS : < Has a separate detailed chapter >
It is a web service that enables applications, end-users, and devices to instantly send and receive notifications from the cloud.
We create topics and subscribe to the topic for messages .
Clients can subscribe to the SNS topic and receive published messages usinga supported protocol, such as Amazon Kinesis Data Firehose, Amazon SQS, AWS Lambda, HTTP, email,mobile push notifications, and mobile text messages (SMS).

    • Application-to-application messaging
    • Application-to-person notifications
    • Standard and FIFO topics
    • Message delivery retry
    • Dead-letter queues -- an Amazon SQS queue for messages that can't be delivered successfully due to client errors or server error.
    • Message attributes -- we can provide arbitrary metadata .
    • Message filtering -- filtering based on policies
    • Message security





##################################################################################################################
##################################################################################################################
END USER COMPUTING :
##################################################################################################################
##################################################################################################################

// The following are used if you have a corporate N/W or if you are an enterprise and you want to provide services to your end users.
   All the services are monitored by an admin and he provides / revokes accesses to users .
   Mainly used by employees working remotely.
   This also make sures that the main organizational information is not stored on users local machine.
// software developers in an organization can use Amazon WorkSpaces to access all desktop resources from any computer or tablet.
   Engineers can use AppStream 2.0 to stream GPU intensive apps.
   And sales leaders can use Amazon WorkLink to access internal web-based content, such as sales data, from their mobile devices.


1. AMAZON WORKSPACES : https://www.youtube.com/watch?v=uCH04Mlg0yU&t=9s
Amazon WorkSpaces enables you to provision virtual, cloud-based Microsoft Windows or Amazon Linux desktops for your users, known as WorkSpaces.
Amazon WorkSpaces eliminates the need to procure and deploy hardware or install complex software.
You can quickly add or remove users as your needs change. Users can access their virtual desktops from multiple devices or web browsers.
As an admin you have the rights to remove add users and assign applications to users using WAM.

Amazon WorkSpaces offers an easy way to provide a cloud-based desktop experience to your end users.
Select from a choice of bundles that offer a range of different amounts of CPU, memory, storage, and a choice of applications.
Users can connect from a PC, Mac desktop computer, iPad, Kindle, or Android tablet.

  • Connect to your WorkSpace and pick up from right where you left off. Amazon WorkSpaces provides a persistent desktop experience.
  • Amazon WorkSpaces provides the flexibility of either monthly or hourly billing for WorkSpaces.
  • Deploy and manage applications for your Windows WorkSpaces by using Amazon WorkSpaces Application Manager (Amazon WAM).
  • For Windows desktops, you can bring your own licenses and applications, or purchase them from the AWS Marketplace for Desktop Apps.
  • Create a standalone managed directory for your users, or connect your WorkSpaces to your on-premises directory so that your users can use their existing credentials to obtain seamless access to corporate resources.
  • From any Operating system , use any other operating system
  • Encrypt data using AWS KMS , Use MFA , Select the range of IP addresses to use.

Each WorkSpace is associated with a virtual privatecloud (VPC), and a directory to store and manage information for your WorkSpaces and users.
Directories are managed through the AWS Directory Service, which offers the following options: Simple AD, AD Connector, or AWS Directory Service for Microsoft Active Directory, also known as AWS Managed Microsoft AD.
The login information is sent to an authentication gateway, which forwards the traffic to the directory for the WorkSpace. After the user is authenticated, streaming traffic is initiated through the streaming gateway.
Client applications use HTTPS over port 443 for all authentication and session-related information.
Client applications use port 4172 (PCoIP) and port 4195 (WSP) for pixel streaming to the WorkSpace and ports 4172 and 4195 for network health checks.
Each WorkSpace has two elastic network interfaces associated with it: a network interface for management and streaming (eth0) and a primary network interface (eth1).
The primary network interface has an IP address provided by your VPC, from the same subnets used by the directory.
This ensures that traffic from your WorkSpace can easily reach the directory. Access to resources in the VPC is controlled by the security groups assigned to the primary network interface.

Example Computer names • Linux: A-1xxxxxxxxxxxx
                       • Windows: IP-Cxxxxxx or WSAMZN-xxxxxxx or EC2AMAZ-xxxxxxx


2. AMAZON WORKSPACES APPLICATION MANAGER -WAM :
Amazon WorkSpaces Application Manager (Amazon WAM) offers a fast, flexible, and secure way for you to deploy and manage applications for Amazon WorkSpaces  with Windows.
Amazon WAM accelerates software deployment, updates, patching, and retirement by packaging Microsoft Windows desktop applications into virtual containers that run as though they are installed natively.
Amazon WAM is fully integrated with the AWS Management Console, and allows you to build an application catalog from your line-of-business applications, third-party applications that you own the license for, and applications purchased through the AWS Marketplace.

Amazon WAM is available in two versions: Amazon WAM Lite and Amazon WAM Standard.
With Amazon WAM Lite, you can manage and deliver applications from the AWS Marketplace free of charge. You pay only for the applications that your users activate.
With Amazon WAM Standard, you can build your application catalog with line-of-business applications, third-party applications for which you own licenses, and applications from the AWS Marketplace for Desktop Apps.
With Amazon WAM, you pay only for the applications that are used, on a per-user basis.


3. AMAZON APP-STREAM 2.0 :
--> Amazon AppStream 2.0 is a fully managed application streaming service that provides users with instant access to their desktop applications from anywhere.
Amazon AppStream 2.0 is a fully managed, secure application streaming service that lets you stream desktop applications to users without re writing applications.
AppStream 2.0 provides users with instant access to the applications that they need with a responsive, fluid user experience on the device of their choice.
AppStream 2.0 provides users access to the applications they need on the device of their choice, with a responsive, fluid user experience that is indistinguishable from natively installed applications.

With AppStream 2.0, you can easily add your existing desktop applications to AWS and enable your users to instantly stream them.
You can maintain a single version of each of your applications, which makes application management easier. Your users always access the latest versions of their applications.
Your applications run on AWS compute resources, and data is never stored on users' devices, which means they always get a high performance, secure experience.

Choose the fleet type that meets your needs. There are two types of fleets:
    • Always-On — Your instances run all the time, even when no users are streaming applications. Use an Always-On fleet to provide your users with instant access to their applications.
    • On-Demand — Your instances run only when users are streaming applications. Idle instances that are available for streaming are in a stopped state. Use an On-Demand fleet to optimize your streaming charges and provide your users with access to their applications after a 1-2 minute wait.

IMAGE BUILDER : It is a virtual machine that you use to create an image. You can launch and connect to an image builder by using the AppStream 2.0 console.
                After you connect to an image builder, you can install, add, and test your applications, and then use the image builder to create an image.
IMAGE         : An image contains applications that you can stream to your users, and default Windows and application settings to enable your users to get started with their applications quickly.
                To add other applications, update existing applications, or change image settings, you must create a new image.
FLEET         : A fleet consists of fleet instances (also known as streaming instances) that run the image that you specify.
                You can set the desired number of streaming instances for your fleet and configure policies to scale your fleet automatically based on demand.
STACK         : A stack consists of an associated fleet, user access policies, and storage configurations.
                You set up a stack to start streaming applications to users.
STREAMING INSTANCE : A streaming instance (also known as a fleet instance) is an EC2 instance that is made available to a single user for application streaming.
                After the user’s session completes, the instance is terminated by EC2.


4. AMAZON WORK-DOCS : https://www.youtube.com/watch?v=DGQGYgiYdnA
--> STORE - SYNC - SHARE FILES from anywhere to anywhere.
--> Directly edit docs online.
Amazon WorkDocs is a fully managed, secure enterprise storage and sharing service with strong administrative controls and feedback capabilities that improve user productivity.
Files are stored in the cloud, safely and securely. Your user's files are only visible to them, and their designated contributors and viewers.
Other members of your organization do not have access to other user's files unless they are specifically granted access.

Users can share their files with other members of your organization for collaboration or review.
The Amazon WorkDocs client applications can be used to view many different types of files, depending on the Internet media type of the file.
Amazon WorkDocs supports all common document and image formats, and support for additional media types is constantly being added.

Administrators use the Amazon WorkDocs console to create and deactivate Amazon WorkDocs sites. With the admin control panel, they can manage users, storage, and security settings.
Non-administrative users use the client applications to access their files. They never use the AmazonWorkDocs console or the administration dashboard.
Amazon WorkDocs offers several different client applications and utilities:
  • A web application used for document management and reviewing.
  • Native apps for mobile devices used for document review.
  • A document synchronization app used to synchronize a folder on your macOS or Windows desktop with your Amazon WorkDocs files.
  • Web clipper browser extensions for several popular web browsers that allow you to save an image of awebpage to your Amazon WorkDocs files.
You must have an AWS account to create or administer an Amazon WorkDocs site. Users do not needan AWS account to connect to and use Amazon WorkDocs.


5. AMAZON WORK-LINK : Similar to UX-Apps TCS // https://www.youtube.com/watch?v=Si2q7rhinMo
--> Give secure access to internal websites and web apps through mobile phones , since using a VPN is not a feasiblee solution .
--> we need to install the work-link application .

Amazon WorkLink is a fully managed, cloud-based service that enables secure, one-click access to internal websites and web apps from mobile devices.
In a single step, your users, such as employees, can access internal websites as efficiently as they access any other public website.
They enter a URL in their web browser, or choose a link to an internal website in an email.
Because website data is never stored or cached locally on mobile browsers, Amazon WorkLink reduces the risk of information loss or theft.
In addition, all cached content is deleted from AWS when users end their browsing session.
To use Amazon WorkLink, your users download the Amazon WorkLink app to their mobile device and login with their company credentials.
After initial setup, the Amazon WorkLink app works in the background while employees browse internal websites using Safari on iOS phones and Google Chrome on Android phones.




##################################################################################################################
##################################################################################################################
APPLICATION INTEGRATION  :
##################################################################################################################
##################################################################################################################

1. AMAZON APP-FLOW : // Explained in ANALYTICS https://www.youtube.com/watch?v=USzaWjjjOJI&t=4s
// We may have SAAS apps such as sales force - slack etc; . Sharing data b/w them and AWS for analytics and archiving and automatic workflow isimportant.
// But managing the transfer is difficult which is handled by APP-FLOW.
Bidirectional data transfer from SAAS Applications To AWS
Amazon App Flow is a fully-managed integration service that enables you to securely exchange data between software as a service (SaaS) applications, such as Salesforce, Slack and AWS services, such as Amazon Simple Storage Service (Amazon S3) and Amazon Redshift.
For example, you can ingest contact records from Salesforce to Amazon Redshift or pull support tickets from Zendesk to an Amazon S3 bucket.

2. AMAZON EVENT-BRIDGE : // Formerly CloudWatch Events.
// Bridge b/w your apps - SAAS - AWS apps
Amazon EventBridge is a serverless event bus service that makes it easy to connect your applications with data from a variety of sources.
EventBridge delivers a stream of real-time data from your own applications, software-as-a-service (SaaS) applications, and AWS services and routes that data to targets such as AWS Lambda.
You can set up routing rules to determine where to send your data to build application architectures that react in real time to all of your data sources.
EventBridge enables you to build event-driven architectures that are loosely coupled and distributed.

  • Events – An event indicates a change in an environment. This can be an AWS environment, an SaaS partner service or application, or one of your own custom applications or services.
  • Rules – A rule matches incoming events and routes them to targets for processing. A single rule can route to multiple targets, all of which are processed in parallel.
  • Targets – A target processes events. Targets can include Amazon EC2 instances, Lambda functions, Kinesis streams, Amazon ECS tasks, Step Functions state machines, Amazon SNS topics, Amazon SQSqueues, and built-in targets. A target receives events in JSON format.
  • Event buses – An event bus receives events. When you create a rule, you associate it with a specific event bus, and the rule is matched only to events received by that event bus.
                  You can create custom event buses to receive events from your custom applications.
                  You can also create partner event buses to receive events from SaaS partner applications.
                  Partner event buses, which can receive events from applications and services created by AWS software as a service (SaaS) partners
  • Partner event sources – A partner event source is used by an AWS partner to send events to an AWS customer account.

3. AMAZON MANAGED MESSAGE BROKER - MQ : // Documented in Chapter SNS-SQS   https://www.youtube.com/watch?v=iDT1zFpy1kE&t=23s
Amazon MQ is a managed message broker service that makes it easy to set up and operate message brokers in the cloud.
Amazon MQ provides interoperability with your existing applications and services.
Amazon MQ works with your existing applications and services without the need to manage, operate, or maintain your own messaging system.

4. AMAZON SNS : < Has a separate detailed chapter > // Also documented in Front-End Web & Mobile.

5. AMAZON SQS : < Has a separate detailed chapter >
It is a fully managed message queuing service that makes it easy to decouple and scale microservices, distributed systems, and serverless applications.
Amazon SQS moves data between distributed application components and helps you decouple these components.
It is a fully managed message queuing service that enables you to send, store, and receive messages between software components at any volume, without losing messages or requiring other services to be available.
SQS lets you decouple application components so that they run independently, increasing the overall fault tolerance of the system.
Multiple copies of every message are stored redundantly across multiple availability zones so that they are available whenever needed.


6. AWS STEP FUNCTIONS :
AWS Step Functions makes it easy to coordinate the components of distributed applications as a series of steps in a visual workflow.
You can quickly build and run state machines to execute the steps of your application in a reliable and scalable fashion.

Step Functions is a serverless orchestration service that lets you combine AWS Lambda functions and other AWS services to build business-critical applications.
Through Step Functions' graphical console, you see your application’s workflow as a series of event-driven steps.
Step Functions is based on state machines and tasks. A state machine is a workflow. A task is a state in a workflow that represents a single unit of work that another AWS service performs. Each step in aworkflow is a state.
With Step Functions' built-in controls, you examine the state of each step in your workflow to make sure that your application runs in order and as expected.

Depending on your use case, you can have Step Functions call AWS services, such as Lambda, to perform tasks.
You can create workflows that process and publish machine learning models.
You can have Step Functions control AWS services, such as AWS Glue, to create extract, transform, and load (ETL) workflows.
You also can create long-running, automated workflows for applications that require human interaction.

We have 6 use-cases : See ref pics ,
  1. Function orchestration :
     You create a workflow that runs a group of Lambda functions (steps) in a specific order.
     One Lambda function's output passes to the next Lambda function's input. The last step in your workflow gives a result.
     With Step Functions, you can see how each step in your workflow interacts with one other, so you can make sure that each step performs its intended function.

  2. Branching :
     A customer requests a credit limit increase. Using a Choice state, you can have Step Functions make decisions based on the Choice state’s input.
     If the request is more than your customer’s pre-approved credit limit, you can have Step Functions send your customer's request to a manager for sign-off.
     If the request is less than your customer’s pre-approved credit limit, you can have Step Functions approve the request automatically.

  3. Error Handling :
     RETRY : In this use case, a customer requests a username. The first time, your customer’s request is unsuccessful. Using a Retry statement, you can have Step Functions try your customer's request again.
     CATCH : In a similar use case, a customer requests an unavailable username. Using a Catch statement, you have Step Functions suggest an available username. If your customer takes the available username, you canhave Step Functions go to the next step in your workflow.

  4. Human In the loop :
     Using a banking app, one of your customers sends money to a friend. Your customer waits for a confirmation email.
     With a call back and a task token, you have Step Functions tell Lambda to send your customer’s money and report back when your customer’s friend receives it.
     After Lambda reports back that your customer’s friend received the money, you can have Step Functions go to the next step in your workflow, which is to send your customer a confirmation email.

  5. Parallel Processing :
    A customer converts a video file into five different display resolutions, so viewers can watch the video on multiple devices.
    Using a Parallel state, Step Functions inputs the video file, so Lambda can process itinto the five display resolutions at the same time.

  6. Dynamic Parallelism :
     A customer orders three items, and you need to prepare each item for delivery.
     You check each item's availability, gather each item, and then package each item for delivery.
     Using a Map state, Step Functions has Lambda process each of your customer's items in parallel.
     Once all of your customer's items are packaged for delivery, Step Functions goes to the next step in your workflow, which is to send your customer a confirmation email with tracking information.


7. AMAZON SIMPLE WORK FLOW - SWF :
Amazon Simple Workflow Service (Amazon SWF) makes it easy to build applications that coordinate work across distributed components.
In Amazon SWF, a task represents a logical unit of work that is performed by a component of your application.
Coordinating tasks across the application involves managing inter task dependencies, scheduling, and concurrency in accordance with the logical flow of the application.
Amazon SWF gives you full control over implementing tasks and coordinating them without worrying about underlying complexities such as tracking their progress and maintaining their state.

When using Amazon SWF, you implement workers to perform tasks.
These workers can run either on cloud infrastructure, such as Amazon Elastic Compute Cloud (Amazon EC2), or on your own premises.
You can create tasks that are long-running, or that may fail, time out, or require restarts—or that may complete with varying throughput and latency.
Amazon SWF stores tasks and assigns them to workers when they are ready, tracks their progress, and maintains their state, including details on their completion.
To coordinate tasks, you write a program that gets the latest state of each task from Amazon SWF and uses it to initiate subsequent tasks.
Amazon SWF maintains an application's execution state durably so that the application is resilient to failures in individual components.
With Amazon SWF, you can implement, deploy, scale, and modify these application components independently.




##################################################################################################################
##################################################################################################################
BUISENES APPLICATION  :
##################################################################################################################
##################################################################################################################

1. ALEXA FOR BUSINESS :
// Similar to your Alexa in house , you can set up 100's of alexa in an organization and sync data between users.
// schedule  meetings , check available rooms for booking , join meetings , get sales reports for your specific company .
Alexa for Business makes it easy for you to use Alexa in your organization.
Alexa for Business gives you the tools you need to manage Alexa devices, enroll your users, and assign skills, at scale.
You can build your own context-aware voice skills using the Alexa Skills Kit, and the Alexa for Business APIs, and you can make these available as private skills for your organization.
Alexa for Business also makes it easy to voice-enable your products and services, providing context-aware voice experiences for your customers.

2. AMAZON CHIME :
Amazon Chime is a secure, real-time, unified communications service that transforms meetings by making them more efficient and easier to conduct.
Amazon Chime is a communications service that transforms online meetings with an application that is secure and comprehensive. Amazon Chime works across your devices so that you can stay connected.
You can use Amazon Chime for online meetings, video conferencing, calls, and chat. You can also share content inside and outside of your organization.
Amazon Chime provides usage-based pricing. You pay only for the users with Pro permissions that host meetings, and only on the days that those meetings are hosted. Meeting attendees and chat users are not charged.

The easiest way for your users to get started with Amazon Chime is to download and use the AmazonChime Pro version for free for 30 days.
To continue using the Amazon Chime Pro version after the 30-day free trial period, you must create an Amazon Chime administrator account and add your users to it.
The invited users receive email invitations to join the Amazon Chime Team account that you created.
When they register their Amazon Chime user accounts, they receive Pro permissions by default, and their 30-day trial ends.
If they have already signed up for an Amazon Chime user account with their work email address, they can continue to use that account.
They can also download the Amazon Chime clientapp at any time by choosing Download Amazon Chime and signing in to their user account.
You are only charged for a user with Pro permissions when they host a meeting.
There is no charge forusers with Basic permissions. Basic users cannot host meetings, but they can attend meetings and usechat.

3. AMAZON HONEY-CODE : https://www.youtube.com/watch?v=vMR2KOJUEr8
// Build a custom managed app using a visual editor .
// First setup data in tables , tables work like spread sheets but have database capabilities.
// Next build the apps layout . Now link app and the tables, set personalization for specific users to see what they can see or not see.
// Set specific actions to be done on an event such as when data is updated .
// EX : A sale can be setup and the customer accepts it on his console and you get the response. once done you may delete the application.
// You are redirected to a HONEYCODE application apart from AWS.

Amazon Honeycode is a fully managed service that allows you to quickly build mobile and web apps for teams—without programming. Build Honeycode apps for managing almost anything, like projects, customers, operations, approvals, resources, and even your team.
When API activity occurs in Amazon Honeycode apps, the activity is recorded in a CloudTrail event.
You can view, search, and download recent events in your AWS account.
For an ongoing record of events in Amazon Honeycode, as well as your other AWS accounts, you cancreate a trail.
A trail enables CloudTrail to continuously deliver events as log files to an Amazon S3bucket.

4. AMAZON WORK-MAIL : https://www.youtube.com/watch?v=3Wq-ghtB0XQ
// Maintaining personal email service is tough !! -> Buy dedicated resources [ h/w and s/w ] -> hire staff to run the resources -> regular maintenance -> upgrades, security patches, backups , disaster recovery .
// 3rd party apps have risk of encryption , mobile compatibility , latency etc ;

Amazon WorkMail is a managed email and calendaring service that offers strong security controls and support for existing desktop and mobile clients.
Can integrate with corporate active directory so existing users and groups can use it in a similar way.
Can access mail and calendar from multiple devices mobile - tablet - desktop - all OS - windows - IOS and has compatibility with OUTLOOK and other popular services .
When your Amazon WorkMail administrator invites you to sign in to your Amazon WorkMail account, you can sign in using the Amazon WorkMail web client.

Amazon WorkMail has a web-based client that you use to access your Amazon WorkMail account from a web browser.
The Amazon WorkMail web client includes integrated applications, such as Mail, Calendar, and Contacts.
Your Amazon WorkMail system administrator provides you with your initial sign-in credentials, which consist of a user name and a password.
Your administrator also provides you with a unique Amazon WorkMail web client URL. This URL contains a unique alias set up by your Amazon WorkMail site administrator.
The web client URL looks like this:https://alias.awsapps.com/mail.


#################################################################################################################
##################################################################################################################
CUSTOMER ENABLEMENT SERVICES  :
##################################################################################################################
##################################################################################################################

1. AWS MANAGED SERVICES - MSP : https://www.youtube.com/watch?v=9yHuq3ABJaI&t=8s
// 3Rd party partners providing full life cycle solutions .
AWS Managed Services is an enterprise service that provides ongoing management of your AWS infrastructure operations.
AMS provides full-lifecycle services to provision, run, and support your infrastructure, and automates common activities such as change requests, monitoring, patch management, security, and backup services.
AMS is an AWS service that operates AWS on behalf of enterprise customers and partners.
Enterprises want to adopt AWS at scale but often the skills that have served them well in traditional IT do not always translate to success in the cloud.
AWS Managed Services (AMS) enables them to migrate to AWS at scale more quickly, reduce their operating costs, improve security and compliance and focus on their differentiating business priorities.

NOTE :
------------
This Concept is completely different from the AWS MANAGED SERVICES which include AWS Lambda, Amazon RDS, Amazon Redshift, Amazon CloudFront, Elastic Map Reduce -EMR , Dynamo DB
AWS is responsible for performing all the operations needed to keep the above service running.

2. AWS PROFESSIONAL SERVICES :
The AWS Professional Services organization is a global team of experts that can help you realize your desired business outcomes when using the AWS Cloud. We work together with your team and your chosen member of the AWS Partner Network (APN) to execute your enterprise cloud computing initiatives.
The team provides assistance through a collection of offerings which help you achieve specific outcomes related to enterprise cloud adoption.
They also deliver focused guidance through our global specialty practices, which cover a variety of solutions, technologies, and industries.

3. AWS SUPPORT :  < Has a separate detailed chapter >
AWS Support provides a mix of tools and technology, people, and programs designed to proactively help you optimize performance, lower costs, and innovate faster.
They save time for your team by helping you to move faster in the cloud and focus on your core business.
They are determined to make our customers successful on their cloud journey and address requests that range from answering best practices questions, guidance on configuration, all the way to break-fix and problem resolution.
On behalf of our customers, we are focused on solving some of the toughest challenges that hold you back in your cloud journey.
Sometimes that means helping you troubleshoot an issue, but more often, it involves "looking around corners" to find ways for you to better utilize AWS services, answer best practices questions, and provide guidance on configuration.
We focus on helping you achieve the outcomes you need to make your business successful. It is our approach to Support that sets AWS apart.

There are multiple SUPPORT PLANS available. All AWS customers automatically have 24/7 access to these features of the Basic support plan:
https://console.aws.amazon.com/support/plans/home?#/ **V.IMP
  --> Basic                 - Included
  --> Current plan          - Starting at $29 per month
  --> Developer	Business    - Starting at $100 per month
  --> Enterprise            - Starting at $15,000 per month

4. AWS TRAINING & CERTIFICATION :
We have a separate directory containing Dumps and PDF's required for the certifications .
  TRAINING : https://aws.amazon.com/training/?id=docs_gateway
  DOCS     : https://docs.aws.amazon.com/

5. AWS IQ :
// Submit request -> review responses -> select expert -> work securely -> Pay from AWS billing.
AWS IQ enables customers to quickly find, engage, and pay AWS Certified third-party experts for on-demand project work.
AWS IQ enables AWS Certified experts to help customers and get paid for their AWS Certification expertise.
You create a request and choose from experts who respond. Before you agree to any payments, compare and chat with experts about your project.
Once you agree to an expert's proposal including project milestones, maximum payment, and terms, the expert can get started.
If necessary, you grant project appropriate permission to your AWS account for the expert to perform the work.
After project milestones are completed, you pay the expert directly from your AWS account.

AWS IQ is a good choice when you're stuck on one or more tasks and need someone to do thework for you.
AWS IQ is also a good choice for projects that are outside your area of expertise.
With AWSIQ, you get help from an expert to move your project forward.


##################################################################################################################
##################################################################################################################
CUSTOMER ENGAGEMENT  :
##################################################################################################################
##################################################################################################################

1. AMAZON CONNECT : https://www.youtube.com/watch?v=syF1CEiWAto
// Set up a Customer care centre.

Amazon Connect is an omnichannel cloud contact center.
You can set up a contact center in afew steps, add agents who are located anywhere, and start engaging with your customers.
Amazon Connect is a contact center as a service (CCaS) solution that offers easy, self-service configuration and enables dynamic, personal, and natural customer engagement at any scale.
Voice and Chat channels and support tickets tracking .
All support channel data is logged in S3.
It uses Machine Learning (ML) and Artificial Intelligence (AI) for analytics .

2. AMAZON PINPOINT :
// Documented in Front End Web & Mobile
Amazon Pinpoint helps you engage your customers by sending them email, SMS and voice messages, and push notifications.
You can use Amazon Pinpoint to send targeted messages (such as promotions and retention campaigns), as well as transactional messages (such as order confirmations and password reset messages).
you have to add customer contact information into Amazon Pinpoint, and then create segments that target certain customers.
Next, you have to create your messages and schedule your campaigns.
Finally, after you send your campaigns, you can use the analytics dashboards that are built into Amazon Pinpoint to see how well the campaigns performed.

3. AMAZON SIMPLE EMAIL SERVICE - SES :  < Has a separate detailed chapter >
Amazon Simple Email Service (Amazon SES) is an email sending and receiving service that provides an easy, cost-effective way for you to send email.
  • Add email-sending capabilities to any application. If your application runs in Amazon Elastic ComputeCloud (Amazon EC2), you can use Amazon SES to send 62,000 emails every month at no additional charge.
    You can send email from Amazon EC2 by using an AWS SDK, by using the Amazon SES SMTP interface, or by making calls directly to the Amazon SES API.
  • Use AWS Elastic Beanstalk to create an email-enabled application such as a program that uses Amazon SES to send a newsletter to customers.
  • Set up Amazon Simple Notification Service (Amazon SNS) to notify you of your emails that bounced, produced a complaint, or were successfully delivered to the recipient's mail server.
    When you use Amazon SES to receive emails, your email content can be published to Amazon SNS topics.
  • Use the AWS Management Console to set up Easy DKIM, which is a way to authenticate your emails.
    Although you can use Easy DKIM with any DNS provider, it is especially easy to set up when you manage your domain with Route 53.

##################################################################################################################
##################################################################################################################
BILLING & COST MANAGEMENT   :
##################################################################################################################
##################################################################################################################

1. AWS BOLLING & COST MANAGEMENT : < Has a separate detailed chapter >
AWS Billing and Cost Management is the service that you use to pay your AWS bill, monitor your usage, and analyze and control your costs.
AWS automatically charges the credit card that you provided when you signed up for a new account with AWS. Charges appear on your monthly credit card bill.
The Billing and Cost Management service provides features that you can use to do the following:
  •Estimate and plan your AWS costs
  •Receive alerts if your costs exceed a threshold that you set
  •Assess your biggest investments in AWS resources
  •Simplify your accounting if you work with multiple AWS accounts

AWS COST & USAGE REPORT :
---------------------------
The AWS Cost & Usage Report is your one-stop shop for accessing the most detailed information available about your AWS costs and usage.
The AWS Cost & Usage Report lists AWS usage for each service category used by an account and its IAM users in hourly or daily line items, as well as any tags that you have activated for cost allocation purposes.
AWS Budgets gives you the ability to set custom budgets that alert you when your costs or usage exceed (or are forecasted to exceed) your budgeted amount.

AWS CONSOLIDATED BILLING :
----------------------------
AWS consolidated billing enables an organization to consolidate payments for multiple Amazon Web Services (AWS) accounts within a single organization by making a single paying account.
For billing purposes, AWS treats all the accounts on the consolidated bill as one account.
For services such as Amazon EC2 that support a free tier, AWS applies the free tier to the total usage across all accounts in an AWS organization. AWS doesn't apply the free tier to each account individually.
Some services, such as Amazon EC2 and Amazon S3 have volume pricing tiers across certain usage dimensions that give the user lower prices when they use the service more.

    For example, let's say that Bob's consolidated bill includes both Bob's own account and Susan's account.
    Bob's account is the management account, so he pays the charges for both himself and Susan.
    Say Bob transfers 8 TB of data during the month and Susan transfers 4 TB.

    For the purposes of this example, AWS charges $0.17 per GB for the first 10 TB of data transferred and $0.13 for the next 40 TB.
    This translates into $174.08 per TB (= .17*1024) for the first 10 TB, and $133.12 per TB (= .13*1024) for the next 40 TB.
    Remember that 1 TB = 1024 GB.

    For the 12 TB that Bob and Susan used, Bob's management account is charged ($174.08 * 10 TB) + ($133.12 * 2 TB) = $1740.80 + $266.24 = $2,007.04.

    If they have been billed individually :
    For the 8Tb that BOB used , ($174.08 * 8 Tb) + For the 4Tb Susan used , ($174.08 * 4 Tb) = $1392.64 + $696.32 = $2088.96
    So they have combinedly saved  $81 using consolidated billing .

    For billing purposes, the consolidated billing feature of AWS Organizations treats all the accounts in the organization as one account.
    This means that all accounts in the organization can receive the hourly cost benefit of Reserved Instances that are purchased by any other account.
    For an Amazon EC2 Reserved Instances example, suppose that Bob and Susan each have an account in an organization.
    Susan has five Reserved Instances of the same type, and Bob has none.
    During one particular hour, Susan uses three instances and Bob uses six, for a total of nine instances on the organization's consolidated bill.
    AWS bills five instances as Reserved Instances, and the remaining four instances as regular instances.

    Bob receives the cost benefit from Susan's Reserved Instances only if he launches his instances in the same Availability Zone where Susan purchased her Reserved Instances.
    For example, if Susan specifies us-west-2a when she purchases her Reserved Instances, Bob must specify us-west-2a when he launches his instances to get the cost benefit on the organization's consolidated bill.
    However, the actual locations of Availability Zones are independent from one account to another.
    For example, the us-west-2a Availability Zone for Bob's account might be in a different location than the location for Susan's account.

    NOTE : The management account of an organization can turn off Reserved Instance (RI) discount and Savings Plans discount sharing for any accounts in that organization, including the management account.
    This means that RIs and Savings Plans discounts aren't shared between any accounts that have sharing turned off.
    To share an RI or Savings Plans discount with an account, both accounts must have sharing turned on.

AWS COST EXPLORER :
--------------------
AWS COST EXPLORER  is a free tool that you can use to view your costs and usage.
You can view data up to the last 13 months, forecast how much you are likely to spend for the next three months, and get recommendations for what Reserved Instances to purchase.
You can use AWS Cost Explorer to see patterns in how much you spend on AWS resources over time, identify areas that need further inquiry, and see trends that you can use to understand your costs.
You can also specify time ranges for the data, and view time data by day or by month.


2. AWS PRICING CALCULATOR : < Has a separate detailed chapter >
AWS Pricing Calculator lets you explore AWS services and create an estimate for the cost of your use cases on AWS.
You can model your solutions before building them, explore the price points and calculations behind your estimate, and find the available instance types and contract terms that meet your needs.
This enables you to make informed decisions about using AWS.
You can plan your AWS costs and usage or price out setting up a new set of instances and services.

3. SAVINGS PLAN : < Has a separate detailed chapter >
Savings Plans is a flexible pricing model that helps you save up to 72 percent on Amazon EC2, AWS Fargate, and AWS Lambda usage.
Savings Plans provides you lower prices for your Amazon EC2 usage, Fargate, and Lambda in exchange for a commitment to a consistent usage amount (measured in $/hour)for a one or three year term.
Every type of compute usage has an On-Demand rate and a Savings Plans rate.
For example, if you commit to $10/hour of compute usage, your usage is charged at your Savings Plans rate up to $10.
Any usage beyond your Savings Plans commitment is charged at your regular On-Demand rates.

##################################################################################################################
##################################################################################################################
SATELLITE -  ROBOTICS -  QUANTUM COMPUTING -  BLOCKCHAIN -  AR & VR -  GAME DEVELOPMENT
##################################################################################################################
##################################################################################################################

1. AWS GROUND STATION : // Satellite
AWS Ground Station is a fully managed service that enables you to control satellite communications, process satellite data, and scale your satellite operations.
With AWS Ground Station, you don't have to build or manage your own ground station infrastructure.

2. AWS ROBO MAKER : // Robotics
AWS RoboMaker is a service that makes it easy to develop, simulate, and deploy intelligent robotics applications at scale.
AWS RoboMaker extends the Robot Operating System (ROS) framework with cloud services. This includes AWS machine learning services.
It includes monitoring services. It even includes analytics services. These combine to enable a robot to do several things on its own.
Stream data, navigate, communicate, comprehend, and learn. AWS RoboMaker provides a robotics application development environment.
It provides a robotics simulation service, which speeds application testing.
You can easily create hundreds of new worlds from templates you define using Simulation WorldForge.
It provides a fleet management service so you can deploy and manage applications remotely

3. AMAZON BRAKET : // Quantum
Quantum computers perform calculations based on the probability of an object's state before it is measured - instead of just 1s or 0s - which means they have the potential to process exponentially more data compared to classical computers.
Amazon Braket is a fully managed service that helps you get started with quantum computing by providing a development environment.
THE environment helps to explore and design quantum algorithms, test them on simulated quantum computers, and run them on your choice of different quantum hardware technologies.

4. AMAZON MANAGED BLOCK CHAIN : // Block chain
In the simplest terms, Blockchain can be described as a data structure that holds transactional records and while ensuring security, transparency, and decentralization.
You can also think of it as a chain or records stored in the forms of blocks which are controlled by no single authority.
Each block has a cryptographic hash of the previous block, a timestamp, and transaction data.
A blockchain is a decentralized ledger of all transactions across a peer-to-peer network. Using this technology, participants can confirm transactions without a need for a central clearing authority.
Amazon Managed Blockchain is a fully managed service that makes it easy to create and manage scalable blockchain networks using the Ethereum or Hyperledger Fabric open-source frameworks.

5. AMAZON SUMERIAN : // AR - VR
Amazon Sumerian is a set of tools for creating high-quality virtual reality (VR) experiences on the web.
With Sumerian, you can construct an interactive 3D scene without any programming experience, test it in the browser, and publish it as a website that is immediately available to users.
The Sumerian 3D engine provides a library for advanced scripting with JavaScript but you don't have to be a programmer to create interactive AR, VR, or 3D!
 the built-in state machine to animate objects and respond to user input like clicks and movement.

6. AMAZON GAME-LIFT : // Game
GameLift provides solutions for hosting session-based multiplayer game servers in the cloud, including a fully managed service for deploying, operating, and scaling game servers.
Built on AWS global computing infrastructure, GameLift helps you deliver high-performance, high-reliability, low-cost game servers while dynamically scaling your resource usage to meet player demand.

7. AMAZON LUMBER-YARD : // Game
Lumberyard is a free AAA game engine deeply integrated with AWS and Twitch—with full source.
Lumberyard provides a growing set of tools to help you create the highest quality games, engage massive communities of fans, and connect games to the vast compute and storage of the cloud.


##################################################################################################################
ADDITIONAL RESOURCES  :
##################################################################################################################
##################################################################################################################

1. AWS MARKET-PLACE :  https://www.youtube.com/watch?v=RxCRHsAqiWs
---------------------------------------------------------------------
// Buy & Sell - Paid or Free AWS Software Solutions .
AWS Marketplace is an online store where you can buy or sell software that runs on Amazon Web Services (AWS).
AWS Marketplace is a curated digital catalog customers can use to find, buy, deploy, and manage third-party software, data, and services that customers need to build solutions and run their businesses.
AWS Marketplace includes thousands of software listings from popular categories such as security, networking, storage, machine learning, business intelligence, database, as well as related professional services to help you manage and support those solutions.
AWS Marketplace also simplifies software licensing and procurement with flexible pricing options and multiple deployment methods.
Customers can quickly launch pre-configured software with just a few clicks, and choose software solutions in AMI and SaaS formats, as well as other formats.
Flexible pricing options include free trial, hourly, monthly, annual, multi-year, and BYOL, and get billed from one source, AWS.

Customers can quickly launch preconfigured software with just a few clicks, and choose software solutions in Amazon Machine Images (AMIs), software as a service (SaaS),
Flexible pricing options include free trial, hourly, monthly, annual, multi-year, and BYOL, and get billed from one source.
The product can be free to use or it can have an associated charge. AWS handles billing and payments, and charges appear on customers’ AWS bill.

SELLER :  seller can be an independent software vendor (ISV), consulting partner, managed services provider (MSP), or individual who has something to offer that works with AWS products and services.

One way of delivering your products to buyers is with Amazon Machine Images (AMIs) - Docker Images - SAAS Apps.
An AMI provides the information required to launch an Amazon Elastic Compute Cloud (Amazon EC2) instance.
You create a custom AMI for your product, and buyers can use it to create EC2 instances with your product already installed and ready to use.


2. AWS GOVCLOUD - United States :
----------------------------------
The AWS GovCloud (US) Regions are isolated AWS Regions designed to enable U.S. government agencies and customers to move sensitive workloads into the cloud by addressing their specific regulatory and compliance requirements.
The AWS GovCloud (US) Regions adhere to U.S. International Traffic in Arms Regulations (ITAR) requirements.

3. AWS MANAGED SERVICES & CUSTOMER MANAGED SERVICES:
--------------------------------------------------------

AWS MANAGED :
-----------------
AWS Lambda, Amazon RDS, Amazon Redshift, Amazon CloudFront, Elastic Map Reduce -EMR , Dynamo DB
For managed services such as Amazon Elastic MapReduce (Amazon EMR) and DynamoDB etc ;
AWS is responsible for performing all the operations needed to keep the service running.
The AWS-managed services automate time-consuming administration tasks such as hardware provisioning, software setup, patching and backups. The AWS-managed services free customers to focus on their applications so they can give them the fast performance, high availability, security and compatibility they need.
Examples of AWS-managed services include Amazon RDS, Amazon DynamoDB, Amazon Redshift, Amazon WorkSpaces, Amazon CloudFront, Amazon CloudSearch, and several other services.
AWS is responsible for patching the underlying hosts, upgrading the firmware, and fixing flaws within the infrastructure for all services, including Amazon EC2.

Amazon EMR launches clusters in minutes.
You don’t need to worry about node provisioning, infrastructure setup, Hadoop configuration, or cluster tuning.
Amazon EMR takes care of these tasks so you can focus on analysis.
DynamoDB is serverless with no servers to provision, patch, or manage and no software to install, maintain, or operate.
DynamoDB automatically scales tables up and down to adjust for capacity and maintain performance.
Availability and fault tolerance are built in, eliminating the need to architect your applications for these capabilities.

CUSTOMER MANAGED :
------------------------
On the other hand, customer-managed services are services that are completely managed by the customer.
For example, a service such as Amazon Elastic Compute Cloud (Amazon EC2), Amazon VPC, and Amazon S3 are categorized as Infrastructure as a Service (IaaS) and, as such, requires the customer to perform all of the necessary security configuration and management tasks.
Customers that deploy an Amazon EC2 instance are responsible for the management of the guest operating system (including updates and security patches), any application software or utilities installed by the customer on the instances, and the configuration of the AWS-provided firewall (called a security group) on each instance.
The customer is responsible for securing their network by configuring Security Groups, Network Access control Lists (NACLs), and Routing Tables. The customer is also responsible for setting a password policy on their AWS account that specifies the complexity and mandatory rotation periods for their IAM users' passwords.
Examples of customer-managed services include Amazon Elastic Compute Cloud (Amazon EC2), Amazon Virtual Private Cloud (Amazon VPC), and AWS Identity And Access Management (AWS IAM).
Patching the guest operating system is the responsibility of AWS for the managed services only (such as Amazon RDS). The customer is responsible for patching the guest OS for other services (such as Amazon EC2).
Data encryption is the responsibility of the customer. Both SERVER SIDE & CLIENT SIDE encryption comes under client responsibility.
  NOTE :
  The AWS managed services we mentioned above are different than the AWS Managed Services (AMS) service.
  AMS is an AWS service that operates AWS on behalf of enterprise customers and partners.
  Enterprises want to adopt AWS at scale but often the skills that have served them well in traditional IT do not always translate to success in the cloud.
  AWS Managed Services (AMS) enables them to migrate to AWS at scale more quickly, reduce their operating costs, improve security and compliance and focus on their differentiating business priorities.

4. AWS SHARED CONTROLS - AWS & CUSTOMER:
--------------------------------------------------
Responsibilities depends on the service used .
For example, when using Amazon EC2, you are responsible for applying operating system and application security patches regularly. However, such patches are applied automatically when using Amazon RDS.

Shared Controls are controls which apply to both the infrastructure layer and customer layers, but in completely separate contexts or perspectives.
In a shared control, AWS provides the requirements for the infrastructure and the customer must provide their own control implementation within their use of AWS services.
Examples include:
** Patch Management – AWS is responsible for patching the underlying hosts and fixing flaws within the infrastructure, but customers are responsible for patching their guest OS and applications.
** Configuration Management – AWS maintains the configuration of its infrastructure devices, but a customer is responsible for configuring their own guest operating systems, databases, and applications.
** Awareness & Training - AWS trains AWS employees, but a customer must train their own employees.

Patching the guest operating system is the responsibility of AWS for the managed services only (such as Amazon RDS).
The customer is responsible for patching the guest OS for other services (such as Amazon EC2).
AWS is responsible for patching the underlying hosts, upgrading the firmware, and fixing flaws within the infrastructure for all services, including Amazon EC2.

Both client side and server side encryption is a responsibility of customer.
AWS offers a lot of services and features that help AWS customers protect their data in the cloud.
Customers can protect their data by encrypting it in transit and at rest. They can use Cloudtrail to log API and user activity, including who, what, and from where calls were made.
They can also use the AWS Identity and Access Management (IAM) to control who can access or edit their data.

5. AWS MICROSERVICES vs MONOLITHIC :
--------------------------------------
AWS recommends adopting microservices architecture, not monolithic architecture. With monolithic architectures, application components are tightly coupled and run as a single service.
With a microservices architecture, an application is built as loosely coupled components.
Benefits of microservices architecture include:
  1- Microservices allow each service to be independently scaled to meet demand for the application feature it support.
  2- Teams are empowered to work more independently and more quickly.
  3- Microservices enable continuous integration and continuous delivery, making it easy to try out new ideas and to roll back if something doesn’t work.
  4- Service independence increases an application’s resistance to failure.
     In a monolithic architecture, if a single component fails, it can cause the entire application to fail.
     With microservices, applications handle total service failure by degrading functionality and not crashing the entire application.

6.  AWS PENETRATION TESTING : https://aws.amazon.com/security/penetration-testing/
-------------------------------
AWS customers are welcome to carry out security assessments and penetration tests against their AWS infrastructure without prior approval for 8 services:
So Penetration testing can be done by customers on their own instances without prior authorization from AWS.

      PERMITTED ACTIVITIES :                        PROHIBITED ACTIVITIES :
  ---------------------------------------------------------------------------------------------------------------------------------------------
      1- Amazon EC2 instances, NAT Gateways,        1. DNS zone walking via Amazon Route 53 Hosted Zones
         and Elastic Load Balancers.
      2- Amazon RDS.                                2. Denial of Service (DoS), Distributed Denial of Service (DDoS), Simulated DoS, Simulated DDoS
      3- Amazon CloudFront.                         3. Port flooding
      4- Amazon Aurora.                             4. Protocol flooding
      5- Amazon API Gateways.                       5. Request flooding  (login request flooding, API request flooding)
      6- AWS Lambda and Lambda Edge functions.
      7- Amazon Lightsail resources.
      8- Amazon Elastic Beanstalk environments.

The AWS customers are responsible for performing penetration tests against their AWS infrastructure.
AWS customers are allowed to perform penetration tests against their AWS infrastructure, but they must ensure that their activities are aligned with AWS policies.
AWS customers are allowed to perform penetration testing on both AWS-managed services such as Amazon RDS and customer-managed services such as Amazon EC2.

Note: Customers are not permitted to conduct any security assessments of AWS infrastructure, or the AWS services themselves.
      If you discover a security issue within any AWS services in the course of your security assessment, please contact AWS Security immediately.

7. HORIZONTAL & VERTICAL SCALING :
-------------------------------------
HORIZONTAL -> Adding more EC2 instances
Scaling horizontally takes place through an increase in the number of resources (e.g., adding more hard drives to a storage array or adding more servers to support an application).
This is a great way to build Internet-scale applications that leverage the elasticity of cloud computing.

VERTICAL -> Add specifications to a single resource.
Scaling vertically takes place through an increase in the specifications of an individual resource (e.g., upgrading a server with a larger hard drive, adding more memory, or provisioning a faster CPU).
On Amazon EC2, this can easily be achieved by stopping an instance and resizing it to an instance type that has more RAM, CPU, I/O,or networking capabilities. This way of scaling can eventually hit a limit and it is not always a cost efficient or highly available approach. However, it is very easy to implement and can be sufficient for many use cases especially as a short term solution.
A "vertically scalable" system is constrained to running its processes on only one computer; in such systems, the only way to increase performance is to add more resources into one computer in the form of faster (or more) CPUs, memory, or storage.
Vertical scaling may improve performance, but not fault-tolerance; because if this "one computer" fails, the whole system will fail.

Vertical-scaling is often limited to the capacity constraints of a single machine, scaling beyond that capacity often involves downtime and comes with an upper limit.
With horizontal-scaling it is often easier to scale dynamically by adding more machines in parallel. Hence, in most cases, horizontal-scaling is recommended over vertical-scaling.
Horizontally scalable systems are oftentimes able to outperform vertically scalable systems by enabling parallel execution of workloads and distributing those across many different computers.

8. AWS WELL ARCHITECTURED FRAMEWORK : https://d1.awsstatic.com/whitepapers/architecture/AWS_Well-Architected_Framework.pdf
---------------------------------------
The Well-Architected Framework identifies a set of general design principles to facilitate good design in the cloud:

  --> STOP GUESSING YOUR CAPACITY NEEDS :
      Eliminate guessing about your infrastructure capacity needs.
      When you make a capacity decision before you deploy a system, you might end up sitting on expensive idle resources or dealing with the performance implications of limited capacity.
      With cloud computing, these problems can go away.
      You can use as much or as little capacity as you need, and scale up and down automatically.

  --> TEST SYSTEMS AT PRODUCTION SCALE :
      In the cloud, you can create a production-scale test environment on demand, complete your testing, and then decommission the resources.
      Because you only pay for the test environment when it's running, you can simulate your live environment for a fraction of the cost of testing on premises.

  --> AUTOMATE TO MAKE YOUR ARCHIECTURAL EXPERIMENTATION EASIER :
      Automation allows you to create and replicate your systems at low cost and avoid the expense of manual effort.
      You can track changes to your automation, audit the impact, and revert to previous parameters when necessary.

  --> ALLOW FOR EVOLUTIONARY ARCHITECTURES :
      In a traditional environment, architectural decisions are often implemented as static, one-time events, with a few major versions of a system during its lifetime.
      As a business and its context continue to change, these initial decisions might hinder the system's ability to deliver changing business requirements.
      In the cloud, the capability to automate and test on demand lowers the risk of impact from design changes.
      This allows systems to evolve over time so that businesses can take advantage of innovations as a standard practice.

  --> DRIVE ARCHITECTURES USING DATA :
      In the cloud you can collect data on how your architectural choices affect the behaviour of your workload.
      This lets you make fact-based decisions on how to improve your workload.
      Your cloud infrastructure is code, so you can use that data to inform your architecture choices and improvements over time.

  --> IMPROVE THROUGH GAME DAYS :
      Test how your architecture and processes perform by regularly scheduling game days to simulate events in production.
      This will help you understand where improvements can be made and can help develop organizational experience in dealing with events.
