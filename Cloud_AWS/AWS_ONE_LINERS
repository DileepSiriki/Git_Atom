CLOUD :  [ Define 4 lines - Uses - Types - Virtualization - Hypervisor & Types]
---------
  DEFINE :
    On-demand - IT resources compute DB n/w apps - public / private - pay as you go -- Buy maintain own - from vendors like AWS & Azure
    Platform independent - no downloaded software
    Small to large - backup / recovery / email / VM / web apps / big data analytics
    hospitals - finance teams - gaming companies

  USES :
    A agile - c cost effective - D deploy global in sec - E elastic - F trade capital expense for variable expenses - G no engineers for maintenance
    Guessing capacity - no running data centres - pay as you go - serverless - scalable , shared responsibility.

  ADVANTAGES :
  Trade capital for variable expense, Benefit from massive economies of scale, Stop guessing capacity, Increase speed and agility, Stop spending money on running and maintaining data centers, Go global in minutes.

  TYPES          : DEPLOYMENT MODELS :  Public Private Hybrid Community --- SERVICE MODELS : IAAS Admins, PAAS Developers, SAAS Users & Diagram
  VIRTUALIZATION : Technique of sharing 1 resource among many by assigning a logical name to resource and providing a pointer to physical resource when demanded.
                   Server, Application, Hardware virtualization
  HYPERVISOR     : Guest machines, Type-1 & Type-2 [ Bare metal & Hosted ] - Diagram

*****************************************************************************************************************************************************************************************
*****************************************************************************************************************************************************************************************

AMAZON WEB SERVICES : ~25 regions & ~80 AZ's --  175 services over 190 countries - 5 pillars P R O C S  [security/shared responsibility ]
-----------------------
Secure cloud service offering a broad set of global cloud-based products. - ACDEFGS from Cloud usages

  5 PILLARS - PROCS : https://aws.amazon.com/blogs/apn/the-5-pillars-of-the-aws-well-architected-framework/
  -----------------------

    Performance Efficiency - >   // Horizontal, Vertical Scaling & Serverless architecture -- Experiment more often
    The performance efficiency pillar includes the ability to use computing resources efficiently to meet system requirements.
    Key topics include selecting the right resource types and sizes based on workload requirements, monitoring performance, and making informed decisions to maintain efficiency as business needs evolve.
    On how you can run services efficiently and scalably.

    Reliability            - >  // Availability Zone & Region -- Stop guessing about capacity
    The reliability pillar includes the ability of a system to recover from infrastructure or service disruptions, dynamically acquire computing resources to meet demand, and mitigate disruptions such as misconfigurations or transient network issues.
    A resilient workload quickly recovers from failures to meet business and customer demand.
    Key topics include distributed system design, recovery planning, and how to handle change.
    On how you can build services that are resilient to both service and infrastructure disruptions.

    Operational excellence - >   // Automation to escape human errors - Cloud formation to automate -- Anticipate failure -- Make frequent, small, reversible changes
    The operational excellence pillar includes the ability to run and monitor systems to deliver business value and to continually improve supporting processes and procedures.
    Key topics include automating changes, responding to events, and defining standards to manage daily operations.
    On how you can continuously improve your ability to run systems, create better procedures, and gain insights.

    Cost optimization      - >   // cost explorer - Budgets - Cost usage
    On the ability to avoid or eliminate unneeded cost or sub-optimal resources. CAPEX to OPEX

    Security               - >  Shared Model // Policies (IAM) , Network Security (VPC) , Encryption (KMS), IAM, KMS, MFA, Cloud Trail, Cloud Watch, SNS, Email. -- Enable traceability
    The security pillar includes the ability to protect information, systems, and assets while delivering business value through risk assessments and mitigation strategies.
    Key topics include confidentiality and integrity of data, identifying and managing who can do what with privilege management, protecting systems, and establishing controls to detect security events.

[ DIAGRAM ] --> Regions - Availability Zones - Local Zone (Similar to regions but closer to users), available only in 3 places - Outposts (Cloud in On-premises)- Wavelengths

*****************************************************************************************************************************************************************************************
*****************************************************************************************************************************************************************************************

-------------------------------------------------------------------------------------------------------------------------------------------------
SERVICES COVERED :
-------------------------------------------------------------------------------------------------------------------------------------------------
Analytics                 |  Compute             |   Data Base              |  Management & Governance       |  Satellite
Application Integration   |  Containers          |   Developer Tools        |  Migration & Transfer          |  Security, Identity & Compliance
AR & VR                   |  Cryptography & PKI  |   End User Computing     |  Networking & Content Delivery |  Storage
Billing & Cost Management |  Customer Enablement |   Front-End Web & Mobile |  Quantum Computing             |
Business Applications     |  Customer Engagement |   Game Development       |  Robotics                      |
Blockchain                |                      |                          |                                |

-------------------------------------------------------------------------------------------------------------------------------------------------
SERVICES NOT COVERED :
-------------------------------------------------------------------------------------------------------------------------------------------------
Internet of Things (IoT)  |
Machine Learning          |
Media Services            |


**********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************
**********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************
:::: ANALYTICS ::::
**********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************
**********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************

1. APP-FLOW       : Bidirectional data transfer from SAAS Applications ( Slack or Sales Force ) to AWS ( S3 or Redshift ) for analytics and archiving and automatic workflow.
2. DATA EXCHANGE  : To securely exchange file-based data sets in the AWS Cloud from qualified data providers and then use the data across a variety of AWS analytics and machine learning service. AWS Data Exchange scans all data published by providers before it is made available to subscribers. Used by colleges, hospitals , scientists to get data from various data lakes .
3. DATA PIPELINE  : To automate the movement and transformation of data across AWS compute and storage resources, as well as your on-premises resources. You upload your pipeline definition to the pipeline, and then activate the pipeline and it schedules and runs tasks by creating Amazon EC2 instances to perform the defined work activities. For example, Task Runner could copy logfiles to Amazon S3 and launch Amazon EMR clusters.
                    Task Runner polls for tasks in pipeline and then performs those tasks. You can define data-driven workflows, so that tasks can be dependent on the successful completion of previous tasks.

4. AWS GLUE            : Fully managed ETL (extract, transform, and load) service used to categorize your data, clean it, enrich it, and move it reliably between various data stores and data streams.
                         It is used to combine data [ Prepare a data lake ] from various sources and clean , normalize and prepare data to one common syntax and save to S3. EXTRACT from various DB's [CSV/JSON/XML]-------------> [[ TRANSFORM to common DB ]] ----------------> LOAD to another DB .
                         Automatically identify type of data in S3/Redshift --> then provides a unified view of data as catalog that can be used with EMR/ATHENA. Glue automatically generates Scala or Python code for your ETL jobs that you can further customize.
5. AWS LAKE FORMATION  : The data lake is your persistent data that is stored in Amazon S3 and managed by Lake Formation using a Data Catalog. AWS Lake Formation is a fully managed service that makes it easier for you to build, secure, and manage data lakes.
                         Collect -> Clean -> Move to S3 -> make data available for Analytics. // Lake Formation simplifies and automates many of the complex manual steps that are usually required to create data lakes. These steps include collecting, cleansing, moving, and cataloging data, and securely making that data available for analytics and machine learning. Once a datalake is formed we can use EMR/Athena/Redshift for analytics.
6. AWS QUCIK SIGHT     : Combines AWS data + 3rd party data + SaaS data --> Combine --> generate catalogs. // It is a cloud-scale business intelligence (BI) service that you can use to deliver easy-to-understand insights to the people who you work with, wherever they are. It connects to your data in the cloud and combines data from many different sources.
                         In a single data dashboard, QuickSight can include AWS data, third-party data, big data, spreadsheet data, SaaS data, B2B data, and more.

7. ATHENA              : Query service that makes it easy to analyze Structured, Unstructured, Semi Structured data (CSV, columnar, standard or JSON format) in Amazon S3 using standard SQL then store results from queries directly into another bucket in S3 or download then to local. Login to console, define your schema, and start querying directly on S3 buckets.
8. AWS KINESIS         : To collect, process, and analyze real-time, streaming data such as video, audio, application logs, website clickstreams, and IoT telemetry data for machine learning, analytics, and other applications. You can use Amazon Kinesis services for real-time application monitoring, fraud detection, and live leader-boards.
                         KINESIS STREAMS --> Collect and store data streams     // KINESIS FIREHOSE --> Process and deliver data streams //  KINESIS ANALYTICS --> Analyze streaming data and get insights.
9. ELASTIC MAP REDUCE  : Apache Spark and apache Hadoop are big data frameworks used to store data from a datalake [ purchase info - social media - server logs ] similar to S3 ad process those logs. Maintaining them is costly so EMR does this for  us . Master node - Core Node - Task Node . Amazon EMR launches clusters in minutes.
    // EMR               Upload data to S3 --> EMR launches cluster with specified EC2 instances --> pull data from S3 into EC2 --> Store output in S3 --> Delete cluster. // EMR uses Hadoop engine and runs Hadoop software on the instances. The central component of Amazon EMR is the cluster. A cluster is a collection of Amazon Elastic Compute Cloud (Amazon EC2) instances. Each instance in the cluster is called a node. Each node has a role within the cluster, referred to as the node type.
                         The customer implements their algorithm in terms of map() and reduce() functions. Clusters comprises of one master and multiple other nodes. The master node divides input data into blocks, and distributes the processing of the blocks to the other nodes. Each node runs MAP and REDUCE functions to break and join the data at the end.
10. AMAZON REDSHIFT    : Petabyte-scale cloud data warehouse for only relational data that analyses all your data using standard SQL.
    // SPECTRUM          It is specifically designed for online analytic processing (OLAP) and business intelligence (BI) applications, which require complex queries against large datasets.
                         Amazon Redshift also includes Amazon Redshift Spectrum, allowing you to run SQL queries directly against exabytes of unstructured data in Amazon S3 data lakes.

You should use Amazon EMR if you use custom code to process and analyze extremely large datasets with big data processing frameworks such as Apache Spark, Hadoop, Presto, or Hbase. Amazon EMR gives you full control over the configuration of your clusters and the software you install on them.
Data warehouses like Amazon Redshift are designed for a different type of analytics altogether.
Data warehouses are designed to pull together data from lots of different sources, like inventory, financial, and retail sales systems.
In order to ensure that reporting is consistently accurate across the entire company, data warehouses store data in a highly structured fashion.
structure builds data consistency rules directly into the tables of the database.
Amazon Redshift is the best service to use when you need to perform complex queries on massive collections of structured and semi-structured data and get fast performance.

11. CLOUD SEARCH        : Add search capabilities to your website or application. Upload all data and documents and it will create a search index. Autocomplete suggestions - Geospatial search - Highlighting - Support for 34 languages.
12. ELASTIC SEARCH      : Log stash is an open source tool for collecting, parsing, and storing logs for future use. Kibana 3 is a web interface that can be used to search and view the logs that Logstash has indexed. Both of these tools are based on Elasticsearch. Elasticsearch, Logstash, and Kibana, when used together is known as an ELK stack.
    ( ELK Stash)          Used to analyse and get real time  insights on machine generated data at a peta byte scale by deploying, operating, and scaling Elasticsearch clusters in the AWS Cloud.
13. AMAZON  MANAGED STREAMING FOR KAFKA - MSK : Managed Streaming for Apache Kafka that enables you to build and run applications that use Apache Kafka to process streaming data.

// BIGDATA HADOOP SPARK - ELASTIC SEARCH, LOG STATSH, KIBANA - KAFKA

**********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************
**********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************
:::: CONTAINERS ::::
**********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************
**********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************

DOCKER FILE  is a text document that contains commands that are used to assemble an image.  Docker builds images automatically by reading the instructions from the Dockerfile. DOCKER image is a read-only template with instructions for creating a Docker container.
DOCKER packages software into standardized units called CONTAINERS . They have everything your software needs to run including Libraries , system tools , code and run time .
DOCKER FILE --> docker build ---> DOCKER IMAGE -- docker run ---> DOCKER CONTAINER.
$ docker build  /path/to/a/Dockerfile  --> To build an image.
$ docker run /path/to/docker_image     --> To create a container.

Elastic container Registry - Elastic container Service -
1. ECR  : It is a fully managed Docker container registry that makes it easy for developers to store, manage, and deploy DOCKER CONTAINER IMAGES. Supports both private / public container image repositories.
          Amazon ECR integrates with Amazon ECS, Amazon EKS, AWS Fargate, AWS Lambda, and the Docker CLI, allowing you to simplify your development and production workflows. Uses S3 for storing.
2. ECS  : Container service that makes it easy to run, stop, and manage containers on a cluster. You can create Amazon ECS clusters within a new or existing VPC.
          After a cluster is up and running, you can create task definitions that define which container images run across your clusters. The task definition is a text file (in JSON format) that describes one or more containers ( up to a maximum of 10 ) that form your application.
          The task definition can be thought of as a blueprint for your application that indicate which containers should be used, which ports should be opened for your application, and what data volumes should be used with the containers in the task.
3. EKS  : KUBERNETIS also known as K8s, is an open-source system for automating deployment, scaling, and management of containerized applications. It groups containers that make up an application into logical units for easy management and discovery.
          It s a managed service that you can use to run Kubernetes on AWS without needing to install, operate, and maintain your own Kubernetes control plane or nodes. Amazon EKS automatically detects and replaces unhealthy control plane instances, and it provides automated version upgrades and patching for them.

// EC2          : Deploy and manage your own cluster of EC2 instances for running the containers - For consistent demand of CPU cores.
                  With the EC2 launch type billing is based on the cost of the underlying EC2 instances. This allows you to optimize price by taking advantage of billing models such as spot instances (bid a low price for an instance), or reserved instances .
// FARGATE      : Run containers directly, without any EC2 instance - Need not manage compute - for tiny and infrequent workloads.
                  If your workload is small with the occasional burst, such as a website that has traffic during the day but low traffic at night, then AWS Fargate is a fantastic choice.

4. FARGATE       : A serverless compute engine for containers that works with both ECS and EKS. Fargate allocates the right amount of compute, eliminating the need to choose instances and scale cluster capacity unlike ECS.
                   Fargate removes the need to provision and manage servers, lets you specify and pay for resources per application, and improves security through application isolation by design.
5. APP2CONTAINER : It is a command-line tool for modernizing .NET and Java applications into containerized applications. Using A2C simplifies your migration tasks by performing inventory and analysis of your existing applications, creating Docker containers that include your application dependencies, and generating deployment templates.
                   After you have reviewed your templates, A2C helps you register your containers to Amazon ECR, deploy to Amazon ECS or Amazon EKS, and build CI/CD pipelines using AWS CodeStar.


**********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************
**********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************
:::: STORAGE :::: // Object S3 - Block EBS  - File EFS, FSx
**********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************
**********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************
1. AWS S3 : 11 9's Durable - Backup Storage for internet - Static websites, FTP apps - virtually unlimited storage - media hosting - software delivery.... // Permissions - Versioning - Replication - Server side & client side encryption - Macie
            Amazon S3 stores data as objects within buckets. An object is a file and any optional metadata that describes the file. Buckets are containers for objects. Max 100 buckets per account, max 5tb per object and unlimited objects per bucket. // OBJECT : Photos/puppy.jpg -- BUCKET : Dileep --> URL : http://Dileep.s3.aws.com/photos/puppy.jpg . Data is stored a key value pairs. Key is the name of the object and the value is the actual content of file.
            S3 Cross-Region Replication (CRR) to replicate buckets into same or different regions using S3 Replication Time Control (S3 RTC) to replicate within 15minutes.
            1. FOR FREQUENTLY ACCESSED OBJECTS   -->  S3 standard ( A-99.99% )& Reduced Redundancy (for reproduceable data, least durable of all classes ) --> Ideal for cloud applications, dynamic websites, content distribution, mobile and gaming applications, and big data analytics.
            2. FOR AUTOMATICALLY OPTIMIZING DATA WITH CHANGING PATTERNS --> S3 Intelligent Tiering ( A-99.9%, min 128kb ) --> 30 (to infrequent), 90 (to archive), 180 (to deep archive) --> Ideal for new applications and data lakes
            3. FOR INFREQUENTLY ACCESSED OBJECTS --> S3 Standard IA ( A-99.9% ) & S3 One zone IA ( A-99.5% , stores data in a single AZ and costs 20% less, not resilient to AZ destruction ) --> Ideal for long-term storage, backups, and as a data store for disaster recovery files.
            4. FOR ARCHIVING OBJECTS --> S3 Glacier (A-99.99%, 1-5 mnt retrieval, 90days) & S3 Glacier Deep Archive (A-99.99%, 12 hrs retrieval, 180 days) --> Ideal for long-term archive. and the Glacier deep is Ideal for those in highly-regulated industries, such as the Financial Services, Healthcare, and Public Sectors — that retain data sets for 7-10 years or longer to meet regulatory compliance requirements.
            PRICE : Standard [ 0.02300 0.02200 0.02100 ] - Infrequent [ 0.0125 0.01000 ] - glacier [ 0.00400 0.00099]
            S3 TRANSFER ACCELERATION - Transferring your data to popular AWS storage platform S3 over long distances, AWS S3 Transfer Acceleration helps you do it faster 171% faster over the public Internet.

2. ELASTIC BLOCK STORE : It is a hard drive to EC2 instances - Persists its data even after termination of an instance - EBS can be connected to only one Instance at a time - But one instance can be connected more than one EBS .
                         As a primary storage device for data that requires frequent and granular updates.
    ( HDD - SDD )        Detach and attach to different instance - change configuration any time - independent of lifecycle of instance - can encrypt and take periodic incremental snapshots.

3. STORAGE GATEWAY     : Hybrid cloud storage service that seamlessly connect on-premises applications to cloud storage, caching data locally for low-latency access. This gives you on-premises access to virtually unlimited cloud storage.
                         Download the virtual appliance gateway or purchase the hardware appliance, configure required storage, and manage and monitor the service. Once configured, you start using the gateway to write and read data to and from AWS storage. You can monitor the status of your data transfer and your storage interfaces through the AWS Management Console.
                         Storage GW can be a --> VM on premises - EC2 instance - Hardware device.
                         1. FILE GW   -> retrieve objects in S3 using Network File System - NFS (linux) / Server Messaging protocol- SMB (Windows), GW deployed on a VM running on Hyper-V / ESXi / KVM
                         2. VOLUME GW -> On-premises applications can access these as Internet Small Computer System Interface (iSCSI)targets --> Cached (Store in s3 and retain freq. data as cache in on-premises)- Stored (Store in on-premises and save backups to S3)
                         3. TAPE GW   -> Cost-effectively and durably archive backup data in GLACIER or DEEP_ARCHIVE -- GW deployed on a VM running on Hyper-V / ESXi / KVM

4. SNOW FAMILY         : Snow cone      -- 8 TB storage   --> 1 SMALL BOX [ uses opshub application for data transfer ]
                         Snow Ball      -- 80 TB storage  --> 1 SUITCASE  [ Snowball is a petabyte-scale data transport solution by cascading snow balls ]
                         Snow Ball edge -- 100 TB storage --> 1 SUITCASE  [ Embedded Ec2 - Lambda for running jobs in remote areas like ships , hills for an immediate results. ]
                         Snow Mobile    -- 1 Exabyte      --> 1 Truck     [ 100Pb per snow mobile --  AWS Snowmobile is the exabyte-scale data migration service ]
    // For edge computing applications, to collect data, process the data to gain immediate insight, and then transfer the data online to AWS by shipping the device to AWS, or online by using AWS DataSync.
    // The Snow cone device supports data transfer from on-premises Windows, Linux, and macOS servers and file-based applications through the NFS interface.
    // Using Snowball addresses common challenges with large-scale data transfers including high network costs, long transfer times, and security concerns.
    // S3 buckets, data, and EC2 AMIs that you choose are automatically configured, encrypted, and pre-installed on your devices. The AWS DataSync agent is also pre-installed before your devices are shipped to you. you connect it to your on-premises network and set the IP address either manually or automatically with DHCP. You must download and install AWS OpsHub for Snow Family, on a windows or mac laptop.

5. AWS BACKUP : Automatic backups of our AWS resources using BAKUP PLANS. It automates and consolidates backup tasks that were previously performed service-by-service, and removes the need to create custom scripts and manual processes.
                These features include Amazon Elastic Block Store (Amazon EBS) snapshots, Amazon Relational Database Service (Amazon RDS) snapshots, Amazon DynamoDB backups, AWS Storage Gateway snapshots, and others.
6. AWS EFS    : Only for LINUX -- simple, scalable, fully managed elastic NFS file system for use with AWS Cloud services and on-premises resources. you can create a file system, mount the file system on your EC2 instances, and then read and write data from your EC2 instances to and from your file system.
                Multiple Amazon EC2 instances can access an Amazon EFS file system at the same time, providing a common data source for workloads and applications running on more than one instance or server.

                EC2-a --
                EC2-b   | <---------->  |||||||||  <---------->  On-Premises
                EC2-c --       NFS         EFS          NFS

7. AWS FSX    : For WINDOWS -- Fully managed third-party file systems with the native compatibility and feature sets for workloads such as Microsoft Windows–based storage, high-performance computing, machine learning, and electronic design automation.
                LUSTURE &  WINDOWS FILE SERVER ---- With file storage on Amazon FSx, the code, applications, and tools that Windows developers and administrators use today can continue to work unchanged.


**********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************
**********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************
:::: CRYPTOGRAPHY AND PKI [ Public Key Infrastructure ]  SERVICES  ::::
**********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************
**********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************

AWS provides multiple services that you can use to help protect your data at rest or in transit.
Cryptography is the practice of protecting information through the use of coded algorithms, hashes, and signatures. Encryption -> Cipher text -> Decryption .
Symmetric [ Share Key - Advanced Encryption Standard (AES) & Triple DES (3DES) ] -  Asymmetric [ Public key - Rivest Shamir Adleman (RSA) & Elliptic Curve Cryptography (ECC) ] - Client side - Server Side

#### AWS OWNED CMK _ AWS MANAGED CMK _ CUSTOMER MANAGED CMK
A customer master key (CMK) is a logical representation of a master key.
The CMK includes metadata, such as the key ID, creation date, description, and key state.
The CMK also contains the key material used to encrypt and decrypt data.
AWS KMS supports symmetric and asymmetric CMKs.

1. CLOUD HSM : HARDWARE SECURITY MODULE is a computing device that performs cryptographic operations and provides secure storage for cryptographic keys.
               Used to Generate, store, import, export, and manage cryptographic keys, including symmetric keys and asymmetric key pairs
               -- Use symmetric and asymmetric algorithms to encrypt and decrypt data -- Use cryptographic hash functions to compute message digests and hash-based message authentication codes (HMACs) -- Cryptographically sign data (including code signing) and verify signatures.
               // If you need to secure your encryption keys in a service backed by FIPS-validated HSMs, but you do not need to manage the HSM, try AWS Key Management Service.
2. AWS KMS   : KEY MANAGEMENT SERVICE  provides tools for generating master keys and other data keys and also interacts with many other AWS services to encrypt their service-specific data.
               The customer master keys (CMKs) that you create in AWS KMS are protected by FIPS 140-2 validated cryptographic modules. They never leave AWS KMS unencrypted. To use or manage your CMKs, you interact with AWS KMS.
               CMK is a 256bit AES symmetric key used to encrypt and decrypt the DATA KEYS that encrypt the actual data. You can also create asymmetric RSA or elliptic curve (ECC) CMKs backed by asymmetric key pairs. The public key in each asymmetric CMK is exportable, but the private key remains within AWS KMS.
               AWS KMS does not store or manage data keys, and you cannot use KMS to encrypt or decrypt with data keys. To use data keys to encrypt and decrypt, use the AWS Encryption SDK.
               KMS -- Generate --> CMK -- Encrypt & Decrypt --> DATA KEYS -- Encrypt & Decrypt --> Actual Data
               |____________________ KMS_________________________________| |_________ ENRYPTION SDK __________|


3. DYNAMO DB ENC. CLIENT : A client-side encryption library that helps you to protect your table data before you send it to Amazon DynamoDB. INTEROPERABLE b/w java and python.
                           It encrypts the attribute values in each table item using a unique encryption key. It then signs the item to protect it against unauthorized changes, such as adding or deleting attributes or swapping encrypted values. It also verifies and decrypts them when you retrieve them.
4. S3 CLIENT SIDE ENCRYP : 1 -->  Using a CMK stored in AWS KMS : Client requests KMS for a CMK to generate Data keys. KMS sends two versions of plain and encrypted data key. Client encrypts data using plain key and embeds encrypted key as meta data.
                                  Upon transfer , S3 gets plain key from encrypted key using KMS using CMK and decrypts the object . ONE KEY PER ONE OBJECT
                           2 -->  Using a master key stored within your application  : You provide a client-side master key to the Amazon S3 encryption client. The client uses the master key only to encrypt the data encryption key that it generates randomly.
                                  S3 client generates a plain text data key , encrypts the data and now the client-side master key encrypts the plain text data key and store it as meta data in object . while downloading first decrypt the data key using client-side master key and use it to decrypt the data.
                                  It's important that you safely manage your client-side master encryption keys. If you lose them, you can't decrypt your data.

// PKI : Public key Infrastructure is a system of hardware, software, people, policies, documents, and procedures.
// It includes the creation, issuance, management, distribution, usage, storage, and revocation of digital certificates.

5. AWS CERTIFICATE MANAGER : ACM - Service that lets you easily provision, manage, and deploy public and private Secure Sockets Layer/Transport Layer Security (SSL/TLS) certificates for use with AWS services and your internal connected resources.
                             These public certificates verify the identity and authenticity of your web server and the ownership of your public keys. In doing so, public certificates initiate a trusted, encrypted connection between you and your users.
6. AWS ACM PRIVATE CERTIFICATE AUTHORITY : ACM PCAIs service is for enterprise customers building a public key infrastructure (PKI)inside the AWS cloud and intended for private use within an organization.
                             With private certificates you can authenticate resources inside an organization. Private certificates allow entities like users, web servers, VPN users, internal API endpoints, and IoT devices to prove their identity and establish encrypted communications channels.

                             Plain text     --> [[ cryptographic hash function - SHA (Secure HASH) ]]  --> Original Message Digest also called a HASH
                             Message Digest --> [[ Plain text private key + Signing Algorithm      ]]  --> SIGNATURE also called DIGITAL SIGNATURE also called a CERTIFICATE

                             ** We need to send both the Original Message Digest also called a HASH and the signature along with the message to the client .

                             Signature   --> [[ Public key  + Decryption Algorithm ]]    --> Generated Message Digest also called a generated HASH
                             Generated Message Digest + Original Message Digest  --> [[ Verification Algorithm ]] --> TRUE or FALSE

7. ENCRYPTION SDK          : A client-side encryption library to help you implement best-practice encryption and decryption in any application even if you're not a cryptography expert.
                             Every successful call to encrypt returns a single portable, formatted encrypted message that contains metadata and the message ciphertext.
                             All implementations are interoperable. For example, you can encrypt your data with the Java library and decrypt it with the Python library. Or you can encrypt data with the C library and decrypt it with the CLI.
8. AWS SECRET MANAGER      : Provides encryption and rotation of encrypted secrets used with AWS-supported databases. To protect all other types of data at their source, use the AWS Encryption SDK.
9. AWS SIGNER              : Fully managed code-signing service to ensure the trust and integrity of your code. Organizations validate code against a digital signature to confirm that the code is unaltered and from a trusted publisher.
                             For example With Code Signing for AWS Lambda, you can ensure that only trusted code runs in your Lambda functions.
10. AWS CRYPTO TOOLS       : Part of the AWS ENCRYPTION SDK . The AWS Crypto Tools libraries are designed to help everyone do cryptography right, even without special expertise.


**********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************
**********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************
:::: DATABASE  ::::
**********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************
**********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************

Structured Query language (SQL) pronounced as "S-Q-L" or sometimes as "See-Quel" is the standard language for dealing with Relational Databases.
A relational database defines relationships in the form of tables. Ex : MySQL Database, Oracle, Ms SQL Server, Sybase, PostgreSQL etc.
ACID --> Atomicity (Either all of its operations are executed or none)                  - Consistency (The database must remain in a consistent state after any transaction)
         Isolation (No transaction will affect the existence of any other transaction)  - Durability  (The database should be durable enough to hold all its latest updates even if the system fails or restarts.)

Not only Structured Query language (NO-SQL) pronounced as "Not only SQL" or sometimes as "Not SQL" is the standard language for dealing with Non-Relational/Distributed  Databases.
NoSQL is a non-relational DMS, that does not require a fixed schema, avoids joins, and is easy to scale. NoSQL database is used for distributed data stores with humongous data storage needs. NoSQL is used for Big data and real-time web apps
Ex : MongoDB, Redis, , Neo4j, Cassandra, Hbase.
BASE : Basically Available - Soft State - Eventually Consistent

DATABASE TYPE     USE CASE                                                      AWS SERVICE
================================================================================================================================================
Relational        Traditional applications, ERP, CRM, e-commerce                Amazon Aurora - Amazon RDS -  Amazon Redshift
================================================================================================================================================
Key-value         High-traffic web apps, e-commerce systems,                    Amazon DynamoDB
(JSON-like)       gaming applications
================================================================================================================================================
In-memory         Caching, session management, gaming leader boards,            Amazon ElastiCache for Memcached -  Amazon ElastiCache for Redis
                  geospatial applications
================================================================================================================================================
Document          Content management, catalogs, user profiles                   Amazon DocumentDB (with MongoDB compatibility)
================================================================================================================================================
Wide column       High scale industrial apps for equipment maintenance,         Amazon Keyspaces (for Apache Cassandra)
                  fleet management, and route optimization
================================================================================================================================================
Graph             Fraud detection, social networking, recommendation engines    Amazon Neptune
================================================================================================================================================
Time series       IoT applications, DevOps, industrial telemetry                Amazon Timestream
================================================================================================================================================
Ledger            Systems of record, supply chain, registrations,               Amazon QLDB
                  banking transactions
================================================================================================================================================

REDSHIFT vs RDS :
Both Amazon Redshift and Amazon RDS enable you to run traditional relational databases in the cloud while offloading database administration.
Customers use Amazon RDS databases primarily for online-transaction processing (OLTP) workload while Redshift is used primarily for reporting and analytics.
OLTP workloads require quickly querying specific information and support for transactions like insert, update, and delete and are best handled by Amazon RDS.
Amazon Redshift harnesses the scale and resources of multiple nodes and uses a variety of optimizations to provide order of magnitude improvements over traditional databases for analytic and reporting workloads against very large data sets.

1. RDS : It is a web service that makes it easier to set up, operate, and scale a relational database in the cloud. DB instances use Amazon Elastic Block Store (Amazon EBS) volumes for database and log storage.
         Amazon RDS provides you with six widely-used database engines to choose from, including Amazon Aurora, PostgreSQL, MySQL, MariaDB, Oracle Database, and SQL Server.
         The basic building block of Amazon RDS is the DB instance. Each DB instance runs a DB engine. The computation and memory capacity of a DB instance is determined by its DB instance class [ Magnetic, General Purpose (SSD), Provisioned IOPS(PIOPS) ] .
         Your primary DB instance is synchronously replicated across Availability Zones to the secondary instance. A security group controls the access to a DB instance. It does so by allowing access to IP address ranges or Amazon EC2 instances that you specify. When you use Amazon RDS, you can choose to use on-demand DB instances or reserved DB instances.
         You can have up to 40 Amazon RDS DB instances. Each DB instance has a DB instance identifier. The identifier is used as part of the DNS hostname allocated to your instance by RDS. For example, if you specify db1 as the DB instance identifier, then RDS will automatically allocate a DNS endpoint for your instance, such as db1.123456789012.us-east-1.rds.amazonaws.com

    READ REPLICAS :
    -----------------
    Amazon RDS Read Replicas enable you to create one or more read-only copies of your database instance within the same AWS Region or in a different AWS Region.
    Updates made to the source database are then asynchronously copied to your Read Replicas. In addition to providing scalability for read-heavy workloads, Read Replicas can be promoted to become a standalone database instance when needed.
    Amazon RDS Multi-AZ deployments provide enhanced availability for database instances within a single AWS Region.
    With Multi-AZ, your data is synchronously replicated to a standby in a different Availability Zone (AZ). In the event of an infrastructure failure, Amazon RDS performs an automatic failover to the standby, minimizing disruption to your applications.

2. AURORA : Fully managed relational database engine that's compatible with MySQL(5 times throughput) and PostgreSQL(3 times throughput).
            The only RDS database that can scale instances automatically is Amazon Aurora. For RDS databases other than Aurora, they only support storage auto-scaling, NOT instance auto-scaling.
            If you want to scale Amazon RDS instances (other than Aurora), you have two options:
                  1- Manual horizontal scaling (by adding read replicas)
                  2- Manual vertical scaling (by upgrading/downgrading an existing instance).
    // AURORA is AWS OWNED --> operates on clusters rather than a single instance.

// DB Instance - Horiz/Verti scaling - DB identifier (DNS endpoint) -- Read replicas -- Multi AZ
// Table - item - attributes - Dynamo Db Streams -- DynamoDB Accelerator (DynamoDB DAX) -- DynamoDB point-in-time recovery (PITR)

3. DYNAMO DB : Fast and flexible NoSQL database service for all applications that need consistent, single-digit millisecond latency at any scale.
               With POINT IN TIME RECOVERY  feature, you can restore a table to any point in time during the last 35 days. Point-in-time recovery helps protect your tables from accidental write or delete operations.
               A table is a collection of items, and each item is a collection of attributes. DynamoDB uses primary keys to uniquely identify each item in a table and secondary indexes to provide more querying flexibility.
               You can use DYNAMO DB STREAMS to capture data modification events in DynamoDB tables.
               TABLE --> ITEM --> ATTRIBUTES . //For example, an item in a People table contains attributes called PersonID, LastName, FirstName, and so on.

               DynamoDB Streams is an optional feature that captures data modification events in DynamoDB tables.
               The data about these events appear in the stream in near-real time, and in the order that the events occurred.
               Each event is represented by a stream record. Event can be an ADDITION - UPDATION - DELETION of an item.

4. AMAZON ELASTIC CACHE FOR REDIS & MemCached :  // Set up in-memory cache environments in cloud .
// Queries that involve joins, we have to pay every time we query so we can cache such data and pay once. --> Caching data and providing it to users quickly rather than getting it from Disks. // example for applications in mobile .
Amazon ElastiCache makes it easy to set up, manage, and scale distributed in-memory cache environments in the AWS Cloud.
It provides a high performance, resizable, and cost-effective in-memory cache, while removing complexity associated with deploying and managing a distributed cache environment.
ElastiCache works with both the Redis and Memcached engines.
The service improves the performance of web applications by allowing you to retrieve information from fast, managed, in-memory data stores, instead of relying entirely on slower disk-based databases.

The most frequently accessed data be stored in elasticache so that the application’s response time is optimal.
The primary purpose of an in-memory data store is to provide ultrafast (sub millisecond latency) and inexpensive access to copies of data.

**********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************
**********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************
:::: DEVELOPER TOOLS  ::::
**********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************
**********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************

1. CLOUD-9       : We get an IDE to work with online. The AWS Cloud9 IDE offers a rich code-editing experience with support for several programming languages and runtime debuggers, and a built-in terminal.
                   // Can connect code and EC2. Store your project's files locally on the instance or server --> Clone a remote code repository — such as a repo in AWS CodeCommit — into your environment --> Work with a combination of local and cloned files in the environment.
2. CLOUD-SHELL   : We get a shell to work with online similar to AWS CLI that you can use to manage AWS services. We need not download a separate CLI.

3. CODE-ARTIFACT : Service that makes it easy for organizations to securely store, publish and share software packages used for application development. Artifacts are produced by some actions and consumed by others.
4. CODE-COMMIT   : Similar to GITHUB and Versioning in S3. It is a version control service that enables you to privately store and manage Git repositories in the AWS Cloud.
                   You can use AWS Cloud9 to make code changes in a CodeCommit repository. CodeCommit is optimized for team software development. It manages batches of changes across multiple files, which can occur in parallel with changes made by other developers.
5. CODE-STAR     : It lets you quickly develop, build, and deploy applications on AWS.
                   Depending on your choice of AWS CodeStar project template, that toolchain might include source control, build, deployment, virtual servers or serverless resources, and more.
                   Go to AWS -> Code Star -> Select a Template -> Add team members -> create a repo for code -> Create a pipeline -> Deploy Project . // wiki tile or a issue-tracking software  can be added for recommendations.

6. X-RAY         : Makes it easy for developers to analyze the behaviour of their distributed applications (Micro Services) by providing request tracing, exception collection, and profiling capabilities.
                   Is a service that collects data about requests that your application serves, and provides tools you can use to view, filter, and gain insights into that data to identify issues and opportunities for optimization.

7. CODE-BUILD    : Compiles your source code, runs unit tests, and produces artifacts that are ready to deploy. It provides pre packaged build environments for popular programming languages and build tools such as Apache Maven, Gradle, and more.
                   BUILD PROJECT = Instructions on how to run Build + Source code repo + Build env. + Build commands + Output repo
                   Source Code + BUILD PROJECT [ BUILD SPEC -> OS + Language + Run time + Build env + Build Tools + where to put output ] -> CODE BUILD creates a BUILD ENVIRONMENT  -> Send output to S3 ->  Send SNS if specified .
8. CODE-DEPLOY   : A deployment service that automates application deployments to Amazon EC2 instances, on-premises instances, serverless Lambda functions, or Amazon ECS services.
                   CodeDeploy can deploy application content that runs on a server and is stored in Amazon S3 buckets, GitHub repositories, or Bitbucket repositories. It can deploy in one or 1000 instances.
                   Deployable content [ from CODE BUILD ] + APPSPEC File -> upload to S3 or GITHUB -> Tell CODE DEPLOY [ s3 url / guthub url && No; of instances ] -> CODE DEPLOY Agent in each instance pulls the revision .
9. CODE-PIPELINE : A continuous delivery [ CI-CD ] service that enables you to model, visualize, and automate [ automates the building, testing, and deployment of your software into production ] the steps required to release your software.
                   CodePipeline can deploy applications to EC2 instances by using CodeDeploy, AWS Elastic Beanstalk, or AWS OpsWorks Stacks. CodePipeline can also deploy container-based applications to services by using Amazon ECS.
                   You can trigger an execution when you change your source code (like in code commit) or manually start the pipeline. You can also trigger an execution through an Amazon CloudWatch Events rule that you schedule.
                   [[ SOURCE STAGE ]] -> Code change/cloud watch alarm --> Code Build --> Virtual env (EC2 / ECS) --> Unit Tests --> Code Deploy --> Integration tests --> [[ PROD STAGE ]]
                   TO STOP A PIPELINE : Stop & Wait OR Stop & Abandon .

**********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************
**********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************
:::: MIGRATION & TRANSFER  ::::
**********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************
**********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************

1. APPLICATION DISCOVERY SERVICE :  Deploy Discovery Agent on resources  - To Migrate to cloud // Integrated with MIGRATION HUB //
                   It helps you plan your migration to the AWS cloud quickly and reliably plan application migration projects by automatically identifying applications running in on-premises data centres, their associated dependencies, and their performance profile. Integrates with application discovery solutions from AWS Partner Network(APN) partners.
                   The AWS Discovery Agent is AWS software that you install on on-premises servers and VMs targeted for discovery and migration. Agents capture system configuration, system performance, running processes, and details of the network connections between systems.
                   After registration, it pings the service at 15-minute intervals for configuration information.

2. MIGRATION-HUB : Provides a single location to track [ Database / Server ] migration tasks across multiple AWS tools and partner solutions. With Migration Hub, you can choose the AWS and partner migration tools that best fit your needs while providing visibility into the status of your migration projects.
                   Also provides key metrics and progress information for individual applications, regardless of which tools are used to migrate them.

3. SERVER MIGRATION SERVICE : Migrate from On-Premises or Microsoft or Azure Servers to AWS. AWS SMS incrementally replicates your server VMs as cloud-hosted Amazon Machine Images (AMIs) ready for deployment on Amazon EC2.
                   The Server Migration Connector is a FreeBSD VM that you install in your on-premises virtualization environment. We need to install this on the on premises / virtual VM. Now open the Server Migration Service on AWS and do the transfer.

4. DATABASE MIGRATION SERVICE - DMS : Service to migrate relational databases, data warehouses, NoSQL databases, and other types of data store bidirectionally from on-premises to cloud.
                   you can perform one-time migrations, and you can replicate ongoing changes to keep sources and targets in sync. If you want to change database engines, you can use the AWS Schema Conversion Tool (AWS SCT) to translate your database schema to the new platform.
                   And then use AWS DMS to migrate to cloud . Any data that can not be converted is marked for review later . The only requirement to use AWS DMS is that one of your endpoints must be on an AWS service. You can't use AWS DMS to migrate from an on-premises database to another on-premises database.
                   A task can consist of three major phases:   --> • The full load of existing data   --> • The application of cached changes   --> • Ongoing replication
                   While the replication is going on , all the transactions are cached and the cached data is migrated at the end to not disturb the flow of business or miss any data transaction.
// DMS is for DATA BASE migration -- // DATA SYNC is for DATA migration

5. AWS SCHEMA CONVERSION TOOL : // USED BY AWS-DMS. Makes heterogeneous database migrations easy by automatically converting the source database schema and a majority of the custom code to a format compatible with the target database.
                   Any code that the tool cannot convert automatically is clearly marked so that you can convert it yourself. Your converted schema is suitable for an Amazon Relational Database Service (Amazon RDS) MySQL, MariaDB, Oracle, SQL Server, PostgreSQL DB, an Amazon Aurora DB cluster, or an Amazon Redshift cluster.
                   The converted schema can also be used with a database on an Amazon EC2 instance or stored as data on an Amazon S3 bucket.

6. AWS DATA-SYNC : Online Transfer of data. It is an online data transfer service that simplifies, automates, and accelerates moving data between on-premises storage systems and AWS storage services, and also among  AWS storage services themselves .
                   DataSync can copy data between Network File System (NFS), Server Message Block (SMB) file servers, self-managed object storage, AWS Snow cone, Amazon Simple Storage Service (Amazon S3)buckets, Amazon EFS file systems, and Amazon FSx for Windows File Server file systems.

                     ON_PREMISIS                                                          AWS CLOUD
                                                              TLS
    Shared File system <----> AWS Data Sync agent  <========================> AWS Data Sync <----> AWS S3 / EFS / FSx


7. SNOW FAMILY         : Snow cone      -- 8 TB storage   --> 1 SMALL BOX [ uses opshub application for data transfer ]
                         Snow Ball      -- 80 TB storage  --> 1 SUITCASE  [ Snowball is a petabyte-scale data transport solution by cascading snow balls ]
                         Snow Ball edge -- 100 TB storage --> 1 SUITCASE  [ Embedded Ec2 - Lambda for running jobs in remote areas like ships , hills for an immediate results. ]
                         Snow Mobile    -- 1 Exabyte      --> 1 Truck     [ 100Pb per snow mobile --  AWS Snowmobile is the exabyte-scale data migration service ]

8. AWS TRANSFER FAMILY : // To set up a sftp server for customers. AWS Transfer Family is a secure transfer service that stores your data in Amazon S3 and simplifies the migration of the following workflows to AWS:
                         --> Secure Shell File Transfer Protocol (SFTP),   --> File Transfer Protocol Secure (FTPS), and   --> File Transfer Protocol (FTP) .
                         It is a secure transfer service that enables you to transfer files into and out of AWS storage services. Supports transferring data from the following :
                                  --> Amazon Simple Storage Service (Amazon S3) storage.
                                  --> Amazon Elastic File System (Amazon EFS) Network File System (NFS) file system

**********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************
**********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************
:::: FRONT END WEB & MOBILE  ::::
**********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************
**********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************

1. AMPLIFY  : Set of SDKs, libraries, tools, and documentation to develop mobile & Web-apps. Provides two main services, hosting and the Admin UI.
              Provides a git-based workflow for hosting fullstack serverless web apps with continuous deployment. The Admin UI is a visual interface for frontend web and mobile developers to create and manage app backends outside the AWS console.

2. APP-SYNC : GraphQL is a query language and server-side runtime for application programming interfaces (APIs) that prioritizes giving clients exactly the data they request and no more.
              As an alternative to REST, GraphQL lets developers construct requests that pull data from multiple data sources in a single API call. App-Sync is an Enterprise level, fully managed GraphQL service with real-time data synchronization and offline programming features.

3. AWS DEVICE FARM  : Performing iOS, Android and Fire OS apps on real mobiles -- Live customer support using remote connection --> Remote access of devices onto which you can load, run, and interact with apps in real time. -> Remotely work on someone's phone
                      Because testing is performed in parallel, tests on multiple devices begin in minutes. As tests are completed, a test report that contains high-level results, low-level logs, pixel-to-pixel screenshots, and performance data is updated.
                      Remote access allows you to swipe, gesture, and interact with a device through your web browser in real time. Install apps for them and explain them on how to use apps and solve any issues they are facing.

4. LOCATION SERVICE : Add maps, points of interest, geocoding, geofences, and tracking to your applications. AWS uses high-quality data from global, trusted providers Esri and HERE.
                      Tracking data and geofencing location information, such as facility, asset, and personnel locations, never leaves your AWS account at all. With Amazon Location, neither Amazon nor third parties have rights to sell your data or use it for advertising.


5. AMAZON PINPOINT : Send promotions, transactional (Order confirmations and password resets) and campaign messages via  email, SMS and voice messages, and push notifications to customers and get campaign statistics.
                     Add customer contact information and then create segments that target certain customers. Next, you have to create your messages and schedule your campaigns. Finally, you can use the analytics dashboards see how well the campaigns performed.
6. AMAZON SIMPLE NOTIFICATION SERVICE - SNS : It is a web service that enables applications, end-users, and devices to instantly send and receive notifications from the cloud. We create topics and subscribe to the topic for messages .
                     Clients can subscribe to the SNS topic and receive published messages using a supported protocol, such as Amazon Kinesis Data Firehose, Amazon SQS, AWS Lambda, HTTP, email, mobile push notifications, and mobile text messages (SMS).

    • Application-to-application messaging  -- HTTP/S - Lambda - SQS
    • Application-to-person notifications   -- SMS - Notifications - SES
    • Standard and FIFO topics
    • Message delivery retry               • Message attributes -- we can provide arbitrary metadata .
    • Message security                     • Message filtering based on policies
    • Dead-letter queues -- an Amazon SQS queue for messages that can't be delivered successfully due to client errors or server error.

SNS : IS A TOPIC Service.  // publishers to subscribers OR Producers to Consumers
SQS : IS A QUEUE Service.
      Unlimited Throughput , minimum one delivery , Best effort ordering - STANDARD QUEUE
      High Throughput , Exactly one delivery , First in First OUT - FIFO QUEUES


**********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************
**********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************
:::: END USER COMPUTING  ::::
**********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************
**********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************
// The following are used if you have a corporate N/W or if you are an enterprise and you want to provide services to your end users. All the services are monitored by an admin and he provides / revokes accesses to users .
   Mainly used by employees working remotely. This also makes sure that the main organizational information is not stored on users local machine.

// software developers in an organization can use Amazon WorkSpaces to access all desktop resources from any computer or tablet.
   Engineers can use AppStream 2.0 to stream GPU intensive apps.
   And sales leaders can use Amazon WorkLink to access internal web-based content, such as sales data, from their mobile devices.

1. WORKSPACES : Enables you to provision virtual, cloud-based Microsoft Windows or Amazon Linux desktops for your users, known as WorkSpaces. You can quickly add or remove users as your needs change. Users can access their virtual desktops from multiple devices or web browsers.
                As an admin you have the rights to remove add users and assign applications to users using WAM. As a user you pick up from right where you left off with a persistent desktop experience.
                you can bring your own licenses and applications, or purchase them from the AWS Marketplace for Desktop Apps. From any Operating system , use any other operating system

2. WORKSPACES APPLICATION MANAGER - WAM : Offers a fast, flexible, and secure way for you to deploy, manage, update, patching, and retire Microsoft Windows desktop applications into virtual containers that run as though they are installed natively for Amazon WorkSpaces  with Windows.
                VERSION-1 : With Amazon WAM Lite, you can manage and deliver applications from the AWS Marketplace free of charge. You pay only for the applications that your users activate.
                VERSION-2 : With Amazon WAM Standard, you can build your application catalog with line-of-business applications, third-party applications for which you own licenses, and applications from the AWS Marketplace for Desktop Apps.

3. APP-STREAM 2.0 : Add your existing desktop applications to AWS and enable your users to instantly stream them - Always-on & On-Demand .
                A fully managed application streaming service that provides users with instant access to their desktop applications from anywhere. Your applications run on AWS compute resources, and data is never stored on users' devices, which means they always get a high performance, secure experience.
                • Always-On — Your instances run all the time, even when no users are streaming applications. Use an Always-On fleet to provide your users with instant access to their applications.
                • On-Demand — Your instances run only when users are streaming applications. Idle instances that are available for streaming are in a stopped state. Use an On-Demand fleet to optimize your streaming charges and provide your users with access to their applications after a 1-2 minute wait.

4. WORK-DOCS :  --> STORE - SYNC - SHARE FILES from anywhere to anywhere. --> Directly edit docs online. Users can share their files with other members of your organization for collaboration or review.
                Files are stored in the cloud, safely and securely. Your user's files are only visible to them, and their designated contributors and viewers. Other members of your organization do not have access to other user's files unless they are specifically granted access.

5. WORK-LINK : Similar to UX-Apps TCS . Used to give secure access to internal websites and web apps through mobile phones , since using a VPN is not a feasible solution. Users need to install the work-link application .
               In a single step, your users, such as employees, can access internal websites as efficiently as they access any other public website. In addition, all cached content is deleted from AWS when users end their browsing session.
               users enter a URL in their web browser, or choose a link to an internal website in an email. Because website data is never stored or cached locally on mobile browsers, Amazon WorkLink reduces the risk of information loss or theft.
               After initial setup, the Amazon WorkLink app works in the background while employees browse internal websites using Safari on iOS phones and Google Chrome on Android phones.


**********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************
**********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************
:::: APPLICATION INTEGRATION  ::::
**********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************
**********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************
1. APP-FLOW       : Bidirectional data transfer from SAAS Applications ( Slack or Sales Force ) to AWS ( S3 or Redshift ) for analytics and archiving and automatic workflow.

2. EVENT-BRIDGE   : // Formerly CloudWatch Events -- Bridge b/w your apps - SAAS - AWS apps.
                    serverless event bus service that makes it easy to connect your applications with data from a variety of sources to targets such as AWS Lambda.
                    You can set up routing rules to determine where to send your data to build application architectures that react in real time to all of your data sources.

3. AMAZON MANAGED MESSAGE BROKER - MQ : A message broker is an intermediary computer program module that translates a message from the formal messaging protocol of the sender to the formal messaging protocol of the receiver.
                    Amazon MQ is a managed message broker service that makes it easy to set up and operate message brokers in the cloud.

4. SIMPLE NOTIFICATION SERVICE - SNS  : It is a web service that enables applications, end-users, and devices to instantly send and receive notifications from the cloud. We create topics and subscribe to the topic for messages .
                     Clients can subscribe to the SNS topic and receive published messages using a supported protocol, such as Amazon Kinesis Data Firehose, Amazon SQS, AWS Lambda, HTTP, email, mobile push notifications, and mobile text messages (SMS).

    • Application-to-application messaging  -- HTTP/S - Lambda - SQS
    • Application-to-person notifications   -- SMS - Notifications - SES
    • Standard and FIFO topics
    • Message delivery retry               • Message attributes -- we can provide arbitrary metadata .
    • Message security                     • Message filtering based on policies
    • Dead-letter queues -- an Amazon SQS queue for messages that can't be delivered successfully due to client errors or server error.

5. AMAZON SQS     : Fully managed message queuing service that makes it easy to decouple and scale microservices, distributed systems, and serverless applications.
                    You can send, store, and receive messages between software components at any volume, without losing messages or requiring other services to be available.
                    Multiple copies of every message are stored redundantly across multiple availability zones so that they are available whenever needed.

6. STEP FUNCTIONS : Makes it easy to coordinate the components of distributed applications as a series of steps in a visual workflow. Service that lets you combine AWS Lambda functions and other AWS services to build business-critical applications.
                    Through Step Functions' graphical console, you see your application’s workflow as a series of event-driven steps.
                    Step Functions is based on state machines and tasks. A state machine is a workflow. A task is a state in a workflow that represents a single unit of work that another AWS service performs. Each step in aworkflow is a state.
                    With Step Functions' built-in controls, you examine the state of each step in your workflow to make sure that your application runs in order and as expected.
                    CHECK 6 USECASES AND PICTURES .

7. SIMPLE WORK FLOW - SWF : Makes it easy to build applications that coordinate work across distributed components. A Task represents a logical unit of work that is performed by a component of your application.
                    Coordinating tasks across the application involves managing inter task dependencies, scheduling, and concurrency in accordance with the logical flow of the application.
                    SWF gives you full control over implementing tasks and coordinating them without worrying about underlying complexities such as tracking their progress and maintaining their state.
                    When using Amazon SWF, you implement workers to perform tasks. These workers can run either on cloud infrastructure, such as Amazon Elastic Compute Cloud (Amazon EC2), or on your own premises.
                    Amazon SWF stores tasks and assigns them to workers when they are ready, tracks their progress, and maintains their state, including details on their completion.
                    To coordinate tasks, you write a program that gets the latest state of each task from Amazon SWF and uses it to initiate subsequent tasks.
                    Amazon SWF maintains an application's execution state durably so that the application is resilient to failures in individual components.
                    With Amazon SWF, you can implement, deploy, scale, and modify these application components independently.

**********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************
**********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************
:::: BUISENES APPLICATION  ::::
**********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************
**********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************
1. ALEXA FOR BUSINESS : Similar to your Alexa in house , you can set up 100's of alexa in an organization and sync data between users.
                        schedule  meetings , check available rooms for booking , join meetings , get sales reports for your specific company .
2. CHIME              : Similar MS Teams (Download Amazon Chime APP). Communications service that transforms meetings by making them more efficient and easier to conduct.
                        You can use Amazon Chime for online meetings, video conferencing, calls, and chat. You can also share content inside and outside of your organization.
                        You pay only for the users with Pro permissions that host meetings, and only on the days that those meetings are hosted. There is no charge for users with Basic permissions. Basic users cannot host meetings, but they can attend meetings and use chat.
3. HONEY-CODE         : Fully managed service that allows you to quickly build mobile and web apps for teams—without programming. Build a custom managed app using a visual editor . First setup data in tables , tables work like spread sheets but have database capabilities.
                        Next build the apps layout . Now link app and the tables, set personalization for specific users to see what they can see or not see. Set specific actions to be done on an event such as when data is updated .
4. WORK-MAIL          : Business email and calendar service. Maintaining personal email service is tough !! -> Buy dedicated resources [ h/w and s/w ] -> hire staff to run the resources -> regular maintenance -> upgrades, security patches, backups , disaster recovery .
                        support for existing desktop and mobile clients. Can access mail and calendar from multiple devices mobile - tablet - desktop - all OS - windows - IOS and has compatibility with OUTLOOK and other popular services .
                        Amazon WorkMail has a web-based client that you use to access your Amazon WorkMail account from a web browser.


**********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************
**********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************
:::: CUSTOMER ENABLEMENT SERVICES  ::::
**********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************
**********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************
1. AWS MANAGED SERVICES      : 3Rd party partners providing full life cycle solutions for ENTERPRISE Customers . An enterprise service that provides ongoing management of your AWS infrastructure operations.
                Provides full-lifecycle services to provision, run, and support your infrastructure, and automates common activities such as change requests, monitoring, patch management, security, and backup services.
                Enterprises want to adopt AWS at scale but often the skills that have served them well in traditional IT do not always translate to success in the cloud. AMS enables them to migrate to AWS at scale more quickly, reduce their operating costs, improve security and compliance and focus on their differentiating business priorities.

2. AWS PROFESSIONAL SERVICES : A global team of experts that can help you realize your desired business outcomes when using the AWS Cloud.
                We work together with your team and your chosen member of the AWS Partner Network (APN) to execute your enterprise cloud computing initiatives.

3. AWS IQ     : Submit request -> review responses -> select expert -> work securely -> Pay directly from AWS billing. Enables customers to quickly find, engage, and pay AWS Certified third-party experts for on-demand project work.
                AWS IQ enables AWS Certified experts to help customers and get paid for their AWS Certification expertise. AWS IQ is a good choice when you're stuck on one or more tasks and need someone to do the work for you.

4. AWS SUPPORT :  There are multiple SUPPORT PLANS available. All AWS customers automatically have 24/7 access to features of the Basic support plan.
                  --> Basic         - Included
                  --> Developer     - Starting at $29 per month       - https://aws.amazon.com/premiumsupport/plans/
                  --> Business      - Starting at $100 per month      - https://aws.amazon.com/premiumsupport/plans/
                  --> Enterprise    - Starting at $15,000 per month   - INFRASTRUCTURE EVENT MANAGEMENT - MIGRATION ACCELERATION PROGRAM - TECHNICAL ACCOUNT MANAGER -  TOTAL COST OF OWNERSHIP
                                      TAM - MAP - IEM - TCO

4. AWS TRAINING & CERTIFICATION : TRAINING : https://aws.amazon.com/training/?id=docs_gateway
                                  DOCS     : https://docs.aws.amazon.com/

**********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************
**********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************
:::: CUSTOMER ENGAGEMENT  ::::
**********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************
**********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************
1. AMAZON CONNECT : Set up a Customer care centre. Add agents who are located anywhere, and start engaging with your customers.
                    Voice and Chat channels and support tickets tracking . All support channel data is logged in S3. It uses Machine Learning (ML) and Artificial Intelligence (AI) for analytics .

2. AMAZON PINPOINT : Send promotions, transactional (Order confirmations and password resets) and campaign messages via  email, SMS and voice messages, and push notifications to customers and get campaign statistics.
                     Add customer contact information and then create segments that target certain customers. Next, you have to create your messages and schedule your campaigns. Finally, you can use the analytics dashboards see how well the campaigns performed.

3. AMAZON SIMPLE EMAIL SERVICE - SES : Add email-sending capabilities to any application. If your application runs in EC2, you can use Amazon SES to send 62,000 emails every month at no additional charge.
                    Use AWS Elastic Beanstalk to create an email-enabled application such as a program that uses Amazon SES to send a newsletter to customers.  Set up SNS to notify you of your emails that bounced, produced a complaint, or were successfully delivered to the recipient's mail server.
                    When you use Amazon SES to receive emails, your email content can be published to Amazon SNS topics.

**********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************
**********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************
:::: BILLING & COST MANAGEMENT  ::::
**********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************
**********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************
1. BILLING & COST MANAGEMENT : Service that you use to pay your AWS bill, monitor your usage, and analyze and control your costs.
   AWS COST & USAGE REPORT   : one-stop shop for accessing the most detailed information available about your AWS costs and usage. Lists AWS usage for each service category used by an account and its IAM users in hourly or daily line items, as well as any tags that you have activated for cost allocation purposes.
   AWS CONSOLIDATED BILLING  : Enables an organization to consolidate payments for multiple Amazon Web Services (AWS) accounts within a single organization by making a single paying account.
   AWS COST EXPLORER         : Is a free tool that you can use to view your costs and usage up to the last 13 months, forecast how much you are likely to spend for the next twelve months (used to be three months), and get recommendations for what Reserved Instances to purchase..
   AWS BUDGETS               : Gives you the ability to set custom budgets that alert you when your costs or usage exceed (or are forecasted to exceed) your budgeted amount.
                                   Cost budgets – Plan how much you want to spend on a service.
                                   Usage budgets – Plan how much you want to use one or more services.
                                   RI utilization budgets – Define a utilization threshold and receive alerts when your RI usage falls below that threshold.
   ORDERS & INVOICES         : To see your past payments and transactions.
   KNOWLEDGE CENTRE          : You can find answers to your questions quickly by visiting the AWS Knowledge Center.
   COST ANAMOLY DETECTION    : Analyze and determine the root cause of the anomaly, such as account, service, Region, or usage type that is driving the cost increase.

2. AWS PRICING CALCULATOR    : Lets you explore AWS services and create an estimate for the cost of your use cases on AWS. You can model your solutions before building them, explore the price points and calculations behind your estimate, and find the available instance types and contract terms that meet your needs.

3. AWS SAVINGS PLAN          : Flexible pricing model that helps you save up to 72 percent on Amazon EC2, AWS Fargate, and AWS Lambda usage. Provides you lower prices in exchange for a commitment to a consistent usage amount (measured in $/hour)for a one or three year term.
                               Every type of compute usage has an On-Demand rate and a Savings Plans rate. If you commit to $10/hour of compute usage, your usage is charged at your Savings Plans rate up to $10. Any usage beyond your Savings Plans commitment is charged at your regular On-Demand rates.


**********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************
**********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************
:::: TOTAL SERVICES :::: // Total = 27 <--> Left-Out = 3 <--> Remaining = 24 <--> [ 4 Important + 14 Regular + 6 Singles ]
**********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************
**********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************
1. ANALYTICS  [ 13  -->  3 + 3 + 3 + 4 ]
   APP-FLOW         DATA-EXCHANGE         DATA-PIPELINE
   GLUE             LAKE-FORMATION        QUCIK-SIGHT
   CLOUD-SEARCH     ELASTIC-SEARCH        MANAGED-STREAMING-FOR-KAFKA
   ATHENA           KINESIS               EMR -- ELASTIC-MAP-REDUCE  / EMR-SPECTRUM       REDSHIFT

2. CONTAINERS = 5
   //  DOCKER FILE --> docker build ---> DOCKER IMAGE -- docker run ---> DOCKER CONTAINER.
   ECR -- ELASTIC-CONTAINER-REGISTERY    ECS -- ELASTIC-CONTAINER-SERVICES      EKS -- ELASTIC-KUBERNETIS-SERVICE     FARGATE     APP2CONTAINER

3. STORAGE  [ 7 --> 3 + 4 ]
   SIMPLE-STORAGE-GATEWAT       EBS -- ELASTIC-BLOCK-STORE                    STORAGE-GATEWAY
   SNOW-FAMILY                  FSx -- MICROSOFT-FLIGHT-SIMULATORE x          EFS -- ELASTIC-FILE-SYSTEM       BACKUP

4. CRYPTOGRAPHY AND PKI - Public Key Infrastructure SERVICES : [ 10 --> 2 + 2 + 2 + 4 ]
   // Symmetric [ Share Key - Advanced Encryption Standard (AES) & Triple DES (3DES) ] -  Asymmetric [ Public key - Rivest Shamir Adleman (RSA) & Elliptic Curve Cryptography (ECC) ] - Hashing [ Secure hash Algorithm (SHA) ]
   HSM -- HARDWARE-SECURITY-MODELS     KMS -- KEY-MANAGEMENT-SERVICE
   DYNAMO-DB-ENCRYPTION-CLIENT         S3-CLIENT-SIDE-ENCRYPTION
   ACM -- AWS-CERTIFICATE-MANAGER      ACM-PCA --  ACM-PRIVATE-CERTIFICATE-AUTHORITY
   ENCRYPTION-SDK                      SECRETS-MANAGER                                       CRYPTO-TOOLS                   SIGNER

5. DATABASE [ 10 --> 5 + 5 ]
   // ACID -- BASE
   RDS -- RELATIONAL-DATABASE-SERVICES   AURORA    REDSHIFT    DYNAMO-DB     ELASTICAHE-FOR-(MEMCAHCED/REDIS)
   DOCUMENT-DB                           NEPTUNE   KEYSPACES   TIME-STREAM   QLDB -- QUANTUM-LEDGER-DATABASE

6. DEVELOPER TOOLS   [ 10 --> 2 + 3 + 3 + 1 ]
   CLOUD-9         CLOUD-SHELL
   CODE-ARTIFACT   CODE-COMMIT   CODE-STAR
   CODE-BUILD      CODE-DEPLOY   CODE-PIPELINE
   X-RAY

7. MIGRATION & TRANSFER  [ 8 --> 6 + 2 ]
   APPLICATION-DISCOVERY-SERVICE    MIGRATION-HUB     SERVER-MIGRATION-SERVICE    DATABASE-MIGRATION-SERVICE     SCHEMA-CONVERSION-TOOL     DATA-SYNC
   SNOW-FAMILY                      TRANSFER-FAMILY

8. FRONT END WEB & MOBILE [ 6 --> 4 + 2 ]
   AMPLIFY   APP-SYNC   DEVICE-FARM    LOCATION-SERVICES
   PINPOINT  SNS -- SIMPLE-NOTIFICATION-SERVICES

9. END USER COMPUTING
   WORKSPACES    WAM -- WORKSPACES-APPLICATION-MANAGER     WORK-DOCS    WORK-LINK    APP-STREAM-2.0

10. APPLICATION INTEGRATION  [ 7 --> 4 + 3 ]
    APP-FLOW                         EVENT-BRIDGE                          STEP-FUNCTIONS                 SWF -- SIMPLE-WORK-FLOW
    MQ -- MANAGED-MESSAGE-BROKER     SNS -- SIMPLE-NOTIFICATION-SERVICES   SQS -- SIMPLE-QUEUE-SERVICE

11. BUISENES APPLICATION
    ALEXA-FOR-BUSINESS     CHIME     HONEY-CODE     WORK-MAIL

12. CUSTOMER ENABLEMENT SERVICES
    AWS-MANAGED-SERVICES    AWS-PROFESSIONAL-SERVICES     AWS-IQ      AWS-SUPPORT      TRAINING-&-CERTIFICATION
    https://aws.amazon.com/premiumsupport/plans/

13. CUSTOMER ENGAGEMENT
    AMAZON-CONNECT          AMAZON-PINPOINT       SES -- SIMPLE-EMAIL-SERVICE

14.  BILLING & COST MANAGEMENT
     BILLING & COST MANAGEMENT --> [ COST-&-USAGE-REPORT --  CONSOLIDATED-BILLING  -- COST-EXPLORER -- BUDGETS -- ORDERS-&-INVOICES -- KNOWLEDGE-CENTRE -- COST-ANAMOLY-DETECTION -- Simple Monthly Calculator  ]
     PRICING CALCULATOR           SAVINGS PLAN

// [[ BAGS-QR ]]
15. SATELLITE            -- AWS GROUND STATION
16. ROBOTICS             -- AWS ROBO MAKER
17. QUANTUM COMPUTING    -- AMAZON BRAKET
18. BLOCKCHAIN           -- AMAZON MANAGED BLOCK CHAIN
19. AR & VR              -- AMAZON SUMERIAN
20. GAME DEVELOPMENT     -- AMAZON GAME-LIFT  &&  AMAZON LUMBER-YARD


21. COMPUTE   [ 8 --> 6 + 2 ]
    ELASTIC-COMPUTE-CLOUD       ELASTIC-BEANSTALK        BATCH [parallel-Clusters HPC(1000 cpu's)]    EC2-IMAGE-BUILDER        LAMBDA          LIGHT-SAIL [launch-wizard]
    OUTPOSTS                    WAVELENGTHS
 // LAUNCH-WIZARD               PARALLEL-CLUSTERS        SAM -- SERVERLESS-APPLICATION-MODEL          SERVERLESS-APPLICATION-REPOSITORY
                                                         (Open-Source framework - cloud-Formation)

22. NETWORKING & CONTENT DELIVERY   [ 10 --> 6 + 4 ]
    CLOUD-FRONT                  DIRECT-CONNECT(Interfaces + Gw's + Dedicated/Hosted)      VIRTUAL-PRIVATE-CLOUD
    VIRTUAL-PRIVATE-NETWORK      ROUTE-53(DOmain + Routing + Health + 8 STEPS)             ELASTIC-LOAD-BALANCING( 3-Types & Cross Zone Load Balancing)
    API-GATEWAY                  APP-MESH                                                  CLOUD-MAP                                                              GLOBAL-ACCELERATOR

23. SECURITY - IDENTITY & COMPLIANCE  [ 19 --> 2 + 2 + 2 + 2 + 4 + 4 + 3 ]
    IDENTITY-ACCESS-MANAGEMENT (Identity [Managed[aws, customer], Inline] & Resource)    COGNITO (fb, insta // authentication -- user pools and  authorization -- identity pools)
    RESOURCE-GROUPS-&-TAG-EDITOR (group resources & perform bulk actions )               RAM --  RESOURCE-ACCESS-MANAGER ( share resources across principles such as AWS accounts, organizational units, or an entire organization from AWS Organizations.)
    ARTIFACT (Security, compliance reports and online agreements)                        AUDIT-MANAGER (audit your AWS usage)
    DIRETORY-SERVICE (To use Microsoft Active Directory - SSO)                           CLOUD-DIRECTORY (generate a conceptual relationship between the objects & Control access)

    WEB-APPLICATION-FIREWALL         SHIELD (Standard(free)/Advanced)         FIREWALL-MANAGER          NETWORK-FIREWALL
    AWS-DETECTIVE                    AWS-GUARD-DUTY                           AWS-INSPECTOR             AWS-MACIE
    SECRETS-MANAGER                  SECURITY-HUB                             SINGLE-SIGN-ON (Service provider <---- TRUST ----> Identity provider // SAML // 8 STEPS)

    // services AWS WAF - AWS SHIELD - AWS FIREWALL MANAGER - AWS NETWORK FIREWALL are interlinked. So which one to use ??
       --> It all starts with AWS WAF. You can automate and then simplify AWS WAF management using AWS Firewall Manager. // (DDOS, HTTP/S to cloud front, SQL injection, cross-site scripting)
           can be deployed on Amazon CloudFront, Application Load Balancer, Amazon API Gateway, AWS AppSync
           AWS WAF is a web application firewall that helps protect web applications from attacks by allowing you to configure rules that allow, block, or monitor (count) web requests based on conditions that you define. These conditions include IP addresses, HTTP headers, HTTP body, URI strings, SQL injection and cross-site scripting.
           AWS WAF is a web application firewall that lets you monitor the HTTP and HTTPS requests that are forwarded to CloudFront, and lets you control access to your content.
           When you use AWS WAF on Amazon CloudFront, your rules run in all AWS Edge Locations, located around the world close to your end-users. This means security doesn’t come at the expense of performance. Blocked requests are stopped before they reach your web servers.
       --> Shield offers free standard services and  Shield Advanced which adds additional features on top of AWS WAF, such as dedicated support from the DDoS Response Team (DRT) and advanced reporting, this requires a business or enterprise support plan.
       --> To use the services of the DRT, you must be subscribed to the Business Support plan or the Enterprise Support plan.
       --> If you want granular control over the protection that is added to your resources, AWS WAF alone is the right choice.
       --> If you want to use AWS WAF across accounts, accelerate your AWS WAF configuration, or automate protection of new resources, use Firewall Manager with AWS WAF.
           Network firewall manager manages all netwrok firewalls , vpc's , subnets etc; Network firewall manages a particular VPC . You can create one for each VPC .
       --> Finally, if you own high visibility websites or are otherwise prone to frequent DDoS attacks, you should consider purchasing the additional features that Shield Advanced provides.

   // All AWS customers benefit from the automatic protections of AWS Shield Standard, at no additional charge. AWS Shield Standard defends against most common, frequently occurring network and transport layer DDoS attacks that target your web site or applications.
      When you use AWS Shield Standard with Amazon CloudFront and Amazon Route 53, you receive comprehensive availability protection against all known infrastructure (Layer 3 and 4) attacks.
      For higher levels of protection against attacks targeting your applications running on Amazon Elastic Compute Cloud (EC2), Elastic Load Balancing (ELB), Amazon CloudFront, AWS Global Accelerator and Amazon Route 53 resources, you can subscribe to AWS Shield Advanced.
      In addition to the network and transport layer protections that come with Standard, AWS Shield Advanced provides additional detection and mitigation against large and sophisticated DDoS attacks, near real-time visibility into attacks, and integration with AWS WAF, a web application firewall.


       // Layer 7 : AWS WAF is a web application firewall that lets you monitor the HTTP and HTTPS requests that are forwarded to an Amazon API Gateway API, Amazon CloudFront or an Application Load Balancer.
                    HTTP and HTTPS requests are part of the Application layer, which is layer 7.
          Layer 3 - Layer 3 is the Network layer and this layer decides which physical path data will take when it moves on the network.
                    AWS Shield offers protection at this layer. WAF does not offer protection at this layer.
          Layer 4 - Layer 4 is the Transport layer and this layer data transmission occurs using TCP or UDP protocols.
                    AWS Shield offers protection at this layer. WAF does not offer protection at this layer.

    /// AWS DETECTIVE - AWS GUARD DUTY - AWS INSPECTOR - AWS MACIE are similar in concepts.
        --> AWS DETECTIVE is mainly for the security findings in login attempts , API calls from AWS CloudTrail and Amazon VPC flow logs. It also ingests findings detected by GuardDuty.
            Amazon Detective can analyze trillions of events from multiple data sources such as Virtual Private Cloud (VPC) Flow Logs, AWS CloudTrail, and Amazon GuardDuty, and automatically creates a unified, interactive view of your resources, users, and the interactions between them over time.
        --> AWS GUARD DUTY does DETETIVE duty plus reports compromised EC2 instances , compromised s3 instances and compromised AWS credentials from VPC Flow Logs, AWS CloudTrail management event logs, Cloudtrail S3 data event logs, and DNS logs.
            It also monitors AWS account access behaviour for signs of compromise, such as unauthorized infrastructure deployments, like instances deployed in a Region that has never been used, or unusual API calls, like a password policy change to reduce password strength.
            GuardDuty is a threat detection service that monitors malicious activity and unauthorized behaviour to protect your AWS account.
            GuardDuty analyzes billions of events across your AWS accounts from AWS CloudTrail (AWS user and API activity in your accounts), Amazon VPC Flow Logs (network traffic data), and DNS Logs (name query patterns).
            Security findings are retained and made available through the Amazon GuardDuty console and APIs for 90-days.
        --> AWS INSPECTOR does an vulnerability assessment based on common security standards and vulnerability definitions updated frequently by AWS .
            Offers predefined software called an agent that you can install in EC2. Examples of built-in rules include checking for remote root login being enabled, or vulnerable software versions installed.
        --> AWS MACIE is for protecting personal and financial data stored  in S3 across your organization and reports if a bucket is unencrypted .
        --> All the services are free for 30 days .Pay from the next month!! During the 30-day free trial period, the cost estimation projects what your estimated costs will be after the free trial period.


24. MANAGEMENT & GOVERNANCE :
    AWS BACKINT AGENT FOR SAP HANA    AMAZON MANAGED SERVICE FOR GRAFANA        MANAGED SERVICE FOR PROMETHEUS            TOOLS FOR POWER-SHELL     DATA LIFE CYCLE MANAGER      WELL-ARCHITECTURED TOOL
    COMMAND LINE INTERFACE            AWS MANAGEMENT CONSOLE                    CONSOLE MOBILE APPLICATION                COMPUTE OPTIMIZER


    CLOUD-FORMATION
    CLOUD-TRAIL              -- past 90 days of API activity in your AWS account, save trail to S3. // Only management events.
    CLOUD-WATCH              -- collect and track metrics, collect and monitor log files, set alarms, and automatically react to changes in your AWS resources. Create custom dashboards
    CONFIG                   -- To assess , audit and evaluate the configuration of your resources . And to make sure resources are compliant with industry standards. We get notified if a resource is non-compliant or matches a rule against compliance. This includes how the resources are related to one another and how they were configured in the past so that you can see how the configurations and relationships change over time.
    OPSWORKS                 -- configuration management service that helps you configure and operate applications in a cloud enterprise by using Puppet or Chef.
    TRUSTED ADVISOR          -- online tool that provides you real time guidance to help you provision your resources following AWS best practices.
    AWS HEALTH               -- PERSONALIZED VIEW OF SERVICE HEALTH -- PROACTIVE NOTIFICATIONS DETAILED -- TROUBLESHOOTING GUIDANCE // Personal Health Dashboard + Service Health Board. While the Service Health Dashboard displays the general status of AWS services, Personal Health Dashboard gives you a personalized view into the performance and availability of the AWS services underlying your AWS resources.
    AUTO-SCALING             -- enabled by cloud-watch -- auto scaling groups (min/max)

    AWS CHATBOT              -- Slack or Amazon Chime -- AWS Chatbot processes AWS service notifications from Amazon Simple Notification Service (Amazon SNS), and forwards them to Slack or Amazon Chime chat rooms so teams can analyze and act on them.
    SERVICE QUOTAS / LIMITS  -- Service Quotas is a service for viewing and managing your quotas easily and at scale as your AWS workloads grow.  Quotas, also referred to as limits, are the maximum number of resources that you can create in an AWS account.
    SERVICE CATALOG          -- enables IT administrators to create, manage, and distribute portfolios of approved products to end users, who can then access the products they need in a personalized portal.
    LICENSE MANAGER          -- Bring Your Own License - BYOL // Sends SNS if a license is expiring. streamlines the process of bringing software vendor licenses ( for example, Microsoft, SAP, Oracle, and IBM) to the cloud.

    AWS CONTROL TOWER        -- Organizations + SSO + Service Catalog // a service that enables you to enforce and manage governance rules for security, operations, and compliance at scale across all your organizations and accounts in the AWS Cloud.
                                provides the easiest way to set up and govern a secure, compliant, multi-account AWS environment based on best practices established by working with thousands of enterprises.
    AWS ORGANIZATIONS        -- Account management service that lets you consolidate multiple AWS accounts into an organization that you create and centrally manage.
                                Includes account management and consolidated billing capabilities that enable you to better meet the budgetary, security, and compliance needs of your business.
                                Basic organization that consists of "n" accounts can be  organized into "m" organizational units (OUs) under the root.
                                The organization also has several policies that are attached to some of the OUs or directly to accounts.

    AWS PROTON               -- As an administrator, you can create service templates to provide standardized infrastructure and deployment tooling for serverless and container-based applications.
                                Then developers on your team, in turn, can select from the available service templates to automate their application or service deployments.
                                platform teams can use AWS Proton to define and manage standard application stacks that contain the architecture, infrastructure resources, and the CI/CD software deployment pipeline.
    APP-CONFIG               -- Automation -- Capability of AWS Systems Manager, to create, manage, and quickly deploy application configurations, includes built-in validation checks and monitoring.

    AWS SYSTEMS MANAGER      -- Use AWS Systems Manager to organize, monitor, and automate management tasks on your AWS resources and also use to view and control your infrastructure on AWS.
                                AWS Systems Manager gives you visibility and control of your infrastructure on AWS.
                                Systems Manager provides a unified user interface so you can view operational data from multiple AWS services and enables you to automate operational tasks across your AWS resources.
                                After tagging the required resources in a region , you can view a consolidated dashboard in Systems Manager that reports the status of all the resources that are part of the region. If a problem arises with any of these resources, you can take corrective action immediately.
                                --> Operations Management, Application Management, Change Management, Node Management, and Shared Resources
                                Group AWS resources together by any purpose or activity you choose, such as application, environment, region, project, campaign, business unit, or software lifecycle.





##################################################################################################################
##################################################################################################################
ADDITIONAL RESOURCES  :
##################################################################################################################
##################################################################################################################

1. AWS MARKET-PLACE :  https://www.youtube.com/watch?v=RxCRHsAqiWs
---------------------------------------------------------------------
// Buy & Sell - Paid or Free AWS Software Solutions .
AWS Marketplace is an online store where you can buy or sell software that runs on Amazon Web Services (AWS).
AWS Marketplace is a curated digital catalog customers can use to find, buy, deploy, and manage third-party software, data, and services that customers need to build solutions and run their businesses.
AWS Marketplace includes thousands of software listings from popular categories such as security, networking, storage, machine learning, business intelligence, database, as well as related professional services to help you manage and support those solutions.
AWS Marketplace also simplifies software licensing and procurement with flexible pricing options and multiple deployment methods.
Customers can quickly launch pre-configured software with just a few clicks, and choose software solutions in AMI and SaaS formats, as well as other formats.
Flexible pricing options include free trial, hourly, monthly, annual, multi-year, and BYOL, and get billed from one source, AWS.
Partners use AWS Marketplace to be up and running in days and offer their software products to customers around the world.
The product can be free to use or it can have an associated charge. AWS handles billing and payments, and charges appear on customers’ AWS bill.

SELLER : seller can be an independent software vendor (ISV), consulting partner, managed services provider (MSP), or individual who has something to offer that works with AWS products and services.
One way of delivering your products to buyers is with Amazon Machine Images (AMIs) - Docker Images - SAAS Apps.
An AMI provides the information required to launch an Amazon Elastic Compute Cloud (Amazon EC2) instance.
You create a custom AMI for your product, and buyers can use it to create EC2 instances with your product already installed and ready to use.

The AWS Marketplace provides value to buyers in several ways:
    1- It simplifies software licensing and procurement with flexible pricing options and multiple deployment methods. Flexible pricing options include free trial, hourly, monthly, annual, multi-year, and BYOL.
    2- Customers can quickly launch pre-configured software with just a few clicks, and choose software solutions in AMI and SaaS formats, as well as other formats.
    3- It ensures that products are scanned periodically for known vulnerabilities, malware, default passwords, and other security-related concerns.

2. AWS GOVCLOUD - United States :
----------------------------------
The AWS GovCloud (US) Regions are isolated AWS Regions designed to enable U.S. government agencies and customers to move sensitive workloads into the cloud by addressing their specific regulatory and compliance requirements.
The AWS GovCloud (US) Regions adhere to U.S. International Traffic in Arms Regulations (ITAR) requirements.


3. AWS MANAGED SERVICES & CUSTOMER MANAGED SERVICES & SHARED CONTROLS :
-----------------------------------------------------------------------
AWS MANAGED :
-----------------
--> AWS Lambda, Amazon RDS, Amazon Redshift, Amazon CloudFront, Elastic Map Reduce, Dynamo DB
For managed services shown above, AWS is responsible for performing all the operations needed to keep the service running.
The AWS-managed services automate time-consuming administration tasks such as hardware provisioning, software setup, patching and backups. The AWS-managed services free customers to focus on their applications so they can give them the fast performance, high availability, security and compatibility they need.
Examples of AWS-managed services include Amazon RDS, Amazon DynamoDB, Amazon Redshift, Amazon WorkSpaces, Amazon CloudFront, Amazon CloudSearch, and several other services.
AWS is responsible for patching the underlying hosts, upgrading the firmware, and fixing flaws within the infrastructure for all services, including Amazon EC2.

Amazon EMR launches clusters in minutes.
You don’t need to worry about node provisioning, infrastructure setup, Hadoop configuration, or cluster tuning.
Amazon EMR takes care of these tasks so you can focus on analysis.
DynamoDB is serverless with no servers to provision, patch, or manage and no software to install, maintain, or operate.
DynamoDB automatically scales tables up and down to adjust for capacity and maintain performance.
Availability and fault tolerance are built in, eliminating the need to architect your applications for these capabilities.

CUSTOMER MANAGED :
------------------------
On the other hand, customer-managed services are services that are completely managed by the customer.
For example, a service such as Amazon Elastic Compute Cloud (Amazon EC2), Amazon VPC, and Amazon S3 are categorized as Infrastructure as a Service (IaaS) and, as such, requires the customer to perform all of the necessary security configuration and management tasks.
Customers that deploy an Amazon EC2 instance are responsible for the management of the guest operating system (including updates and security patches), any application software or utilities installed by the customer on the instances, and the configuration of the AWS-provided firewall (called a security group) on each instance.
The customer is responsible for securing their network by configuring Security Groups, Network Access control Lists (NACLs), and Routing Tables. The customer is also responsible for setting a password policy on their AWS account that specifies the complexity and mandatory rotation periods for their IAM users' passwords.
Examples of customer-managed services include Amazon Elastic Compute Cloud (Amazon EC2), Amazon Virtual Private Cloud (Amazon VPC), and AWS Identity And Access Management (AWS IAM).
Patching the guest operating system is the responsibility of AWS for the managed services only (such as Amazon RDS). The customer is responsible for patching the guest OS for other services (such as Amazon EC2).
Data encryption is the responsibility of the customer. Both SERVER SIDE & CLIENT SIDE encryption comes under client responsibility.
  NOTE :
  The AWS managed services we mentioned above are different than the AWS Managed Services (AMS) service.
  AMS is an AWS service that operates AWS on behalf of enterprise customers and partners.
  Enterprises want to adopt AWS at scale but often the skills that have served them well in traditional IT do not always translate to success in the cloud.
  AWS Managed Services (AMS) enables them to migrate to AWS at scale more quickly, reduce their operating costs, improve security and compliance and focus on their differentiating business priorities.

AWS is responsible for physical controls and environmental controls. Customers inherit these controls from AWS.
Inherited Controls are controls which a customer fully inherits from AWS such as physical controls and environmental controls.


4. AWS SHARED CONTROLS - AWS & CUSTOMER:
--------------------------------------------------
Responsibilities depends on the service used .
For example, when using Amazon EC2, you are responsible for applying operating system and application security patches regularly. However, such patches are applied automatically when using Amazon RDS.

Shared Controls are controls which apply to both the infrastructure layer and customer layers, but in completely separate contexts or perspectives.
In a shared control, AWS provides the requirements for the infrastructure and the customer must provide their own control implementation within their use of AWS services.
Examples include:
** Patch Management         – AWS is responsible for patching the underlying hosts and fixing flaws within the infrastructure, but customers are responsible for patching their guest OS and applications.
** Configuration Management – AWS maintains the configuration of its infrastructure devices, but a customer is responsible for configuring their own guest operating systems, databases, and applications.
** Awareness & Training     - AWS trains AWS employees, but a customer must train their own employees.

Patching the guest operating system is the responsibility of AWS for the managed services only (such as Amazon RDS).
The customer is responsible for patching the guest OS for other services (such as Amazon EC2).
AWS is responsible for patching the underlying hosts, upgrading the firmware, and fixing flaws within the infrastructure for all services, including Amazon EC2.

Both client side and server side encryption is a responsibility of customer.
AWS offers a lot of services and features that help AWS customers protect their data in the cloud.
Customers can protect their data by encrypting it in transit and at rest.
They can use Cloudtrail to log API and user activity, including who, what, and from where calls were made.
They can also use the AWS Identity and Access Management (IAM) to control who can access or edit their data.

5. AWS MICROSERVICES vs MONOLITHIC :
--------------------------------------
AWS recommends adopting microservices architecture, not monolithic architecture. With monolithic architectures, application components are tightly coupled and run as a single service.
With a microservices architecture, an application is built as loosely coupled components.
Benefits of microservices architecture include:
  1- Microservices allow each service to be independently scaled to meet demand for the application feature it support.
  2- Teams are empowered to work more independently and more quickly.
  3- Microservices enable continuous integration and continuous delivery, making it easy to try out new ideas and to roll back if something doesn’t work.
  4- Service independence increases an application’s resistance to failure.
     In a monolithic architecture, if a single component fails, it can cause the entire application to fail.
     With microservices, applications handle total service failure by degrading functionality and not crashing the entire application.

6.  AWS PENETRATION TESTING : https://aws.amazon.com/security/penetration-testing/
-------------------------------
Penetration testing is the practice of testing a network or web application to find security vulnerabilities that an attacker could exploit.
AWS customers are welcome to carry out security assessments and penetration tests against their AWS infrastructure without prior approval for 8 services:
So Penetration testing can be done by customers on their own instances without prior authorization from AWS.

      PERMITTED ACTIVITIES :                        PROHIBITED ACTIVITIES :
  ---------------------------------------------------------------------------------------------------------------------------------------------
      1- Amazon EC2 instances, NAT Gateways,        1. DNS zone walking via Amazon Route 53 Hosted Zones
         and Elastic Load Balancers.
      2- Amazon RDS.                                2. Denial of Service (DoS), Distributed Denial of Service (DDoS), Simulated DoS, Simulated DDoS
      3- Amazon CloudFront.                         3. Port flooding
      4- Amazon Aurora.                             4. Protocol flooding
      5- Amazon API Gateways.                       5. Request flooding  (login request flooding, API request flooding)
      6- AWS Lambda and Lambda Edge functions.
      7- Amazon Lightsail resources.
      8- Amazon Elastic Beanstalk environments.

The AWS customers are responsible for performing penetration tests against their AWS infrastructure.
AWS customers are allowed to perform penetration tests against their AWS infrastructure, but they must ensure that their activities are aligned with AWS policies.
AWS customers are allowed to perform penetration testing on both AWS-managed services such as Amazon RDS and customer-managed services such as Amazon EC2.

Note: Customers are not permitted to conduct any security assessments of AWS infrastructure, or the AWS services themselves.
      If you discover a security issue within any AWS services in the course of your security assessment, please contact AWS Security immediately.

7. HORIZONTAL & VERTICAL SCALING :
-------------------------------------
HORIZONTAL -> Adding more EC2 instances
Scaling horizontally takes place through an increase in the number of resources (e.g., adding more hard drives to a storage array or adding more servers to support an application).
This is a great way to build Internet-scale applications that leverage the elasticity of cloud computing.

VERTICAL -> Add specifications to a single resource.
Scaling vertically takes place through an increase in the specifications of an individual resource (e.g., upgrading a server with a larger hard drive, adding more memory, or provisioning a faster CPU).
On Amazon EC2, this can easily be achieved by stopping an instance and resizing it to an instance type that has more RAM, CPU, I/O, or networking capabilities. This way of scaling can eventually hit a limit and it is not always a cost efficient or highly available approach. However, it is very easy to implement and can be sufficient for many use cases especially as a short term solution.
A "vertically scalable" system is constrained to running its processes on only one computer; in such systems, the only way to increase performance is to add more resources into one computer in the form of faster (or more) CPUs, memory, or storage.
Vertical scaling may improve performance, but not fault-tolerance; because if this "one computer" fails, the whole system will fail.

Vertical-scaling is often limited to the capacity constraints of a single machine, scaling beyond that capacity often involves downtime and comes with an upper limit.
With horizontal-scaling it is often easier to scale dynamically by adding more machines in parallel. Hence, in most cases, horizontal-scaling is recommended over vertical-scaling.
Horizontally scalable systems are oftentimes able to outperform vertically scalable systems by enabling parallel execution of workloads and distributing those across many different computers.

8. AWS WELL ARCHITECTURED FRAMEWORK : https://d1.awsstatic.com/whitepapers/architecture/AWS_Well-Architected_Framework.pdf
---------------------------------------
The Well-Architected Framework identifies a set of general design principles to facilitate good design in the cloud:

  --> STOP GUESSING YOUR CAPACITY NEEDS :
      Eliminate guessing about your infrastructure capacity needs.
      When you make a capacity decision before you deploy a system, you might end up sitting on expensive idle resources or dealing with the performance implications of limited capacity.
      With cloud computing, these problems can go away.
      You can use as much or as little capacity as you need, and scale up and down automatically.

  --> TEST SYSTEMS AT PRODUCTION SCALE :
      In the cloud, you can create a production-scale test environment on demand, complete your testing, and then decommission the resources.
      Because you only pay for the test environment when it's running, you can simulate your live environment for a fraction of the cost of testing on premises.

  --> AUTOMATE TO MAKE YOUR ARCHIECTURAL EXPERIMENTATION EASIER :
      Automation allows you to create and replicate your systems at low cost and avoid the expense of manual effort.
      You can track changes to your automation, audit the impact, and revert to previous parameters when necessary.

  --> ALLOW FOR EVOLUTIONARY ARCHITECTURES :
      In a traditional environment, architectural decisions are often implemented as static, one-time events, with a few major versions of a system during its lifetime.
      As a business and its context continue to change, these initial decisions might hinder the system's ability to deliver changing business requirements.
      In the cloud, the capability to automate and test on demand lowers the risk of impact from design changes.
      This allows systems to evolve over time so that businesses can take advantage of innovations as a standard practice.

  --> DRIVE ARCHITECTURES USING DATA :
      In the cloud you can collect data on how your architectural choices affect the behaviour of your workload.
      This lets you make fact-based decisions on how to improve your workload.
      Your cloud infrastructure is code, so you can use that data to inform your architecture choices and improvements over time.

  --> IMPROVE THROUGH GAME DAYS :
      Test how your architecture and processes perform by regularly scheduling game days to simulate events in production.
      This will help you understand where improvements can be made and can help develop organizational experience in dealing with events.

The AWS Well-Architected Tool helps customers review the state of their workloads and compares them to the latest AWS architectural best practices.
The tool is based on the AWS Well-Architected Framework, developed to help cloud architects build secure, high-performing, resilient, and efficient application infrastructure.

################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
To ensure that the application has the highest level of availability :
-> Deploy across multiple regions and availability zones , not across availability zones & Subnets.
In addition to replicating applications and data across multiple data centers in the same Region using Availability Zones, you can also choose to increase redundancy and fault tolerance further by replicating data between geographic Regions (especially if you are serving customers from all over the world).
You can do so using both private, high speed networking and public internet connections to provide an additional layer of business continuity, or to provide low latency access across the globe.

####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
SECURITY is a shared responsibility.
Most (not all) data and network security are taken care of for you.
When we talk about the data/network security, AWS has a “shared responsibility model” where AWS and the customer share the responsibility of securing them.
For example, the customer is responsible for creating rules to secure their network traffic using the security groups and is also responsible for protecting data with encryption.

Gaining complete control over the physical infrastructure" is incorrect.
The Physical infrastructure is a responsibility of AWS, not the customer.

The principle of least privilege is one of the most important security practices and it means granting users the required permissions to perform the tasks entrusted to them and nothing more.
The security administrator determines what tasks users need to perform and then attaches the policies that allow them to perform only those tasks.
You should start with a minimum set of permissions and grant additional permissions when necessary. Doing so is more secure than starting with permissions that are too lenient and then trying to tighten them down.

####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
Elastic Load Balancers do not scale resources.
Elastic Load Balancers distributes incoming application traffic across multiple targets, such as Amazon EC2 instances, containers, IP addresses, and Lambda functions.

Scaling resources is a function of AWS AUTO SCALING.
AWS Auto Scaling is the feature that automates the process of adding/removing server capacity (based on demand).
Autoscaling allows you to reduce your costs by automatically turning off resources that aren’t in use.
On the other hand, Autoscaling ensures that your application runs effectively by provisioning more server capacity if required.
Auto Scaling helps you ensure that you have the correct number of Amazon EC2 instances available to handle the load for your application. You create collections of EC2 instances, called Auto Scaling groups.
You can specify the minimum number of instances in each Auto Scaling group, and Amazon EC2 Auto Scaling ensures that your group never goes below this size.

####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
What does AWS provide to deploy popular technologies - such as IBM MQ - on AWS with the least amount of effort and time?

AWS Quick Start Reference Deployments outline the architectures for popular enterprise solutions on AWS and provide AWS CloudFormation templates to automate their deployment.
Each Quick Start launches, configures, and runs the AWS compute, network, storage, and other services required to deploy a specific workload on AWS, using AWS best practices for security and availability.
Quick Starts are built by AWS solutions architects and partners to help you deploy popular technologies on AWS, based on AWS best practices.
These accelerators reduce hundreds of manual installation and configuration procedures into just a few steps, so you can build your production environment quickly and start using it immediately.
Each Quick Start includes AWS CloudFormation templates that automate the deployment and a guide that discusses the architecture and provides step-by-step deployment instructions.

AWS Quick Start Reference Deployments outline the architectures for popular enterprise solutions on AWS and provide AWS CloudFormation templates to automate their deployment.
Each Quick Start launches, configures, and runs the AWS compute, network, storage, and other services required to deploy a specific workload on AWS, using AWS best practices for security and availability.
Quick Starts are built by AWS solutions architects and partners to help you deploy popular technologies on AWS, based on AWS best practices.
These accelerators reduce hundreds of manual installation and configuration procedures into just a few steps, so you can build your production environment quickly and start using it immediately.


####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
Your customers complain that sometimes they can’t reach your application. Which AWS service allows you to monitor the performance of your EC2 instances to assist in troubleshooting these issues?
ANS : CLOUD WATCH
Amazon CloudWatch is a service that monitors AWS cloud resources and the applications you run on AWS.
You can use Amazon CloudWatch to collect and track metrics, collect and monitor log files, set alarms, and automatically react to changes in your AWS resources.
Amazon CloudWatch can monitor AWS resources such as Amazon EC2 instances, Amazon DynamoDB tables, and Amazon RDS DB instances, as well as custom metrics generated by your applications and services, and any log files your applications generate.
You can use CloudWatch to detect anomalous behavior in your environments, take automated actions, troubleshoot issues, and discover insights to keep your applications running smoothly.

AWS CloudTrail is an AWS service that can be used to monitor all user interactions with the AWS environment.

####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
IN MEMEMORY vs ELASTICACHE :
--------------------------------
The most frequently accessed data be stored in elasticache so that the application’s response time is optimal.
The primary purpose of an in-memory data store is to provide ultrafast (sub millisecond latency) and inexpensive access to copies of data.
Querying a database is always slower and more expensive than locating a copy of that data in a cache. Some database queries are especially expensive to perform.
An example is queries that involve joins across multiple tables or queries with intensive calculations. By caching (storing) such query results, you pay the price of the query only once.
Then you can quickly retrieve the data multiple times without having to re-execute the query.

#################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
--> NETWORK BASED SERVICES : Private Network Connectors to AWS            - AWS DIRECT CONNECT
                             Edge Locations for S3 Enabled applications   - AMAZON S3 TRANSFER ACCELERATION

--> ONLINE  DATA TRANSFER :  Load stream Data into S3                     - AMAZON KINESIS
                             Online transfer of active data               - AWS DATA SYNC
                             SFTP transfer to S3                          - AWS TRANSFER FOR SFTP
                             Database Transfer                            - AWS DATABSE MIGRATION
                             VM Image transfer - Live applications        - AWS SERVER MIGRATION SERVICES  // ENDURE

  // AWS ENDURE : CloudEndure Migration is a highly automated solution that simplifies the process of migrating applications from physical, virtual, and cloud-based infrastructure, ensuring that they are fully operational in any AWS Region without compatibility issues.

--> OFFLINE DATA TRANSFER :  Ship Static data in and out of AWS           - AWS SNOW FAMILY [ Snow cone / Snow Ball / Snow Mobile ]

--> HYBRID DATA STORAGE   :  Access AWS storage from On-premisis          - AWS STORAGE GATEWAY

####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################

To achieve High Availability, you need the ability to redirect traffic in the case of instance failure. There are several options:

1. USE ELASTIC LOAD BALANCER :
   This is the preferred way to provide High Availability.
   Run multiple Amazon EC2 instances, preferably in different Availability Zones (AZs).
   Users connect to the ELB (via the supplied DNS name), which redirects traffic to the EC2 instances.
   If an instance fails, ELB notices this via regular Health Checks, and will only direct traffic to the healthy instances.
   Auto Scaling can be used to create these multiple instances across multiple Availability Zones, and it can also update the Load Balancing service when it adds/removes instances.

2. REDIRECT AN ELASTIC IP ADDRESS :
   Run multiple instances (preferably across multiple Availability Zones).
   Point an Elastic IP address to the instance you desire. Users connect via the Elastic IP address and are directed to the instance.
   If the instance fails, re-associate the Elastic IP address to a different instance, which will then start receiving the traffic immediately.
   This method is not recommended because only one instance is receiving all the traffic while the other instance(s) are sitting idle.
   It also requires a mechanism to detect failure and re-associate the Elastic IP (which you must do yourself).

3. REASSIGN AN ELASTIC NETWORK INTERFACE :
   All EC2 instances have a primary ENI. They can optionally have additional ENIs.
   It is possible to direct traffic to a secondary ENI and then move that secondary ENI to another instance. This is similar to reassigning an Elastic IP address.
   This method is not recommended for the same reason as re-associating an Elastic IP address (above), but also because an ENI can only be reassigned within the same AZ. It cannot be used to direct traffic to an EC2 instance in a different AZ.

Bottom line: Use an Elastic Load Balancer. It provides true High Availability and can do it automatically.

####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
CONFIG vs CLOUD TRAIL - Change Management Tools :
Change management is defined as “the Process responsible for controlling the Lifecycle of all Changes.
The primary objective of Change Management is to enable beneficial changes to be made, with minimum disruption to IT Services.
Despite all of the investments in software and hardware, an erroneous configuration or misstep in a process can frequently undo these efforts and lead to failure.
AWS Config and AWS CloudTrail are change management tools that help AWS customers audit and monitor all resource and configuration changes in their AWS environment
Customers can use AWS Config to answer “What did my AWS resource look like?” at a point in time. Customers can use AWS CloudTrail to answer “Who made an API call to modify this resource?” For example, a customer can use the AWS Management Console for AWS Config to detect that the security group “Production-DB” was incorrectly configured in the past. Using the integrated AWS CloudTrail information, they can pinpoint which user misconfigured the “Production-DB” security group. In brief, AWS Config provides information about the changes made to a resource, and AWS CloudTrail provides information about who made those changes. These capabilities enable customers to discover any misconfigurations, fix them, and protect their workloads from failures.

####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
BYOL - Dedicated HOSTS :
// We have access to physical cores of hardware and underlying sockets that we can use with licenses.
You have a variety of options for using new and existing Microsoft software licenses on the AWS Cloud. By purchasing Amazon Elastic Compute Cloud (Amazon EC2) or Amazon Relational Database Service (Amazon RDS) license-included instances, you get new, fully compliant Windows Server and SQL Server licenses from AWS.
The BYOL model enables AWS customers to use their existing server-bound software licenses, including Windows Server, SQL Server, and SUSE Linux Enterprise Server.
Your existing licenses may be used on AWS with Amazon EC2 Dedicated Hosts, Amazon EC2 Dedicated Instances or EC2 instances with default tenancy using Microsoft License Mobility through Software Assurance.
Dedicated Hosts provide additional control over your instances and visibility into Host level resources and tooling that allows you to manage software that consumes licenses on a per-core or per-socket basis, such as Windows Server and SQL Server. This is why most BYOL scenarios are supported through the use of Dedicated Hosts, while only certain scenarios are supported by Dedicated Instances.

####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
TOTAL COST OF OWNERSHIP - TCO :
-----------------------------------
Which of the following should be considered when performing a TCO analysis to compare the costs of running an application on AWS instead of on-premises?  --> PHYSICAL HARDWARE

Weighing the financial considerations of owning and operating a data center facility versus employing a cloud infrastructure requires detailed and careful analysis.
The Total Cost of Ownership (TCO) is often the financial metric used to estimate and compare costs of a product or a service.
When comparing AWS with on-premises TCO, customers should consider all costs of owning and operating a data center. Examples of these costs include facilities, physical servers, storage devices, networking equipment, cooling and power consumption, data center space, and Labor IT cost.

“Application development” is incorrect. Application development is the process of creating a program or a set of programs to perform the different tasks that a business requires.
Application development is a separate process that customers need to perform regardless of whether they will be using AWS or an on-premises data center. Application development is not part of the total cost of owning and operating a data center (TCO), and thus is an incorrect answer.

“Market Research” is incorrect. Market research is an organized effort to gather information about target audience and customers to determine how viable a product or service might be.
Market research is a separate process that customers need to perform regardless of whether they will be using AWS or an on-premises data center.

“Business analysis” is incorrect. Business analysis is a multistage process aimed at identifying business needs and determining solutions to business problems.
Business analysis is a separate process that customers need to perform regardless of whether they will be using AWS or an on-premises data center.

The AWS TCO (Total Cost of Ownership) Calculator is a free tool that provides directional guidance on possible realized savings when deploying AWS.
This tool is built on an underlying calculation model, that generates a fair assessment of value that a customer may achieve given the data provided by the user which includes the number of servers migrated to AWS, the server type, the number of processors and so on.

####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
"Save when you Reserve" For Customers that can commit to using EC2 over a 1 or 3-year term, it is better to use Amazon EC2 Reserved Instances. Reserved Instances provide a significant discount (up to 75%) compared to On-Demand instance pricing.
"Pay as you go" . Reserved Instances provide a significant discount (up to 75%) compared to On-Demand (pay-as-you-go) instance pricing.
"Pay less as AWS grows" . Pay less as AWS grows refers to the discounts that you get over time as AWS grows. This sometimes called “AWS Economies of Scale”. For example, AWS has reduced the per GB storage price of S3 by 80% since the service was first introduced in 2006.
"Pay less by using more". “Pay less by using more” means that you get volume based discounts and as your usage increases.  For services such as S3, pricing is tiered, meaning the more you use, the less you pay per GB.

####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
AWS MIGRATION ACCELERATION PROGRAMME - MAP:
--------------------------------------------
The AWS Migration Acceleration Program (MAP) is a comprehensive and proven cloud migration program based upon AWS’s experience of migrating a thousand enterprise customers to the cloud.
The program packages best practices, tools, expertise, financial incentives, and a partner ecosystem to make cloud adoption easier.
MAP consists of a three-phase journey that helps you achieve your migration goals by reducing migration complexity and costs.
Leverage the performance, security, and reliability of the cloud

  Step 1: Assess your readiness
  Conduct a migration readiness assessment to identify and evaluate strengths, opportunities for improvement, and business benefits.
  Create a TCO model to build business justification for your migration project.

  Step 2: Mobilize your resources
  Create a migration plan and detailed business case. Then, gain experience by migrating pilot workloads.
  Build your capabilities and experience how easy it is to migrate to AWS.

  Step 3: Migrate and modernize your workloads
  Execute the large-scale migration plan developed during the mobilize phase with the help of migration-certified AWS Partners and the AWS ProServe team.

####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
To enable HTTPS connections to your website or application in AWS, you need an SSL/TLS server certificate.
You can use a server certificate provided by AWS Certificate Manager (ACM) or one that you obtained from an external provider.
You can use ACM or IAM to store and deploy server certificates. Use IAM as a certificate manager only when you must support HTTPS connections in a region that is not supported by ACM.
IAM supports deploying server certificates in all regions, but you must obtain your certificate from an external provider for use with AWS.
Amazon Route 53 is used to register domain names or use your own domain name to route your end users to Internet applications.
Route 53 is not responsible for creating SSL certifications.

####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
AWS recommends some practices to help organizations avoid unexpected charges on their bill. Which of the following is NOT one of these practices?
"Deleting unused AutoScaling launch configuration" will not help, and thus is the correct choice.
The AutoScaling launch configuration does not incur any charges. Thus, it will not make any difference whether it is deleted or not.

AWS will charge the user once the AWS resource is allocated (even if it is not used). Thus, it is advised that once the user's work is completed they should:
1- Delete all Elastic Load Balancers.
2- Terminate all unused EC2 instances.
3- Delete the attached EBS volumes that they don’t need.
4- Release any unused Elastic IPs.

####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
If you suspect that your account has been compromised, or if you have received a notification from AWS that the account has been compromised, perform the following tasks:
1- Change your AWS root account password and the passwords of any IAM users.
2- Delete or rotate all root and AWS Identity and Access Management (IAM) access keys.
3- Delete any potentially compromised IAM users.
4- Delete any resources on your account you didn’t create, such as EC2 instances and AMIs, EBS volumes and snapshots, and IAM users.
5- Respond to any notifications you received from AWS Support through the AWS Support Center.

####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
AWS Serverless Services include:

Compute: AWS Lambda, AWS Fargate
Messaging: Amazon SNS, Amazon SQS
Database: Amazon DynamoDB, Amazon Aurora Serverless
Orchestration: AWS Step Functions

####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
How does AWS notify customers about security and privacy events pertaining to AWS services? -- Security Bulletins page -- https://aws.amazon.com/security/security-bulletins/
AWS supports several MFA device options including Virtual MFA devices, Universal 2nd Factor (U2F) security key, and Hardware MFA devices.

####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
Amazon Cloud Directory and AWS Directory Service are two different services.
AWS Directory Service is the service that provides single sign-on (SSO) to applications and services on AWS.
AWS Directory Service uses secure Windows trusts to enable users to sign in to the AWS Management Console and the AWS Command Line Interface (CLI) using their existing corporate Microsoft Active Directory credentials

Amazon Cloud Directory is a cloud-native, highly scalable, high-performance directory service that provides web-based directories to make it easy for you to organize and manage all your application resources such as users, groups, locations, devices, and policies, and the rich relationships between them.
Unlike existing traditional directory systems, Cloud Directory does not limit organizing directory objects in a single fixed hierarchy.
In Cloud Directory, you can organize directory objects into multiple hierarchies to support multiple organizational pivots and relationships across directory information.
For example, a directory of users may provide a hierarchical view based on reporting structure, location, and project affiliation.
Similarly, a directory of devices may have multiple hierarchical views based on its manufacturer, current owner, and physical location. With Cloud Directory, you can create directories for a variety of use cases, such as organizational charts, course catalogs, and device registries.

####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
PLOT LIGHT :
-----------
A pilot light scenario is a disaster recover / business continuity scenario wherein a minimal amount of services are kept running in a failover location to enable the business to meet their Recovery Time Objective (RTO) and Recovery Point Objective (RPO) in the event of a disaster.
By nature, a pilot light scenario will take some time to spin up and promote to production (as opposed to an active-active DR scenario) and will therefore not mitigate the per-minute losses that will be experienced by the company in the event of an outage.
Additional information: Recovery time objective (RTO) and recovery point objective (RPO) are two key metrics to consider when developing a disaster recover (DR) plan. RTO represents how many hours it takes customers to return to a working state after a disaster. RPO, which is also expressed in hours, represents how much data customers could lose when a disaster happens.
For example, an RPO of 1 hour means that customers could lose up to 1 hour’s worth of data when a disaster occurs.

####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
DynamoDB global tables are ideal for massively scaled applications with globally dispersed users. Global tables provide automatic replication to AWS Regions world-wide. They enable you to deliver low-latency data access to your users no matter where they are located.
DynamoDB Accelerator (DynamoDB DAX) is an in-memory cache for DynamoDB that reduces response times from milliseconds to microseconds.
DynamoDB point-in-time recovery (PITR) is used to back up your data with per-second granularity and restore to any single second from the time it was enabled up to the prior 35 days. DynamoDB PITR works as additional insurance against accidental loss of data.
DynamoDb Streams is an optional feature that captures data modification events in DynamoDB tables.

Amazon RDS Read Replicas enable you to create one or more read-only copies of your database instance within the same AWS Region or in a different AWS Region.
Updates made to the source database are then asynchronously copied to your Read Replicas. In addition to providing scalability for read-heavy workloads, Read Replicas can be promoted to become a standalone database instance when needed.
Amazon RDS Multi-AZ deployments provide enhanced availability for database instances within a single AWS Region.
With Multi-AZ, your data is synchronously replicated to a standby in a different Availability Zone (AZ). In the event of an infrastructure failure, Amazon RDS performs an automatic failover to the standby, minimizing disruption to your applications.
For RDS, You can use Read Replicas for both improved read performance as well as Disaster Recovery (using cross-region read replicas), however, by itself, multi-AZ cannot be used for disaster recovery.

When you provision a Multi-AZ DB Instance, Amazon RDS automatically creates a primary DB Instance and synchronously replicates the data to a standby instance in a different Availability Zone (AZ).
In case of an infrastructure failure, Amazon RDS performs an automatic failover to the standby (or to a read replica in the case of Amazon Aurora), so that you can resume database operations as soon as the failover is complete.
Since the endpoint for your DB Instance remains the same after a failover, your application can resume database operation without the need for manual administrative intervention.
For RDS, Read replicas allow you to create read-only copies that are synchronized with your master database.  There is no standby available while using read replicas.
In case of infrastructure failure, you have to manually promote the read replica to be its own standalone DB Instance, which means that the database endpoint would change.

####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
Which of the following factors should be considered when determining the region in which AWS Resources will be deployed?
--> Per AWS Best Practices, proximity to your end users, regulatory compliance, data residency constraints, and cost are all factors you have to consider when choosing the most suitable AWS Region.

The other options are incorrect:
The planned number of VPCs" is incorrect. The number of VPCs a customer can have in a given region is the same irrespective of which AWS Region the customer is using.
The AWS Region’s security level" is incorrect. The level of security is almost identical for all AWS regions.
​Geographic proximity to the company's location" is incorrect.
To achieve the lowest network latency and the quickest response, the best practice is to choose the closest AWS region to the end-users (not to the company's location).
For example, if an application is developed in Japan but is primarily accessed by users in North America, the customers will have a better experience (lower application latency) if the application is deployed to AWS Regions in North America than if it were deployed to the Tokyo Region.

####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
LOCAL ZONES vs OUTPOSTS :
--------------------------
Local zones are availability zones closure to large population, industry or an IT centre.
AWS Local Zones are a type of AWS infrastructure deployment that places AWS compute, storage, database, and other select services close to large population, industry, and IT centers.
With AWS Local Zones, you can easily run applications that need single-digit millisecond latency closer to end-users in a specific geography.
AWS Local Zones are ideal for use cases such as media & entertainment content creation, real-time gaming, live video streaming, and machine learning inference.

Even though we can use cloud , some applications still need to be on premises for latency issues . Hence OUTPOSTS.
So instead of changing applications to meet both cloud and on premises apps , we can use same API's to access resources in both.
AWS Technician comes and set up all the resources needed to connect resources in cloud and resources in on-premises.
Now we connect the outpost to the nearest REGION using a DIRECT CONNECT or VPN or use a local GW to connect to on premises resources.
Also create a VPC and enable communication between your instances.
An Outpost is a pool of AWS compute and storage capacity deployed at a customer site.
AWS Outposts brings native AWS services, infrastructure, and operating models to virtually any data centre, co-location space, or on-premises facility.
You can use the same services, tools, and partner solutions to develop for the cloud and on premises .

####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
BEANSTALK vs CLOUD FORMATION : https://stackoverflow.com/questions/14422151/what-is-the-difference-between-elastic-beanstalk-and-cloudformation-for-a-net-p
---------------------------------------
They're actually pretty different.
Elastic Beanstalk is intended to make developers' lives easier.
Cloud Formation is intended to make systems engineers' lives easier.

Elastic Beanstalk is a PaaS-like layer on top of AWS's IaaS services which abstracts away the underlying EC2 instances, Elastic Load Balancers, auto scaling groups, etc.
This makes it a lot easier for developers, who don't want to be dealing with all the systems stuff, to get their application quickly deployed on AWS.
It's very similar to other PaaS products such as Heroku, EngineYard, Google App Engine, etc.
With Elastic Beanstalk, you don't need to understand how any of the underlying magic works.

CloudFormation, on the other hand, doesn't automatically do anything.
It's simply a way to define all the resources needed for deployment in a huge JSON file.
So a CloudFormation template might actually create two ElasticBeanstalk environments (production and staging), a couple of Elastic Cache clusters, a DyanmoDB table, and then the proper DNS in Route53.
I then upload this template to AWS, walk away, and 45 minutes later everything is ready and waiting.
Since it's just a plain-text JSON file, I can stick it in my source control which provides a great way to version my application deployments.
It also ensures that I have a repeatable, "known good" configuration that I can quickly deploy in a different region.

Behind the scenes, Elastic Beanstalk uses CloudFormation to create and maintain resources.
If your application requirements dictate more custom control, the additional functionality of CloudFormation gives you more options to control your workloads.

AWS CloudFormation supports Elastic Beanstalk application environments as one of the AWS resource types.
This allows you, for example, to create and manage an AWS Elastic Beanstalk–hosted application along with an RDS database to store the application data.
In addition to RDS instances, any other supported AWS resource can be added to the group as well.

Tools used for quicker app deployments --> [[ lightsail - quick start references - cloud formation - bean stalk - serverless application models - launch wizard - Code Star]

####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
VPC Peering vs PrivateLink:

These 2 developed separately, but have more recently found themselves intertwined.
VPC Peering - applies to VPC
PrivateLink - applies to Application/Service

With VPC Peering you connect your VPC to another VPC. Both VPC owners are involved in setting up this connection.
When one VPC, (the visiting) wants to access a resource on the other (the visited), the connection need not go through the internet.

PrivateLink provides a convenient way to connect to applications/services by name with added security.
You configure your application/service in your VPC as an AWS PrivateLink-powered service (referred to as an endpoint service).
AWS generates a specific DNS hostname for the service. Other AWS principals can create a connection to your endpoint service after you grant them permission.

VPC Peering + PrivateLink :
As of March 7, 2019, applications in a VPC can now securely access AWS PrivateLink endpoints across VPC peering connections.
AWS PrivateLink endpoints can now be accessed across both intra-region and inter-region VPC peering connections

####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################

AMAZON COMPREHEND :
----------------------
It is a natural language processing (NLP) service that uses machine learning to find insights and relationships in text. No machine learning experience required.
There is a treasure trove of potential sitting in your unstructured data. Customer emails, support tickets, product reviews, social media, even advertising copy represents insights into customer sentiment that can be put to work for your business.
The question is how to get at it? As it turns out, Machine learning is particularly good at accurately identifying specific items of interest inside vast swathes of text (such as finding company names in analyst reports), and can learn the sentiment hidden inside language (identifying negative reviews, or positive customer interactions with customer service agents), at almost limitless scale.
Amazon Comprehend uses machine learning to help you uncover the insights and relationships in your unstructured data. The service identifies the language of the text; extracts key phrases, places, people, brands, or events; understands how positive or negative the text is; analyzes text using tokenization and parts of speech; and automatically organizes a collection of text files by topic. You can also use AutoML capabilities in Amazon Comprehend to build a custom set of entities or text classification models that are tailored uniquely to your organization’s needs.
For extracting complex medical information from unstructured text, you can use Amazon Comprehend Medical. The service can identify medical information, such as medical conditions, medications, dosages, strengths, and frequencies from a variety of sources like doctor’s notes, clinical trial reports, and patient health records. Amazon Comprehend Medical also identifies the relationship among the extracted medication and test, treatment and procedure information for easier analysis. For example, the service identifies a particular dosage, strength, and frequency related to a specific medication from unstructured clinical notes.
Amazon Comprehend is fully managed, so there are no servers to provision, and no machine learning models to build, train, or deploy. You pay only for what you use, and there are no minimum fees and no upfront commitments.

AWS QUICK STARTS :
----------------------
AWS Quick Start Reference Deployments outline the architectures for popular enterprise solutions on AWS and provide AWS CloudFormation templates to automate their deployment.
Each Quick Start launches, configures, and runs the AWS compute, network, storage, and other services required to deploy a specific workload on AWS, using AWS best practices for security and availability.
Quick Starts are built by AWS solutions architects and partners to help you deploy popular technologies on AWS, based on AWS best practices.
These accelerators reduce hundreds of manual installation and configuration procedures into just a few steps, so you can build your production environment quickly and start using it immediately.
Each Quick Start includes AWS CloudFormation templates that automate the deployment and a guide that discusses the architecture and provides step-by-step deployment instructions.

AWS MARKET-PLACE :  Buy & Sell - Paid or Free AWS Software Solutions .
--------------------
AWS Marketplace is an online store where you can buy or sell software that runs on Amazon Web Services (AWS).
AWS Marketplace is a curated digital catalog customers can use to find, buy, deploy, and manage third-party software, data, and services that customers need to build solutions and run their businesses.
AWS Marketplace includes thousands of software listings from popular categories such as security, networking, storage, machine learning, business intelligence, database, as well as related professional services to help you manage and support those solutions.
AWS Marketplace also simplifies software licensing and procurement with flexible pricing options and multiple deployment methods.
Customers can quickly launch pre-configured software with just a few clicks, and choose software solutions in AMI and SaaS formats, as well as other formats.
Flexible pricing options include free trial, hourly, monthly, annual, multi-year, and BYOL, and get billed from one source, AWS.

TECHNICAL ACCOUNT MANAGER - TAM :
------------------------------------
Specific to Enterprise customers. work with a technical account manager (TAM) for your specific use cases and applications. Technical account managers play an important role in winning sales.
They are responsible for analyzing prospects' business and technical requirements and developing solutions that meet those needs.
In some cases, they may work with product development teams to customize products for individual customers.
A Technical Account Manager (TAM) is your designated technical point of contact who helps you onboard, provides advocacy and guidance to help plan and build solutions using best practices, coordinates access to subject matter experts, assists with case management, presents insights and recommendations on your AWS spend

CLOUD ADOPTION FRAMEWORK - CAF :
----------------------------------
AWS developed the AWS Cloud Adoption Framework (AWS CAF), which helps organizations understand how cloud adoption transforms the way they work.
AWS CAF leverages our experiences assisting companies around the world with their Cloud Adoption Journey.
Assessing migration readiness across key business and technical areas, referred to as Perspectives, helps determine the most effective approach to an enterprise cloud migration effort.
First, let’s outline what we mean by perspective. AWS CAF is organized into six areas of focus, which span your entire organization.
We describe these areas of focus as Perspectives: Business, People, Governance, Platform, Security, and Operations.
AWS CAF provides a mental model to establish areas of focus in determining readiness to migrate and creating a set of migration execution workstreams.
As these are key areas of the business impacted by cloud adoption, it’s important that we create a migration plan that considers and incorporates the necessary requirements across each area.

The AWS Migration Acceleration Program - MAP :
------------------------------------------------
A comprehensive and proven cloud migration program based upon AWS’s experience of migrating a thousand enterprise customers to the cloud.
The program packages best practices, tools, expertise, financial incentives, and a partner ecosystem to make cloud adoption easier.
MAP consists of a three-phase journey that helps you achieve your migration goals by reducing migration complexity and costs.
Leverage the performance, security, and reliability of the cloud

INFRASTRUCTURE EVENT MANAGEMENT - IEM :
------------------------------------------
Short-term engagement with AWS Support to get a deep understanding of your use case.
After analysis, provide architectural and scaling guidance for an event.
AWS Infrastructure Event Management is a short-term engagement with AWS Support, included in the Enterprise-level Support product offering, and available for additional purchase for Business-level Support subscribers.
AWS Infrastructure Event Management partners with your technical and project resources to gain a deep understanding of your use case and provide architectural and scaling guidance for an event.
Common use-case examples for AWS Event Management include advertising launches, new product launches, and infrastructure migrations to AWS.

AWS PARTNER NETWORK - APN :  // APN Consulting partners & APN Technology Partners
----------------------------
The AWS Partner Network (APN) is the global partner program for technology and consulting businesses that leverage Amazon Web Services to build solutions and services for customers.
The startup can work with experts from APN to build a custom solution for this infrastructure migration.

     CONSULTING : professional services firms that help customers of all types and sizes design, architect, build, migrate, and manage their workloads and applications on AWS, accelerating their migration to AWS cloud.
     TECHNOLOGY : provide hardware, connectivity services, or software solutions that are either hosted on or integrated with, the AWS Cloud. APN Technology Partners cannot help in migrating to AWS and managing applications on AWS Cloud.

TOTAL COST OF OWNERSHIP - TCO  :
------------------------------------
The Total Cost of Ownership (TCO) is often the financial metric used to estimate and compare costs of a product or a service.
When comparing AWS with on-premises TCO, customers should consider all costs of owning and operating a data center. Examples of these costs include facilities, physical servers, storage devices, networking equipment, cooling and power consumption, data center space, and Labor IT cost.
The AWS TCO (Total Cost of Ownership) Calculator is a free tool that provides directional guidance on possible realized savings when deploying AWS.
This tool is built on an underlying calculation model, that generates a fair assessment of value that a customer may achieve given the data provided by the user which includes the number of servers migrated to AWS, the server type, the number of processors and so on.

AWS ABUSE TEAM :
------------------
AWS Support can't assist with reports of abuse or questions about notifications from the AWS Abuse team.
The AWS Abuse team can assist you when AWS resources are used to engage in the following types of abusive behaviour :
  --> Spam  : You are receiving unwanted emails from an AWS-owned IP address, or AWS resources are used to spam websites or forums.
  --> Port scanning: Your logs show that one or more AWS-owned IP addresses are sending packets to multiple ports on your server, and you believe this is an attempt to discover unsecured ports.
  --> DOS attacks: Your logs show that one or more AWS-owned IP addresses are used to flood ports on your resources with packets, and you believe that this is an attempt to overwhelm or crash your server or the software running on your server.
  --> Intrusion attempts: Your logs show that one or more AWS-owned IP addresses are used to attempt to log in to your resources.
  --> Hosting objectionable or copyrighted content: You have evidence that AWS resources are used to host or distribute illegal content or distribute copyrighted content without the consent of the copyright holder.
  --> Distributing malware: You have evidence that AWS resources are used to distribute software that was knowingly created to compromise or cause harm to computers or machines on which it is installed.
If you suspect that AWS resources are used for abusive purposes, contact the AWS Abuse team using the Report Amazon AWS abuse form, or by contacting abuse@amazonaws.com.
Provide all the necessary information, including logs in plaintext, email headers, and so on, when you submit your request.
The AWS Abuse team doesn't open attachments under any circumstance. You must provide any necessary information in plaintext.

AWS TRUSTED ADVISOR :
------------------------
Cost Optimization -  Performance -  Security - Fault Tolerance -  Service Limits
AWS Trusted Advisor offers a rich set of best practice checks and recommendations across five categories: cost optimization, security, fault tolerance, performance, and service limits.
AWS Trusted Advisor is your customized cloud expert! It helps you to observe best practices for the use of AWS by inspecting your AWS environment with an eye toward saving money, improving system performance and reliability, and closing security gaps.
All AWS customers can start optimizing their cloud resources by using four of AWS Trusted Advisor’s most popular best-practice recommendations on service limits and security configurations.
Access to the four free checks does not expire, and you can access the remaining 33 checks and additional features by upgrading to Business or Enterprise-level Support.

The four free checks include : Security Groups - Specific Ports Unrestricted, Service Limits, IAM Use, and MFA on Root Account
You can also subscribe to Business and Enterprise-level support to access the full suite of Trusted Advisor best-practice checks, API access, and a host of other support features, such as phone and chat access to support personnel, architecture guidance, API access, and help with third-party software configuration.
AWS Trusted Advisor is an online tool that provides you real time guidance to help you provision your resources following AWS best practices.
Trusted Advisor checks help optimize your AWS infrastructure, increase security and performance, reduce your overall costs, and monitor service limits.
Whether establishing new workflows, developing applications, or as part of ongoing improvement, take advantage of the recommendations provided by Trusted Advisor on a regular basis to help keep your solutions provisioned optimally.

AWS Basic Support and AWS Developer Support customers get access to 6 security checks (S3 Bucket Permissions, Security Groups - Specific Ports Unrestricted, IAM Use, MFA on Root Account, EBS Public Snapshots, RDS Public Snapshots) and 50 service limit checks.
AWS Business Support and AWS Enterprise Support customers get access to all 115 Trusted Advisor checks (14 cost optimization, 17 security, 24 fault tolerance, 10 performance, and 50 service limits) and recommendations.


####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
A policy is an object in AWS that, when associated with an identity or resource, defines their permissions.
AWS evaluates these policies when an IAM principal (user or role) makes a request. Permissions in the policies determine whether the request is allowed or denied.

Each policy consists of:

1- Principal: Who needs access.
2- Action: What action to allow or deny.
3- Resource: Which resource to allow or deny the action on.
4- Effect: What will be the effect when the user requests access - either allow or deny.
5- Condition: Which conditions must be present for the policy to take effect.
              For example, you might allow access only to the specific S3 buckets if the user is connecting from a specific IP range or has used multi-factor authentication at login.
Note: Permissions are granted to IAM identities (users, groups, and roles) to determine whether they are authorized to perform an action or not.

####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################

Chef and Puppet are automation platforms that allow you to use code to automate the configurations of your servers.
OpsWorks lets you use Chef and Puppet to automate how servers are configured, deployed, and managed across your Amazon EC2 instances or on-premises compute environments.

Key-pairs are regional specific.
Launching a new ec2 instance in new region needs us to create a new key-pair in that particular region.

Placement Groups are logical groupings or clusters of EC2 instances within a single Availability Zone.
Placement groups are recommended for applications that require low network latency, high network throughput, or both.

Data protection refers to protecting data while in-transit (as it travels to and from Amazon S3) and at rest (while it is stored on disks in Amazon data centers).
You can protect data in transit by using SSL/TLS or by using client-side encryption.
Also, you have the following options of protecting data at rest in Amazon S3.
1- Use Server-Side Encryption – You configure Amazon S3 to encrypt your object before saving it on disks in its data centers and decrypt it when you download the objects. ( by encrypting data prior to uploading it )
2- Use Client-Side Encryption – You can encrypt your data on the client-side and upload the encrypted data to Amazon S3. In this case, you manage the encryption process, the encryption keys, and related tools. ( by using s3 encryption while at rest )

Amazon DynamoDB Accelerator (DAX) is a fully managed, highly available, in-memory cache for DynamoDB that delivers performance improvements from milliseconds to microseconds – even at millions of requests per second.
DAX adds in-memory acceleration to your DynamoDB tables without requiring you to manage cache invalidation, data population, or cluster management.
Multi-AZ and Read Replica are Amazon RDS features not for Dynamo Db.

AWS Directory Service is a managed Microsoft Active Directory in the AWS Cloud.
Customers can use it to manage users and groups, provide single sign-on (SSO) to applications and services, as well as create and apply group policies.
Note: What is Single sign-on (SSO)? AWS Single sign-on (AWS SSO) enables a company’s employees to sign in to AWS using their existing corporate Microsoft Active Directory credentials.

AWS Virtual Private Network (AWS VPN connection goes over the public internet) provides an internet-based connection that enables customers to connect their on-premises network or branch office site to AWS.
Internet-based connectivity can have unpredictable performance and despite being encrypted, can present security concerns.
AWS Direct Connect bypasses the public Internet and uses a standard Ethernet fiber-optic cable to establish a secure, dedicated, and more consistent connectivity from on-premises data centers into AWS.
AWS VPN is incorrect because transferring large data sets over the Internet can be time consuming and expensive.
Additionally, AWS VPN is an internet-based connection and does not meet the requirement of consistent connectivity.
But Unlike AWS Direct Connect, VPN Connections can be configured in minutes and are a good solution if customers have an immediate need, have low to modest bandwidth requirements, and can tolerate the inherent variability in Internet-based connectivity.

AWS Global Accelerator is a networking service that improves the availability and performance of the applications that you offer to your global users.
Today, if you deliver applications to your global users over the public internet, your users might face inconsistent availability and performance as they traverse through multiple public networks to reach your application.
These public networks can be congested and each hop can introduce availability and performance risk.
AWS Global Accelerator uses the highly available and congestion-free AWS global network to direct internet traffic from your users to your applications on AWS, making your users’ experience more consistent.
To improve the availability of your application, you must monitor the health of your application endpoints and route traffic only to healthy endpoints.
AWS Global Accelerator improves application availability by continuously monitoring the health of your application endpoints and routing traffic to the closest healthy endpoints.

Amazon S3 Transfer Acceleration is used to enable fast transfers of files over long distances between your client and an S3 bucket.
You might want to use Transfer Acceleration on a bucket for various reasons, including the following:
1- You have customers that upload to a centralized bucket from all over the world.
2- You transfer gigabytes to terabytes of data on a regular basis across continents.
3- You are unable to utilize all of your available bandwidth over the Internet when uploading to Amazon S3.

AWS has eliminated "bidding" in the new AWS Spot instance pricing model.
The way the new pricing model works is that you just pay the Spot price that’s in effect for the current hour for the instances that you launch.
It’s that simple. Now you can request Spot capacity just like you would request On-Demand capacity, without having to spend time analyzing market prices or setting a bid price.
In the new model, the Spot prices are more predictable, updated less frequently, and are determined by the long-term supply and demand for Amazon EC2 spare capacity, not bid prices.
Your Spot Instance runs whenever capacity is available and the maximum price per hour for your request exceeds the Spot price.
An example to illustrate: If the current AWS Spot price is $0.08 per hour and you set a maximum price of $0.17, you’ll pay $0.08 and you will lose the instances if the AWS Spot price rises above $0.17 or if capacity is no longer available.

Reserved instance types include:
- Standard RIs    : These provide the most significant discount (up to 72% off On-Demand) and are best suited for steady-state usage.
- Convertible RIs : These provide a discount (up to 54% off On-Demand) and the capability to change the attributes of the RI as long as the exchange results in the creation of Reserved Instances of equal or greater value.
- Schedules RI's
Therefore, Standard RIs provides more discounts than Convertible RIs.


AWS Snowball is a petabyte-scale data transport solution that uses secure appliances to transfer large amounts of data into and out of the AWS cloud.
Using Snowball addresses common challenges with large-scale data transfers, including high network costs, long transfer times, and security concerns.
Transferring data with Snowball is simple, fast, secure, and can cost as little as one-fifth the cost of using high-speed internet.
 --> Additionally, With AWS Snowball, you can access the compute power of the AWS Cloud locally and cost-effectively in places where connecting to the internet might not be an option. AWS Snowball is a perfect choice if you need to run computing in rugged, austere, mobile, or disconnected (or intermittently connected) environments.
 --> With AWS Snowball, you have the choice of two devices, Snowball Edge Compute Optimized with more computing capabilities, suited for higher performance workloads, or Snowball Edge Storage Optimized with more storage, which is suited for large-scale data migrations and capacity-oriented workloads.
Snowball Edge Storage Optimized devices provides up to 80 TB of usable storage.


AWS Cost Explorer has an easy-to-use interface that lets you visualize, understand, and manage your AWS costs and usage over time.
Cost Explorer’s cost forecast capabilities use machine learning to learn each customer’s historical spend patterns and use that information to forecast expected costs.
Cost Explorer’s forecasting enables you to get a better idea of what your costs and usage may look like in the future, so that you can plan ahead.
Forecasting capabilities have been enhanced to support twelve month forecasts (previously forecasts were limited to three months) for multiple cost metrics, including unblended and amortized costs.
AWS Cost Explorer forecasts your future costs based on your past usage; NOT based on your expected usage.
The AWS tool that can provide accurate estimates of AWS service costs based on your expected usage is the AWS Simple Monthly Calculator.
For example, if you are planning to use 500 GB of S3 storage, you can input this value directly in the AWS Simple Monthly Calculator interface and the calculator provides an estimate of what you will pay monthly for this amount of storage.


When you launch instances on a Dedicated Host, the instances run on a physical server that is dedicated for your use.
// We have access to physical cores of hardware and underlying sockets that we can use with licenses.
While Dedicated instances also run on dedicated hardware, Dedicated Hosts provide further visibility and control by allowing you to place your instances on a specific, physical server.
This enables you to deploy instances using configurations that help address corporate compliance and regulatory requirements.
Note:   Amazon EC2 purchasing options include: On-Demand, Savings Plans, Reserved Instances, Spot Instances, Dedicated Hosts and Dedicated instances.
        Dedicated Instances also provides Hardware isolation. Dedicated Instances are Amazon EC2 instances that run in a virtual private cloud (VPC) on hardware that's dedicated to a single customer.
        Your Dedicated instances are physically isolated at the host hardware level from instances that belong to other AWS accounts. However, Dedicated Instances may share hardware with other instances from the same AWS account that are not Dedicated Instances.

The difference between Dedicated Hosts and Dedicated Instances:
// We have access to physical cores of hardware and underlying sockets that we can use with licenses.
1- Dedicated Instances guarantee that the instances will run on hardware that's dedicated to a single AWS account.
   But, as we mentioned above, Dedicated Instances may share hardware with other instances from the same AWS account that are not Dedicated Instances.
   That is not the case with Dedicated Hosts. Dedicated Hosts allow you to control how instances are placed on a specific physical server, and you can consistently deploy your instances to the same physical server over time.
   For that reason, Dedicated Hosts is a better option to handle compliance and regulatory requirements in most scenarios.
   PER INSTANCE BILLING.
2- Dedicated Hosts enable you to benefit from the Bring Your Own License (BYOL) model for almost every BYOL scenario, while only certain scenarios are supported by Dedicated Instances.
   The BYOL model enables AWS customers to use their existing server-bound software licenses, including Windows Server, SQL Server, and SUSE Linux Enterprise Server.
   Dedicated Hosts provide additional control over your instances and visibility into Host level resources and tooling that allows you to manage software that consumes licenses on a per-core or per-socket basis, such as Windows Server and SQL Server.
   This is why most BYOL scenarios are supported through the use of Dedicated Hosts, while only certain scenarios are supported by Dedicated Instances.
   PER HOST BILLING.

When you enable Multi-AZ, Amazon Relational Database Service (Amazon RDS) maintains a redundant and consistent standby copy of your data.
If you encounter problems with the primary copy, Amazon RDS automatically switches to the standby copy (or to a read replica in the case of Amazon Aurora) to provide continued availability to the data.
The two copies are maintained in different Availability Zones (AZs), hence the name “Multi-AZ.” Each AZ runs on its own physically distinct, independent infrastructure, and is engineered to be highly reliable.
Having separate Availability Zones greatly reduces the likelihood that both copies will concurrently be affected by most types of disturbances.

Federation is an AWS feature that enables users to access and use AWS resources using their existing corporate credentials.
Federation uses open standards, such as Security Assertion Markup Language 2.0 (SAML), to exchange identity and security information between an identity provider (IdP) and an application.
AWS offers multiple options for federating your identities in AWS:
1- AWS Identity and Access Management (IAM): You can use AWS Identity and Access Management (IAM) to enable users to sign in to their AWS accounts with their existing corporate credentials.
2- AWS Directory Service: AWS Directory Service for Microsoft Active Directory, also known as AWS Microsoft AD, uses secure Windows trusts to enable users to sign in to the AWS Management Console, AWS Command Line Interface (CLI), and Windows applications running on AWS using their existing corporate Microsoft Active Directory credentials.
3- AWS Single-Sign-On (AWS SSO) Service: You can use the AWS SSO service to federate your identities into your AWS environment.

In relation to Amazon RDS databases:
  AWS is responsible for:
      1- Managing the underlying infrastructure and foundation services.
      2- Managing the operating system.
      3- Database setup.
      4- Patching and backups.
  The customer is still responsible for:
      1- Protecting the data stored in databases (through encryption and IAM access control).
      2- Managing the database settings that are specific to the application.
      3- Building the relational schema.
      4- Network traffic protection.

AWS Cost Governance Best Practices:
1- Resource controls (policy-based and automated) govern who can deploy resources and the process for identifying, monitoring, and categorizing these new resources. These controls can use tools such as AWS Service Catalog, AWS Identity and Access Management (IAM) roles and permissions, and AWS Organizations, as well as third-party tools such as ServiceNow.
2- Cost allocation applies to teams using resources, shifting the emphasis from the IT-as-cost-center mentality to one of shared responsibility.
3- Budgeting processes include reviewing budgets and realized costs, and then acting on them.
4- Architecture optimization focuses on the need to continually refine workloads to be more cost-conscious to create better architected systems.
5- Tagging and tagging enforcement ensure cost tracking and visibility across organization lines.

####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################

AWS environments are continuously audited, and its infrastructure and services are approved to operate under several compliance standards and industry certifications across geographies and industries, including PCI DSS, ISO 2700, ISO 9001, and HIPAA.
You can use these certifications to validate the implementation and effectiveness of AWS security controls.
For example, AWS companies that use AWS products and services to handle credit card information can rely on AWS technology infrastructure as they manage their PCI DSS compliance certification.
AWS services are assessed regularly to comply with common compliance standards NOT with local laws and regulations.

The Payment Card Industry Data Security Standard (PCI DSS) helps ensure that companies maintain a secure environment for storing, processing, and transmitting credit card information or sensitive authentication data (SAD).
AWS customers who use AWS services to store, process, or transmit cardholder data can rely on AWS infrastructure as they manage their own PCI DSS compliance certification.
Security and compliance are important shared responsibilities between AWS and the customer.
It is the customer’s responsibility to maintain their PCI DSS cardholder data environment (CDE) and scope, and be able to demonstrate compliance of all PCI controls, but customers are not alone in this journey.
The use of PCI DSS compliant AWS services can facilitate customer compliance, and the AWS Security Assurance Services team can assist customers with additional information specific to demonstrating the PCI DSS compliance of their AWS workloads.
AWS Services listed as PCI DSS compliant means that they have the ability to be configured by customers to meet their PCI DSS requirements. It does not mean that any use of that service is automatically compliant.
A good rule-of-thumb is that if a customer can set a particular configuration, they are responsible for setting it appropriately to meet PCI DSS requirements.
AWS customers are also responsible for creating a policy that addresses information security for all personnel, and implementing strong access controls to restrict any access to cardholder data.


There are seven design principles for security in the cloud:
1- Implement a strong identity foundation: Implement the principle of least privilege and enforce separation of duties with appropriate authorization for each interaction with your AWS resources. Centralize privilege management and reduce or even eliminate reliance on long-term credentials.
2- Enable traceability: Monitor, alert, and audit actions and changes to your environment in real time. Integrate logs and metrics with systems to automatically respond and take action.
3- Apply security at all layers: Rather than just focusing on protection of a single outer layer, apply a defense-in-depth approach with other security controls. Apply to all layers (e.g., edge network, VPC, subnet, load balancer, every instance, operating system, and application).
4- Automate security best practices: Automated software-based security mechanisms improve your ability to securely scale more rapidly and cost effectively. Create secure architectures, including the implementation of controls that are defined and managed as code in version-controlled templates.
5- Protect data in transit and at rest: Classify your data into sensitivity levels and use mechanisms, such as encryption, tokenization, and access control where appropriate.
6- Keep people away from data: Create mechanisms and tools to reduce or eliminate the need for direct access or manual processing of data. This reduces the risk of loss or modification and human error when handling sensitive data.
7- Prepare for security events: Prepare for an incident by having an incident management process that aligns to your organizational requirements. Run incident response simulations and use tools with automation to increase your speed for detection, investigation, and recovery.

5 PILLARS : https://aws.amazon.com/blogs/apn/the-5-pillars-of-the-aws-well-architected-framework/
-------------
    Performance Efficiency - >   // Horizontal & Vertical Scaling & Serverless architecture -- Experiment more often
    The performance efficiency pillar includes the ability to use computing resources efficiently to meet system requirements.
    Key topics include selecting the right resource types and sizes based on workload requirements, monitoring performance, and making informed decisions to maintain efficiency as business needs evolve.
    On how you can run services efficiently and scalably.

    Reliability            - >  // Availability Zone & Region -- Stop guessing about capacity
    The reliability pillar includes the ability of a system to recover from infrastructure or service disruptions, dynamically acquire computing resources to meet demand, and mitigate disruptions such as misconfigurations or transient network issues.
    A resilient workload quickly recovers from failures to meet business and customer demand.
    Key topics include distributed system design, recovery planning, and how to handle change.
    On how you can build services that are resilient to both service and infrastructure disruptions.

    Operational excellence - >   // Automation to escape human errors - Cloud formation to automate -- Anticipate failure -- Make frequent, small, reversible changes
    The operational excellence pillar includes the ability to run and monitor systems to deliver business value and to continually improve supporting processes and procedures.
    Key topics include automating changes, responding to events, and defining standards to manage daily operations.
    On how you can continuously improve your ability to run systems, create better procedures, and gain insights.

    Cost optimization      - >   // cost explorer - Budgets - Cost usage
    On the ability to avoid or eliminate unneeded cost or sub-optimal resources. CAPEX to OPEX

    Security               - >  Shared Model // Policies (IAM) , Network Security (VPC) , Encryption (KMS), IAM, KMS, MFA, Cloud Trail, Cloud Watch, SNS, Email. -- Enable traceability
    The security pillar includes the ability to protect information, systems, and assets while delivering business value through risk assessments and mitigation strategies.
    Key topics include confidentiality and integrity of data, identifying and managing who can do what with privilege management, protecting systems, and establishing controls to detect security events.

To protect your AWS infrastructure in this situation you should lock down your root user account and all IAM user accounts that the administrator had access to.
To protect your AWS infrastructure you should:
1- Change the email address and the password of the root user account
2- Enable MFA on the root user account
3- Rotate (change) all access keys for all accounts
4- Change the user name and password of all IAM users
5- Enable MFA on all IAM user accounts


Durability refers to the ability of a system to assure data is stored and data remains consistent in the system as long as it is not changed by legitimate access.
This means that data should not become corrupted or disappear due to a system malfunction. Durability is used to measure the likelihood of data loss.
For example, assume you have confidential data stored in your Laptop.
If you make a copy of it and store it in a secure place, you have just improved the durability of that data. It is much less likely that all copies will be simultaneously destroyed.
Amazon EBS volume data is replicated across multiple servers in an Availability Zone to prevent the loss of data from the failure of any single component.
The replication of data makes EBS volumes 20 times more durable than typical commodity disk drives, which fail with an AFR (annual failure rate) of around 4%. For example, if you have 1,000 EBS volumes running for 1 year, you should expect 1 to 2 will have a failure.
Additional information :  Amazon S3 is also considered a durable storage service. Amazon S3 is designed for 99.999999999% (11 9’s) durability. This means that if you store 100 billion objects in S3, you will lose one object at most.

AWS Organizations has five main benefits:
1) Centrally manage access polices across multiple AWS accounts.
2) Automate AWS account creation and management.
3) Control access to AWS services. - SCP's
4) Consolidate billing across multiple AWS accounts.
5) Configure AWS services across multiple accounts.
** Control access to AWS services: AWS Organizations allows you to restrict what services and actions are allowed in your accounts. You can use Service Control Policies (SCPs) to apply permission guardrails on AWS Identity and Access Management (IAM) users and roles. For example, you can apply an SCP that restricts users in accounts in your organization from launching any resources in regions that you do not explicitly allow.
** Consolidate billing across multiple AWS accounts: You can use AWS Organizations to set up a single payment method for all the AWS accounts in your organization through consolidated billing. With consolidated billing, you can see a combined view of charges incurred by all your accounts, as well as take advantage of pricing benefits from aggregated usage, such as volume discounts for Amazon EC2 and Amazon S3.

####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################

AWS Service Control Policies (SCPs):
AWS Service Control Policies (SCPs) or AWS Organizations Policies are a type of organization policy that you can use to manage permissions for all accounts in your organization.
SCPs offer central control over the maximum available permissions for all member accounts in your organization. SCPs help you to ensure member accounts stay within your organization's access control guidelines.
In SCPs, you can restrict which AWS services, resources, and individual API actions the users and roles in each member account can access.
When AWS Organizations blocks access to a service, resource, or API action for a member account, a user or role in that account can't access it.
This block remains in effect even if an administrator of a member account explicitly grants such permissions in an IAM policy.

    Additional information:  What's the difference between an AWS Organizations service control policy (SCP) and an IAM policy?
    An IAM policy provides granular control over what users and roles in individual accounts can do. AWS Organizations expands that control to the account level by giving you control over what users and roles in an account or a group of accounts can do.
    The resulting permissions are the logical intersection of what is allowed by AWS Organizations at the account level and the permissions that are explicitly granted by IAM at the user or role level within that account.
    In other words, the user can access only what is allowed by both the AWS Organizations policies and IAM policies. If either blocks an operation, the user can't access that operation.
    For example, if an SCP applied to an account states that the only actions allowed are Amazon EC2 actions, and the permissions on a principal (IAM user or role) in the same AWS account allow both EC2 actions and Amazon S3 actions, the principal is able to access only the EC2 actions.

You can remove an account from your organization only if the account has the information that is required for it to operate as a standalone account.
For each account that you want to make standalone, you must accept the AWS Customer Agreement, choose a support plan, provide and verify the required contact information, and provide a current payment method.
AWS uses the payment method to charge for any billable (not AWS Free Tier) AWS activity that occurs while the account isn't attached to an organization.
The AWS account can have any Service Control Policies (SCPs) attached to it.
The principals in the AWS account are no longer affected by any service control policies (SCPs) that were defined in the organization.
This means that restrictions imposed by those SCPs are gone, and the users and roles in the account might have more permissions than they had before.

SCPs don't affect users or roles in the management account. They affect only the member accounts in your organization.
Service control policies (SCPs) are a type of organization policy that you can use to manage permissions in your organization.
SCPs offer central control over the maximum available permissions for all accounts in your organization.
SCPs help you to ensure your accounts stay within your organization’s access control guidelines. SCPs are available only in an organization that has all features enabled.
SCPs aren't available if your organization has enabled only the consolidated billing features.
SCPs alone are not sufficient to grant permissions to the accounts in your organization.
No permissions are granted by an SCP. An SCP defines a guardrail, or sets limits, on the actions that the account's administrator can delegate to the IAM users and roles in the affected accounts.
The administrator must still attach identity-based or resource-based policies to IAM users or roles, or to the resources in your accounts to actually grant permissions.
The effective permissions are the logical intersection between what is allowed by the SCP and what is allowed by the IAM and resource-based policies.

When using AWS Organizations with consolidated billing, best practices include:
– Always enable multi-factor authentication (MFA) on the root account.
– Always use a strong and complex password on the root account.
– The Paying account should be used for billing purposes only. Do not deploy resources into the Paying account.


AMAZON POLLY       - Amazon Polly is a service that turns text into lifelike speech, allowing you to create applications that talk, and build entirely new categories of speech-enabled products.
                     Polly's Text-to-Speech (TTS) service uses advanced deep learning technologies to synthesize natural sounding human speech. It cannot be used to discover and protect your sensitive data in AWS.
AMAZON TRANSCRIBE  - You can use Amazon Transcribe to add speech-to-text capability to your applications.
                     Amazon Transcribe uses a deep learning process called Automatic speech recognition (ASR) to convert speech to text quickly and accurately.
                     Amazon Transcribe can be used to transcribe customer service calls, to automate closed captioning and subtitling, and to generate metadata for media assets.
AMAZON TRANSLATE   - Amazon Translate is used for language translation.
                     Amazon Translate uses neural machine translation via deep learning models to deliver more accurate and more natural-sounding translation than traditional statistical and rule-based translation algorithms.
ELASTIC TRANSCODER - Is a media transcoding service.
                     It is designed to be a highly scalable, easy-to-use, and cost-effective way to convert (or transcode) media files from their source format into versions that will play back on devices like smartphones, tablets, and PCs.
AMAZON LEX         - Amazon Lex is a service for building conversational interfaces using voice and text.  ( To build a chatbot using NATURAL LANGUAGE UNDERSTAND -- NLU )
                     Powered by the same conversational engine as Alexa, Amazon Lex provides high-quality speech recognition and language understanding capabilities, enabling the addition of sophisticated, natural language ‘chatbots’ to new and existing applications.


REKOGNITION        - With Amazon Rekognition, you can identify objects, people, text, scenes, and activities in images and videos, as well as detect any inappropriate content.
                     Amazon Rekognition also provides highly accurate facial analysis and facial search capabilities that you can use to detect, analyze, and compare faces for a wide variety of user verification, people counting, and public safety use cases.
SAGEMAKER          - Amazon SageMaker is a fully-managed platform that enables developers and data scientists to quickly and easily build, train, and deploy machine learning models at any scale.
                     Amazon SageMaker removes all the barriers that typically slow down developers who want to use machine learning.
AWS IoT Core       - AWS IoT Core lets you connect IoT devices to the AWS cloud without the need to provision or manage servers.
                     AWS IoT Core can support billions of devices and trillions of messages and can process and route those messages to AWS endpoints and to other devices reliably and securely.
                     With AWS IoT Core, your applications can keep track of and communicate with all your devices, all the time, even when they aren’t connected.
                     AWS IoT Core also makes it easy to use AWS and Amazon services like AWS Lambda, Amazon Kinesis, Amazon S3, Amazon SageMaker, Amazon DynamoDB, Amazon CloudWatch, AWS CloudTrail, Amazon QuickSight, and Alexa Voice Service to build IoT applications that gather, process, analyze and act on data generated by connected devices, without having to manage any infrastructure.

SQS & SNS --> Can be used to decouple components of a microservices based application on AWS Cloud.
Each AWS Region consists of two or more Availability Zones & Each Availability Zone (AZ) consists of one or more discrete data centers.

You are not charged for data transfer IN into AWS and data transfer B/W services within a same region (S3 to EC2 residing in same region etc;)
There are three fundamental drivers of cost with AWS: compute, storage, and outbound data transfer.
In most cases, there is no charge for inbound data transfer or data transfer between other AWS services within the same region.
Outbound data transfer is aggregated across services and then charged at the outbound data transfer rate.
Per AWS pricing, data transfer between S3 and EC2 instances within the same region is not charged, so there would be no data transfer charge for moving 500 GB of data from an EC2 instance to an S3 bucket in the same region.
You are also not charged for data retrieval from S3 Standard and S3 Intelligent Tiering storage types.


When you create a record, you choose a routing policy, which determines how Amazon Route 53 responds to queries:
Simple routing policy       – Use for a single resource that performs a given function for your domain, for example, a web server that serves content for the example.com website.
Failover routing policy     – Use when you want to configure active-passive failover.
Geolocation routing policy  – Use when you want to route traffic based on the location of your users.
Geoproximity routing policy – Use when you want to route traffic based on the location of your resources and, optionally, shift traffic from resources in one location to resources in another.
Latency routing policy      – Use when you have resources in multiple AWS Regions and you want to route traffic to the region that provides the best latency.
Multivalue answer routing   – Use when you want Route 53 to respond to DNS queries with up to eight healthy records selected at random.
Weighted routing policy     – Use to route traffic to multiple resources in proportions that you specify.

PRICES COMPARED TO ON-DEMAND INSTANCES ----- >
SPOT - 90%,
RESERVED-STANDARD - 75%, RESERVED-CONVERTABLE -50
EC2 INSTANCE SAVINGS PLAN - 72%, COMPUTE SAVINGS PLAN - 66%
DEDICATED-HOSTS - 70%,

RESERVED INSTANCES : For EC2, FARGATE, RDS, REDSHIFT, ELASTICAHE
SAVINGS PLAN       : Only for EC2 & FARGATE

The percentage savings for each Reserved option is as follows:
    All you need to remember is that a 3 years term would always be more cost-effective than a 1-year term.
    Then within a term, "all upfront" is better than "partial upfront" which in turn is better than "no upfront" from a cost savings perspective.

          "No upfront payment option with the standard 1-year term"       - 36%
          "Partial upfront payment option with the standard 3-years term" - 39%
          "All upfront payment option with the standard 1-year term"      - 40%

          "No upfront payment option with the standard 3-years term"      - 56%
          "Partial upfront payment option with the standard 3-years term" - 59%
          "All upfront payment option with the standard 1-year term       - 62%

// A VPC endpoint is used to access public services from a VPC without traversing the Internet.
A VPC endpoint enables you to privately connect your VPC to supported AWS services and VPC endpoint services powered by AWS PrivateLink without requiring an internet gateway, NAT device, VPN connection, or AWS Direct Connect connection.
Instances in your VPC do not require public IP addresses to communicate with resources in the service. Traffic between your VPC and the other service does not leave the Amazon network.
There are two types of VPC endpoints: INTERFACE ENDPOINTS & GATEWAY ENDPOINTS.
INTERFACE ENDPOINTS : An elastic network interface with a private IP address from the IP address range of your subnet that serves as an entry point for traffic destined to a supported service.
                      Interface endpoints are powered by AWS Private Link, a technology that enables you to privately access services by using private IP addresses.
GATEWAY ENDPOINTS   : A gateway endpoint is a gateway that you specify as a target for a route in your route table for traffic destined to a supported AWS service.
                      The following AWS services are supported: AMAZON S3 & DYNAMO DB.

https://aws.amazon.com/premiumsupport/plans/
https://aws.amazon.com/s3/storage-classes/
https://aws.amazon.com/blogs/apn/the-5-pillars-of-the-aws-well-architected-framework/
https://docs.aws.amazon.com/general/latest/gr/root-vs-iam.html#aws_tasks-that-require-root
AWS Trusted Advisor provides real-time guidance to help customers provision resources following AWS best practices.
The service offers guidance for cost optimization, performance, security, fault tolerance, and service limits.

AWS services can be accessed in three different ways: // API GATEWAY IS INCORRECT
AWS Management Console           - This is a simple web interface for accessing AWS services.
AWS Command Line Interface (CLI) - You can access AWS services from the command line and automate service management with scripts.
AWS Software Developer Kit (SDK) - You can also access via AWS SDK that provides language-specific abstracted APIs for AWS services.

A research lab wants to optimize the caching capabilities for its scientific computations application running on EC2 instances. Which EC2 storage option is best suited for this use-case?
--> AMAZON EC2 INSTANCE STORE // question about scientific data -> instance store is correct option.

An Amazon Machine Image (AMI) provides the information required to launch an instance. You must specify an AMI when you launch an instance.
You can launch multiple instances from a single AMI when you need multiple instances with the same configuration.
The AMI must be in the same region as that of the EC2 instance to be launched.
If the AMI exists in a different region, you can copy that AMI to the region where you want to launch the EC2 instance.
The region of AMI has no bearing on the performance of the EC2 instance.
When using EBS Volumes with EC2, the volume and the instance must be in the same Availability Zone.

Amazon EFS is a regional service storing data within and across multiple Availability Zones (AZs) for high availability and durability.
Amazon EC2 instances can access your file system (EFS) across AZs, regions, and VPCs, while on-premises servers can access using AWS Direct Connect or AWS VPN.
EFS is highly available and durable . Data is replicated across AZ's by default.

DEVELOPER  : Provides general architectural guidance on how AWS services can be used for various use-cases, workloads, or applications
BUISENESS  : Contextual to your use-cases
ENTERPRISE : Consultative review and guidance based on your applications

Create separate AWS accounts for development and production environments to receive separate invoices". Every AWS account provides its own invoice end of the month.
You can get separate invoices for development and production environments by setting up separate AWS accounts for each environment.
You cannot create separate invoices based on tags. A Cost Allocation Tag is a label that you or AWS assigns to an AWS resource.
Each tag consists of a key and a value. For each resource, each tag key must be unique, and each tag key can have only one value.
You can use tags to organize your resources, and cost allocation tags to track your AWS costs on a detailed level.
AWS provides two types of cost allocation tags, an AWS generated tags and user-defined tags.
AWS defines, creates, and applies the AWS generated tags for you, and you define, create, and apply user-defined tags.
You must activate both types of tags separately before they can appear in Cost Explorer or on a cost allocation report.

// BY DEFAULT ENCRYPTION IS ENABLED FOR :
Amazon S3 Glacier   - Amazon S3 Glacier, is a storage service optimized for infrequently used data, or "cold data. Data at rest stored in S3 Glacier is automatically server-side encrypted using 256-bit Advanced Encryption Standard (AES-256) with keys maintained by AWS.
AWS Storage Gateway - AWS Storage Gateway is a hybrid cloud storage service that gives you on-premises access to virtually unlimited cloud storage. All data transferred between the gateway and AWS storage is encrypted using SSL (for all three types of gateways - File, Volume and Tape Gateways).

// BY DEFAULT BACKUP IS ENABLED FOR :
Which of the below AWS services supports automated backups as a default configuration? -- RDS , // S3 replicates but not backup
BACKUP is online <--> Offline  && REPLICATION is online <--> online

// LAMBDA PRICING IS BASED ON :
Number of requests for the lambda function
The time it takes for the lambda function to execute

// RDS PRICING IS BASED ON :
Size of database, the uptime,
Any additional storage of backup (above the DB size),
Requests made to RDS and data transfer outbound.
Deployment type (e.g. you pay for multi AZ).

// EBS PRICING IS BASED ON :
Amount of data provisioned (not consumed) per month. This means you can have empty space within a volume and you still pay for it. With provisioned IOPS volumes you are also charged for the amount you provision in IOPS
  INCORRECT: "The amount of data storage consumed" is incorrect as you pay for the amount provisioned.
  INCORRECT: "Number of snapshots" is incorrect. You pay for the storage consumed by snapshots, not by the number of snapshots.

  Amazon EBS pricing has two factors:
  1- Volumes: Volume storage for all EBS volume types is charged by the amount of GB you provision per month, until you release the storage.
  2- Snapshots: Snapshot storage is based on the amount of space your data consumes in Amazon S3.
     Because Amazon EBS does not save empty blocks, it is likely that the snapshot size will be considerably less than your volume size.
     Copying EBS snapshots is charged based on the volume of data transferred across regions.
     For the first snapshot of a volume, Amazon EBS saves a full copy of your data to Amazon S3.
     For each incremental snapshot, only the changed part of your Amazon EBS volume is saved.
     After the snapshot is copied, standard EBS snapshot charges apply for storage in the destination region.

    When you want to reduce the costs of Amazon EBS consider the following:
    1- Delete Unattached Amazon EBS Volumes: An easy way to reduce wasted spend is to find and delete unattached volumes. However, when EC2 instances are stopped or terminated, attached EBS volumes are not automatically deleted and will continue to accrue charges since they are still operating.
    2- Resize or Change the EBS Volume Type: Another way to optimize storage costs is to identify volumes that are underutilized and downsize them or change the volume type.
    3- Delete Stale Amazon EBS Snapshots: If you have a backup policy that takes EBS volume snapshots daily or weekly, you will quickly accumulate snapshots. Check for stale snapshots that are over 30 days old and delete them to reduce storage costs.


// CLOUD FRONT PRICING IS BASED ON :
Traffic Distribution : Data transfer and request pricing varies across geographic regions, and pricing is based on the edge location through which your content is served.
Requests             : The number and type of requests (HTTP or HTTPS) made and the geographic region in which the requests are made.
Data Transfer OUT    : The amount of data transferred out of your Amazon CloudFront edge locations.

//S3 PRICING IS BASED ON :
Total amount of data (in GB) stored on S3
Storage class (S3 Standard, S3 Intelligent-Tiering, S3 Standard-Infrequent Access, S3 One Zone-IA, S3 Glacier, or S3 Glacier Deep Archive)
Amount of data transferred out of AWS from S3. // data transferred in to S3 is not at all a factor, it is free of cost
Number of requests to S3

// #READ QUESTION CAREFULLY -- REGIONAL vs GLOBAL SERVICES
But few services, by definition, need to be in a global scope because of the underlying service they offer. AWS IAM, Amazon CloudFront, Route 53, WorkSpaces and WAF are some of the global services.
Amazon S3 is a unique service in the sense that it follows a global namespace but the buckets are regional. You specify an AWS Region when you create your Amazon S3 bucket. This is a regional service.

// EBS is locked to an AZ.
   An EBS in us-east-1a can not be attached to us-east-1b.
   To move volumes across , You need to make snapshots.
   But EBS is redundant, saying it is replicated across a single AZ.

   To share an EBS snapshot, select the snapshot and then goto permissions, change it to PUBLIC or in PRIVATE, add the account number to which to share it.
   The reviever now changes the filter to private and then can access the snapshot.
   To share an encrypted snapshots, we must use the KMS-CMK service.

Examples for Linux\Ubuntu based instances:
--------------------------------------------------
1- If you run a Linux instance for 4 seconds or 20 seconds or 59 seconds, you will be charged for one minute. (this is what we mean by minimum of 1 minute)
2- If you run a Linux instance for 1 minute and 3 seconds, you will be charged for 1 minute and 3 seconds.
3- If you run a Linux instance for 3 hours, 25 minutes and 7 seconds, you will be charged for 3 hours, 25 minutes and 7 seconds.

Examples for non-Linux\Ubuntu instances:
---------------------------------------------------
1- If you run an instance for 4 seconds or 20 seconds or 59 seconds, you will be charged for one hour.
2- If you run an instance for 1 minute and 3 seconds, you will be charged for one hour.
3- If you run an instance for 3 hours, 25 minutes and 7 seconds, you will be charged for 4 hours.

For transferring 50PB of data , the cost efficient transport is SNOW MOBILE, not SNOW BALL.
Snow mobile can transport 100pb at once, while snow ball can do 100TB.

An engineering team is new to the AWS Cloud and it would like to launch a dev/test environment with low monthly pricing. Which AWS service can address this use-case --> LIGHT SAIL, not CLOUD FORMATION as it requires experience in working with AWS.

IAM ACCESS ADVISOR    : Shows the service permissions granted to a user and when those services were last accessed.
                        You can use this information to revise your policies. To summarize, you can identify unnecessary permissions so that you can revise your IAM policies accordingly.
IAM CREDENTIAL REPORT : You can generate and download a credential report that lists all users in your account and the status of their various credentials, including passwords, access keys, and MFA devices.
                        It is not used to review permissions granted to a user.
                        You can get a credential report from the AWS Management Console, the AWS SDKs, and Command Line Tools.
                        You can use credential reports to assist in your auditing and compliance efforts.
                        You can use the report to audit the effects of credential lifecycle requirements, such as password and access key rotation.
                        You can provide the report to an external auditor, or grant permissions to an auditor so that he or she can download the report directly.

AWS CLOUDTRAIL INSIGHTS : AWS CloudTrail Insights helps AWS users identify and respond to unusual activity associated with write API calls by continuously analyzing CloudTrail management events.
                          Insights events are logged when CloudTrail detects unusual write management API activity in your account. If you have CloudTrail Insights enabled, and CloudTrail detects unusual activity, Insights events are delivered to the destination S3 bucket for your trail.

Memory Optimized instance types  - Memory optimized instances are designed to deliver fast performance for workloads that process large data sets in memory. Memory-optimized instances offer large memory size for memory intensive applications including in-memory applications, in-memory databases, in-memory analytics solutions, High Performance Computing (HPC), scientific computing, and other memory-intensive applications.
Compute Optimized instance types - Compute Optimized instances are designed for applications that benefit from high compute power. These applications include compute-intensive applications like high-performance web servers, high-performance computing (HPC), scientific modelling, distributed analytics, and machine learning inference.
Storage Optimized instance types - Dense-storage instances are designed for workloads that require high sequential read and write access to very large data sets, such as Hadoop distributed computing, massively parallel processing data warehousing, and log processing applications. The Dense-storage instances offer the best price/GB-storage and price/disk-throughput across other EC2 instances.
Accelerated computing instance types - Accelerated Computing instance family is a family of instances that use hardware accelerators, or co-processors, to perform some functions, such as floating-point number calculation and graphics processing, more efficiently than is possible in software running on CPUs. Amazon EC2 provides three types of Accelerated Computing instances – GPU compute instances for general-purpose computing,

In CLOUDTRAIL, by default only MANAGEMENT events are logged.
DATA EVENTS & INSIGHT EVENTS are available for extra cost.

AWS Elastic Beanstalk - AWS Elastic Beanstalk is an easy-to-use service for deploying and scaling web applications and services developed with Java, .NET, PHP, Node.js, Python, Ruby, Go, and Docker on familiar servers such as Apache, Nginx, Passenger, and IIS.
You can simply upload your code or ZIP file  and Elastic Beanstalk automatically handles the deployment, from capacity provisioning, load balancing, auto-scaling to application health monitoring.
At the same time, you retain full control over the AWS resources powering your application and can access the underlying resources at any time.

Control Tower is an AWS native service providing a pre-defined set of blueprints and guardrails to help customers implement a landing zone for new AWS accounts.
Control Tower is designed to provide an easy, self-service setup experience and an interactive user interface for ongoing governance with guardrails.
While Control Tower automates creation of a new landing zone with pre-configured blueprints (e.g., AWS SSO for directory and access),
   the AWS Landing Zone solution provides a configurable setup of a landing zone with rich customization options through custom add-ons (e.g., Active Directory, Okta Directory) and ongoing modifications through a code deployment and configuration pipeline.

Amazon Quantum Ledger Database - Amazon QLDB is a fully managed ledger database that provides a transparent, immutable, and cryptographically verifiable transaction log ‎owned by a central trusted authority.
Amazon QLDB can be used to track each and every application data change and maintains a complete and verifiable history of changes over time.
Ledgers are typically used to record a history of economic and financial activity in an organization.
Many organizations build applications with ledger-like functionality because they want to maintain an accurate history of their applications' data, for example, tracking the history of credits and debits in banking transactions, verifying the data lineage of an insurance claim, or tracing the movement of an item in a supply chain network.

AWS offers two types of Savings Plans:
COMPUTE SAVINGS PLAN  provide the most flexibility and help to reduce your costs by up to 66%.
These plans automatically apply to EC2 instance usage regardless of instance family, size, AZ, region, OS or tenancy, and also apply to Fargate and Lambda usage. For example, with Compute Savings Plans, you can change from C4 to M5 instances, shift a workload from EU (Ireland) to EU (London), or move a workload from EC2 to Fargate or Lambda at any time and automatically continue to pay the Savings Plans price.
EC2 INSTANCE SAVINGS PLAN  Plans provide the lowest prices, offering savings up to 72% in exchange for a commitment to the usage of individual instance families in a region (e.g. M5 usage in N. Virginia). This automatically reduces your cost on the selected instance family in that region regardless of AZ, size, OS or tenancy. EC2 Instance Savings Plans give you the flexibility to change your usage between instances within a family in that region. For example, you can move from c5.xlarge running Windows to c5.2xlarge running Linux and automatically benefit from the Savings Plans prices.

Tape Gateway is formerly known as Gateway Virtual Tape Library.
AWS enables covered entities and their business associates subject to the U.S. Health Insurance Portability and Accountability Act of 1996 (HIPAA) to use the secure AWS environment to process, maintain, and store protected health information.
"HIPAA"      is related to  U.S. Health Insurance Portability and Accountability Act
"ISO 27001"  is an information security standard.
"PCI DSS"    is related to the security of credit card payments.
"SOC 1"      is related to financial reporting.

SCHEDULED RIs: These are available to launch within the time windows you reserve.
This option allows you to match your capacity reservation to a predictable recurring schedule that only requires a fraction of a day, a week, or a month.
Other types include, STANDARD & CONVERTABLE.

Which AWS dashboard displays relevant and timely information to help users manage events in progress, and provides proactive notifications to help plan for scheduled activities? -- AWS Personal Health Dashboard
AWS Personal Health Dashboard provides alerts and remediation guidance when AWS is experiencing events that may impact you.
While the Service Health Dashboard displays the general status of AWS services, Personal Health Dashboard gives you a personalized view into the performance and availability of the AWS services underlying your AWS resources.
The dashboard displays relevant and timely information to help you manage events in progress, and provides proactive notification to help you plan for scheduled activities.
With Personal Health Dashboard, alerts are triggered by changes in the health of AWS resources, giving you event visibility, and guidance to help quickly diagnose and resolve issues.

A resource group is a collection of resources that share one or more tags or portions of tags.
To create a resource group, you simply identify the tags that contain the items that members of the group should have in common.

An IAM policy is a policy document that is used to define permissions that can be applied to users, groups and roles.
You don’t apply the policy to the service, you apply it to the role. The role is then used to assign permissions to the AWS service.
With IAM Roles you can delegate permissions to resources for users and services without using permanent credentials (e.g. username and password).
To do so you can create a role and assign an IAM policy to the role that has the permissions required.

AWS Elastic Beanstalk and AWS CloudFormation are both examples of automation.
Beanstalk is a platform service that leverages the automation capabilities of CloudFormation to build out application architectures.

Supported authentication methods for an IAM USER include console passwords, access keys and server certificates.
Access keys are a combination of an access key ID and a secret access key and can be used to make programmatic calls to AWS.
Server certificates are SSL/TLS certificates that you can use to authenticate with some AWS services.

You need to run a production process that will use several EC2 instances and run constantly on an ongoing basis. The process cannot be interrupted or restarted without issue. What EC2 pricing model would be best for this workload? --> RESERVED INSTANCES. , Not ON-DEMAND
Amazon Route 53 features include domain registration, DNS, traffic flow, health checking, and failover. Route 53 does not support DHCP, IP routing or caching.

The 6 advantages of cloud computing are:
– Trade capital expense for variable expense.
– Benefit from massive economies of scale.
– Stop guessing about capacity.
– Increase speed and agility.
– Stop spending money running and maintaining data centers.
– Go global in minutes.

The AWS Storage Gateway service enables hybrid cloud storage between on-premises environments and the AWS Cloud. It seamlessly integrates on-premises enterprise applications and workflows with Amazon’s block and object cloud storage services through industry standard storage protocols.
Amazon Elastic File System (EFS)" is incorrect. Amazon EFS is not a hybrid cloud storage solution. With EFS you can mount file systems from on-premises servers, however it does not offer a local cache or method of moving data into the cloud.

A company has deployed several relational databases on Amazon RDS. Every month, the database software vendor releases new security patches that need to be applied to the database. What is the MOST efficient way to apply the security patches?
  --> Enable automatic patching for the instances using the Amazon RDS console
  Periodically, Amazon RDS performs maintenance on Amazon RDS resources. Maintenance most often involves updates to the DB instance's underlying hardware, underlying operating system (OS), or database engine version.
  Updates to the operating system most often occur for security issues and should be done as soon as possible.
  Required patching is automatically scheduled only for patches that are related to security and instance reliability.
  Such patching occurs infrequently (typically once every few months) and seldom requires more than a fraction of your maintenance window.

An Elastic IP Address can be remapped between EC2 instances across which boundaries? -- AVAILABILITY ZONES
Elastic IP addresses are for use in a specific region only and can therefore only be remapped between instances within that region. You can use Elastic IP addresses to mask the failure of an instance in one Availability Zone by rapidly remapping the address to an instance in another Availability Zone.

Object lifecycle management can be used with objects so that they are stored cost effectively throughout their lifecycle. Objects can be transitioned to another storage class or expired.

The TCO calculator asks for the number of servers (Physical or VMs) you are running on-premises. You also need to supply the resource information (CPU, RAM) and specify whether the server is a DB or non-DB.
Use this new calculator to compare the cost of your applications in an on-premises or traditional hosting environment to AWS. Describe your on-premises or hosting environment configuration to produce a detailed cost comparison with AWS.
"The number of storage systems in your company" is incorrect. You don’t need to specify the number of storage systems, you just need to specify the raw capacity.

Amazon LightSail provides an easy, low cost way to consume cloud services without needing the skill set for using VPC resources.
The product set includes virtual private servers (instances), managed MySQL databases, HA storage, and load balancing
You can connect to other AWS services such as S3, DynamoDB, and CloudFront, however these are not part of the LightSail product range

DATA BACKUP      --> Copies necessary files and applications to a secondary location such as a hosted or offsite server. // ONLINE SERVER <---> OFFLINE SERVER
DATA REPLICATION --> Mirrors the data stored on a server to another server. // ONLINE SERVER <---> ONLINE SERVER

Standard retrievals in archive access tier and deep archive access tier are free.
Using the S3 console, you can pay for expedited retrievals if you need faster access to your data from the archive access tiers.



####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################

                                                                          :::: SOLUTIONS ARCHITECT ASSOCIATE - EXAM TIPS ::::

####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################

####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
:::: EC2 ::::
####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################

Question --> I was able to connect yesterday to an Instance , but today I can't ?
This is probably because you have stopped your EC2 instance and then started it again today.
When you do so, the public IP of your EC2 instance will change. Therefore, in your command, or Putty configuration, please make sure to edit and save the new public IP.

A vCPU stands for virtual central processing unit. One or more vCPUs are assigned to every Virtual Machine (VM) within a cloud environment. Each vCPU is seen as a single physical CPU core by the VM's operating system.

EC2 -- TIMEOUT is an security group issue .
       CONNECTION REFUSED is an application issue inside an EC2.

INSTALLING APACHE WEBSERVER ON LINUX EC2 : // https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_Tutorials.WebServerDB.CreateWebServer.html
---------------------------------------------------
sudo yum update -y
sudo amazon-linux-extras install -y lamp-mariadb10.2-php7.2 php7.2
cat /etc/system-release // Should give the output Amazon linux 2
sudo yum install -y httpd
sudo systemctl start httpd
sudo systemctl enable httpd

--> Now open a web browser and type in http://<pubic-ip>:80 --> Should see the apache homepage
--> Make sure to open , HTTP - TCP - 80 - ALL in the security group .

[root@ip-172-31-40-203 ec2-user]# echo "HELLO WORLD" > /var/www/html/index.html
[root@ip-172-31-40-203 ec2-user]# echo "HELLO WORLD from $(hostname -f)" > /var/www/html/index.html
Now refresh the page again.

EC2-UserData :
----------------
Used to boot-strap an instance. // Launch custom scripts when system starts.
The script is only run once at the instance first start, useful for " Installing updates, download common files from internet etc;"
Runs only with root user so all commands have sudo permissions and more the script, more the start time of instance.
Option available at STEP-3 : CONFIGURE INSTANCE DETAILS.

 EXAMPLE :
 #!/bin/sh
 sudo yum update -y
 sudo amazon-linux-extras install -y lamp-mariadb10.2-php7.2 php7.2
 cat /etc/system-release // Should give the output Amazon linux 2
 sudo yum install -y httpd
 sudo systemctl start httpd
 sudo systemctl enable httpd

Reserved Instances are used  for steady state usage applications such as a DATABASE.
GREAT COMBO : Use a mix of Reserved instances for a baseline such as a web server which will run continuously and On demand instance for un-predictable workloads.

SPOT BLOCKS : you can request a spot instance for a specified time frame - 1 to 6 years.
SPOT REQUEST = maximum price + Desired number of instances + launch specification + Requirement type : one-time / Persistent + Valid from + Valid Until

                                 ---Terminate / Interrupt (Persistent) ---
                                |       (until specified interval)         |
                                V                                          ^
 Create a request -------> Spot Request  ----> Launch Instance ------> [[*****]] ----->Terminate / Interrupt (One-time)
                          (specification)                             (Instances)

SPOT REQUESTS can be of 2 types :
   1. One-time    : Spot request is deleted once an instance is assigned.  So this is a onetime request.
   2. Persistent  : If a spot instance is terminated then the request keeps on requesting for another set of specified spot instance with the max price we defined and until the Valid-from & Valid-to time. This is a consistant request.

You can only cancel spot instance requests that are in OPEN / ACTIVE / DISABLED state.
Cancelling a spot request does not terminate instances.
So to terminate a spot request, you must first cancel a spot request and then terminate the associated spot instances.
Because if we directly terminate and the instance type is persistent, then the instances will be launched again.

// Spot instance, if we know the exact instance or capacity and AZ
// Spot fleet, if we can specify that AWS can chose all these instance types and all the AZ to meet our requirement.

SPOT FLEET : set of spot instances + Optional On-Demand Instances . // we define multiple launch pools , multiple instance types
spot fleets allow us to automatically request spot instances with the lowest price.
Spot fleet will try to meet the target capacity with price constraints.
 --> Define possible launch pools: Instance type, OS , AZ
 --> Can have multiple launch pools, so that the fleet can Choose
 --> spot fleet stops launching instances when reaching capacity or max cost.

STRATEGIES TO ALLOCATE SPOT INSTANCES :
 --> Lowest price : from the pool with the lowest price (cost optimization, short workload)
 --> Diversified  : distributed across all pools        (great for availability, long workloads) // if one pool goes away, then other pools will be active.
 --> Capacity Optimized : pool with the optimal capacity for number of instances


R - Applications that need a lot of RAM - in-memory caches
C - Applications that need a good CPU - compute/databases
M - Applications that are balanced - general / web app
I - Applications that need good local I/O instance storage - databases
G - Applications that need a GPU - video rendering/machine learning
T2/T3 - Burstable instances  // Burst is the BOOST of a CPU.
T2/T3 - Unlimited : unlimited bursts

A CPU core is a CPU’s processor. In the old days, every processor had just one core that could focus on one task at a time. Today, CPUs have been two and 18 cores, each of which can work on a different task.
Bursts has BURST CREDITS, that gets used by spikes in load. If there is high spike then all credits gets used up and CPU goes down.
Again when the load gets low, Credits gets accumulated back.

EBS SNAPSHOT vs AMI Images :
------------------------------
An EBS snapshot is a backup of a single EBS volume. The EBS snapshot contains all the data stored on the EBS volume at the time the EBS snapshot was created.
An AMI image is a backup of an entire EC2 instance. Associated with an AMI image are EBS snapshots.
As a Backup Strategy, Should You Create AMI Images or EBS Snapshots?
The answer to this depends on many factors, including your restore time objective, desired restore procedure, and many other business-related factors.
If your restore objective is to create a fresh EC2 instance quickly to replace a failed EC2 instance, then creating AMI images is definitely the way to go.
If your EC2 instance contains a mix of static EBS volumes (for example, a non-changing root volume) and dynamic EBS volumes (for example, a data volume), then creating EBS snapshots may be better.
Since EBS snapshots can be thought of as incremental, you only pay for the storage of the changed data. So creating multiple EBS snapshots of a non-changing EBS volume will not incur high charges.
The process of creating an AMI image will also create EBS snapshots. So even if you need to restore a single EBS volume, you can do so from the individual EBS snapshot.
For these reasons, unless there are business factors weighing against it, we recommend creating AMI images instead of just EBS snapshots.

       EBS-1 <-----> EBS-2 <-----> EC2
        |             |             |
      Snap-1        Snap-2          |
        |             |             |
  ----------------------------------------
  [[       AMAZON  MACHINE IMAGE         ]]  AMI = snap-1 + snap-2  + ec2 snap
  ----------------------------------------

CROSS ACCOUNT AMI COPY :
---------------------------
Your AMI take space and they live in Amazon S3
• Amazon S3 is a durable, cheap and resilient storage where most of your backups will live (but you won’t see them in the S3 console)
• By default, your AMIs are private, and locked for your account / region
• You can also make your AMIs public and share them with other AWS accounts or sell them on the AMI Marketplace

You can share an AMI with another AWS account.
• Sharing an AMI does not affect the ownership of the AMI.
• If you copy an AMI that has been shared with your account, you are the owner of the target AMI in your account.
• To copy an AMI that was shared with you from another account, the owner of the source AMI must grant you read permissions for the storage that backs the AMI, either the associated EBS snapshot (for an Amazon EBS-backed AMI) or an associated S3 bucket (for an instance store-backed AMI).

LIMITS:
• You can't copy an encrypted AMI that was shared with you from another account.
  Instead, if the underlying snapshot and encryption key were shared with you, you can copy the snapshot while re- encrypting it with a key of your own.
  You own the copied snapshot, and can register it as a new AMI.
• You can't copy an AMI with an associated billing Product code that was shared with you from another account. This includes Windows AMIs and AMIs from the AWS Marketplace.
  To copy a shared AMI with a billing Product code, launch an EC2 instance in your account using the shared AMI and then create an AMI from the instance.

PLACEMENT GROUPS :
---------------------
Sometimes you want control over the EC2 Instance placement strategy , because we cannot directly access infrastructure and launch as we wish.
• That strategy can be defined using placement groups
• When you create a placement group, you specify one of the following strategies for the group:
    • Cluster    — clusters instances into a low-latency group in a single Availability Zone, instances are on same hardware in same AZ.
                   Cluster placement groups are recommended for applications that benefit from low network latency, high network throughput, or both. --> HPC - High-Performance Computing
                   They are also recommended when the majority of the network traffic is between the instances in the group.
                   To provide the lowest latency and the highest packet-per-second network performance for your placement group, choose an instance type that supports enhanced networking.
                   • Pros : Great network (10 Gbps bandwidth between instances)
                   • Cons : If the rack fails, all instances fails at the same time
                   • Use case: Big Data job that needs to complete fast or an application that needs extremely low latency and high network throughput

    • Spread     — spreads instances across underlying hardware (max 7 instances per group per AZ)
                   • Pros : Can span across Availability Zones (AZ) - Reduced risk is simultaneous failure - EC2 Instances are on different physical hardware ( so 7 ec2 on 7 different Hardware)
                   • Cons : Limited to 7 instances per AZ per placement group
                   • Use case: Application that needs to maximize high availability & Critical Applications where each instance must be isolated from failure from each other

    • Partition  — spreads instances across many different partitions (which rely on different sets of racks) within an AZ. Scales to 100s of EC2 instances per group (Hadoop, Cassandra, Kafka)
                   • Up to 7 partitions per AZ • Up to 100s of EC2 instances • The instances in a partition do not share racks with the instances in the other partitions
                   • A partition failure can affect many EC2 but won’t affect other partitions
                   • EC2 instances get access to the partition information as metadata
                   • Use cases: HDFS, HBase, Cassandra, Kafka

EC2 HIBERNATE :
-----------------
• We know we can stop, terminate instances
  • Stop: the data on disk (EBS) is kept intact in the next start
  • Terminate: any EBS volumes (root) also set-up to be destroyed is lost
• On start, the following happens:
  • First start: the OS boots & the EC2 User Data script is run
  • Following starts: the OS boots up
  • Then your application starts, caches get warmed up, and that can take time!
• Introducing EC2 Hibernate:
  • The in-memory (RAM) state is preserved
  • The instance boot is much faster!  (the OS is not stopped / restarted)
  • Under the hood: the RAM state is written to a file in the root EBS volume
  • The root EBS volume must be encrypted
• Use cases: long-running processing - saving the RAM state - services that take time to initialize
• Supported instance families - C3, C4, C5, M3, M4, M5, R3, R4, and R5.
• Instance RAM size - must be less than 150 GB.
• Instance size     - not supported for bare metal instances.
• AMI : Amazon Linux 2, Linux AMI, Ubuntu & Windows…
• Root Volume: must be EBS, encrypted, not instance store, and large
• Available for On-Demand and Reserved Instances and not Spot Instances.
• An instance cannot be hibernated more than 60 days

--> We need to have one classic load balancer infront of each application ,
    But with application load balancer, we can have one AL infront of multiple applications.

--> SSL – Server Name Indication (SNI)
• SNI solves the problem of loading multiple SSL certificates onto one web server (to serve multiple websites)
• It’s a “newer” protocol, and requires the client to indicate the hostname of the target server in the initial SSL handshake
• The server will then find the correct certificate, or return the default one
Note:  • Only works for ALB & NLB (newer generation), CloudFront -- • Does not work for CLB (older gen)

--> ELB – Connection Draining // Waiting for existing connections to complete while an EC2 is getting deregistered / unhealthy -- terminated or stopped.
• Feature naming: • CLB: Connection Draining -- • Target Group: Deregistration Delay (for ALB & NLB)
• Time to complete “in-flight requests” while the instance is de-registering or unhealthy
• Stops sending new requests to the instance which is de-registering
• Between 1 to 3600 seconds, default is 300 seconds
• Can be disabled (set value to 0)
• Set to a low value if your requests are short and high value say 1000sec, if instance is slow to process requests.
  If you set it to 0, then the inflight requests will get an error, and upon refresh by user, the new requests will be routed to other ELB's.

-> N/W load balancer do not have a security group associated with it. It sends the same incoming requests to the ec2 instances directly,
-> Appplication load balancer had a security group associated with it. So we can place our EC2 in a private subnet and it does not know the actual client talking to it.
So block an IP, we can deploy WAF on application load balancer.


AUTO SCALING GROUPS have the following attributes :
• A launch configuration • AMI + Instance Type • EC2 User Data • EBS Volumes • Security Groups • SSH Key Pair • Min Size / Max Size / Initial Capacity • Network + Subnets Information
• Load Balancer Information ( which load balancer to attach the ASG to) • Scaling Policies (what will scale out and what will scale in)
--> We trigger based on alarms from cloudwatch

EBS VOLUME TYPES : // Only GP2 and IO1 can be used as boot volumes
• EBS Volumes come in 4 types
• GP2 (SSD): General purpose SSD volume that balances price and performance for a wide variety of workloads
• IO1 (SSD): Highest-performance SSD volume for mission-critical low-latency or high- throughput workloads
             I/O optimized instances and provisioned IOPS EBS volumes are more geared towards storage performance than network performance.
• ST1 (HDD): Low cost HDD volume designed for frequently accessed, throughput- intensive workloads
• SC1 (HDD): Lowest cost HDD volume designed for less frequently accessed workloads
--> command lsblk will show the attached volumes .
--> To attach a volume to instance in another AZ, create a snapshot of volume,  then make a volume from that snapshot which asks where to store the volume, specify the required AZ , now attach it to the instance.
    // Snapshot is not attached to an AZ. So to attach volume to instance in another region, copy the snap to required region , then make a volume from it in the required AZ in the required region.

                                          SOLID STATE DRIVES                                                        HARD DISK DRIVES
  ==================================================================================================================================================================
    VOLUME TYPE      General Purpose SSD - GP2           Provisioned IPOS SSD - io1         Throughput Optimized - st1                Cold HDD - SC1
    DESCRIPTION      Balance of price to performance     High performance                   low cost                                  lowest cost
    USE CASES        Systemm boot Volumes                More than 10000 IPOS               Big Data -- Data warehouses               Throughput oriented storage for large volumes
                    virtual desktops                     Large Database workloads           Fast throughput                             - of infrequently acessed data

    VOLUME SIZE      1gib - 16Tib                        4gib - 16Tib                       500gib - 16Tib                             500gib - 16Tib
    MAX IPOS         10000                               32000                              500                                        250
    Max Throughput   160 Mib/s                           500 Mib/s                          500 Mib/s                                  250 Mib/s

    ** Disk throughput is the measurement of how fast (per second) your storage can read/write data.


EBS VS INSTANCE STORE : For Heigher IPOS , say in millions the solution is instance store, not EBS.
---------------------------------------
• Some instance do not come with Root EBS volumes, Instead, they come with “Instance Store” (= ephemeral storage)
• Instance store is physically attached to the INSTANCE (EBS is a network drive)
• Pros:  Better I/O performance (EBS gp2 has an max IOPS of 16000, io1 of 64000) -- Good for buffer / cache / scratch data / temporary content -- Data survives reboots
• Cons:  On stop or termination, the instance store is lost -- You can’t resize the instance store -- Backups must be operated by the user

EBS RAID OPERATIONS : // To get more IOPS or performance with EBS. // 0 and 1 are recommended, 5 and 6 are not.
-------------------------
Amazon EBS volume data is replicated across multiple servers within the same availability zone to prevent the loss of data from the failure of any single component.
This replication makes Amazon EBS volumes more reliable, which means customers who follow the guidance found on the Amazon EBS and Amazon EC2 product detail pages typically achieve good performance out of the box.
However, there are certain scenarios where it is necessary to achieve a higher network throughput with much better IOPS.

One way to accomplish this is by configuring a software level RAID array.
RAID is supported by almost all operating systems and is used to boost the IOPS and network throughput of volumes of EBS in AWS.
Before configuring RAID, it is important to know how large your RAID array should be and how many IOPS are required.
In the most basic terms, configuring a RAID array works by using two volumes as one, using the combined IOPS and network throughput of both volumes.

Things to Remember When Configuring RAID on Amazon EBS:
   Typically, RAIDS types such as 0, 1, 5 and 6 can be configured on AWS.
   As per AWS, the ideal RAID configuration is RAID 0 and 1: the reason for this is because in RAID 0 users can stripe across multiple volumes to gain a better distribution of I/O and therefore better performance.
   In the same way, RAID 1 is used to achieve better availability and fault-tolerant behaviour as two volumes are mirrored, with one acting as backup copy for the other in case of a failure.
   Different types of instance resources such as General Purpose, Provisioned IOPS, Throughput Optimized, Cold HDD volumes can be combined together in a RAID 0 configuration.
   This uses the accumulated available bandwidth of all the volumes to achieve higher throughput and IOPS.

RAID - 0 : // way to increase performance. --> For data [a + b + c + d ],  EBS-1 [ a + b] EBS-2 [ c + d]
   LOGICAL VOLUME to an instance = EBS-1 + EBS-2
   But one disk fails, all the data is failed
   • Use cases would be: An application that needs a lot of IOPS and doesn’t need fault-tolerance, A database that has replication already built-in
   • Using this, we can have a very big disk with a lot of IOPS
   • For example: Two 500 GiB Amazon EBS io1 volumes with 4,000 provisioned IOPS each will create  1000 GiB RAID 0 array with an available bandwidth of 8,000 IOPS and 1,000 MB/s of throughput

RAID - 1 : // To increase fault tolerance. --> For data [a + b + c + d ],  EBS-1 [ a + b + c + d ] EBS-2 [ a + b + c + d] --> EBS-1 is a mirror of EBS-2
   • If one disk fails, our logical volume is still working
   • We have to send the data to two EBS volume at the same time (2x network)
   • Use case: Application that need increase volume fault tolerance -- Application where you need to service disks
   • For example: Two 500 GiB Amazon EBS io1 volumes with 4,000 provisioned IOPS each will create  500 GiB RAID 1 array with an available bandwidth of 4,000 IOPS and 500 MB/s of throughput

EFS elastic File System : // EBS is locked to single AZ, hence EFS but so costly and pay for what you use unlike provisioned disk in EBS.
----------------------------
• Managed NFS (network file system) that can be mounted on many EC2
• EFS works with EC2 instances in multi-AZ
• Highly available, scalable, expensive (3x gp2), pay per use
--> EFS has a lifecycle management : Storage Tiers (lifecycle management feature – move file after N days)
    • Standard: for frequently accessed files
    • Infrequent access (EFS-IA): cost to retrieve files, lower price to store

####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
:::: DATABASE ::::
####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
READ REPLICAS : // ASYNCHRONOUS // RDS Read Replicas for read scalability
----------------
Up to 5 Read Replicas Within AZ, Cross AZ or Cross Region
• Replication is ASYNC, so reads are eventually consistent
• Replicas can be promoted to their own DB
--> Read replicas are used for SELECT (=read) only kind of statements (not INSERT, UPDATE, DELETE)
--> Creating a READ REPLICA in another AZ will cost us money, so create it in the same AZ.

RDS Multi AZ : // SYNCHRONOUS // (Disaster Recovery)
------------------------------------------
One DNS name – automatic app failover to standby --> ONE DNS NAME , So automatic failover to the standby and because it is synchronous.
• Increase availability
• Failover in case of loss of AZ, loss of network, instance or storage failure
• No manual intervention in apps and  Not used for scaling
• Note:The Read Replicas be setup as Multi AZ for Disaster Recovery (DR)
--> Unlike read replicas, no one can read/write to the standby database instance.

RDS ENCRYPTION :
-------------------------
--> Encrypting RDS backups
    • Snapshots of un-encrypted RDS databases are un-encrypted
    • Snapshots of encrypted RDS databases are encrypted
    • Can copy a snapshot into an encrypted one
--> To encrypt an un-encrypted RDS database:
    • Create a snapshot of the un-encrypted database
    • Copy the snapshot and enable encryption for the snapshot
    • Restore the database from the encrypted snapshot
    • Migrate applications to the new database, and delete the old database

AURORA --> Writer & Reader ENdpoints.
An Aurora global database consists of one primary AWS Region where your data is mastered, and up to five read-only, secondary AWS Regions.
Aurora replicates data to the secondary AWS Regions with typical latency of under a second. You issue write operations directly to the primary DB instance in the primary AWS Region.

REDIS      :  • Multi AZ with Auto-Failover -- • Read Replicas to scale reads and have high availability -- • Data Durability using AOF persistence -- • Backup and restore features
MEMCACHED  :  • Multi-node for partitioning of data (sharding) -- • Non persistent  • No backup and restore -- • Multi-threaded architecture
Storing Session Data in ElastiCache is a common pattern to ensuring different instances can retrieve your user's state if needed.
Logging in one application will replicate login credentials to other apps using session data.
// REDIS AUTH

Important ports:  FTP: 21 - SSH: 22 - SFTP: 22 (same as SSH) - HTTP: 80 - HTTPS: 443
RDS Databases ports: PostgreSQL: 5432 - MySQL: 3306 - Oracle RDS: 1521 - MSSQL Server: 1433 - MariaDB: 3306 (same as MySQL) - Aurora: 5432 (if PostgreSQL compatible) or 3306 (if MySQL compatible)

Which database technology does NOT support TDE (Transparent Data Encryption) on RDS? -- PostgreSQL
Which RDS database technology does NOT support IAM authentication? -- Oracle
How can you enhance the security of your Redis cache to force users to enter a password? -- REDIS AUTH
To save costs, enable Read Replicas in the Same AZ.
To create a disaster recovery strategy for your RDS PostgreSQL database so that in case of a regional outage, a database can be quickly made available for Read and Write workload in another region. -- Create a Read Replica in a different Region and enable Multi-Az  on main Db.
PostgreSQL for RDS can be used to obtain short-lived credentials.

REDSHIFT :
-------------
• Redshift is based on PostgreSQL, but it’s not used for OLTP
• It’s OLAP – online analytical processing (analytics and data warehousing)
• 10x better performance than other data warehouses, scale to PBs of data
• Columnar storage of data (instead of row based)
• Leader node: for query planning, results aggregation
• Compute node: for performing the queries, send results to leader
• Redshift Spectrum: perform queries directly against S3 (no need to load)
--> You can configure Amazon Redshift to automatically copy snapshots (automated or manual) of a cluster to another AWS Region
• Query data that is already  S3 without loading it
• Must have a Redshift cluster available to start the query
• The query is then submitted to thousands of Redshift Spectrum nodes

ELASTIC-SEARCH :
--------------------
Example: In DynamoDB, you can only find by primary key or indexes.
• With ElasticSearch, you can search any field, even partially matches
• It’s common to use ElasticSearch as a complement to another database
• ElasticSearch also has some usage for Big Data applications
Comes with Kibana (visualization) & Logstash (log ingestion) – ELK stack

####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
:::: ROUTE 53 ::::
####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
DNS is a collection of rules and records which helps clients understand how to reach a server through URLs.
--> In AWS, the most common records are:
    • A: hostname to IPv4
    • AAAA: hostname to IPv6
    • CNAME: hostname to hostname
    • Alias: hostname to AWS resource.

TTL is the TIME TO LIVE --> The time an Ip for a DNS can be cached by a web browser . Once the TTL expires, web browser needs to again query the DNS server for the IP again.
• High TTL: (e.g. 24hr) --  Less traffic on DNS • Possibly outdated records
• Low TTL: (e.g 60 s)   --  More traffic on DNS • Records are outdated for less time • Easy to change records
• TTL is mandatory for each DNS record

CNAME vs ALIAS :
-----------------
• AWS Resources (Load Balancer, CloudFront…) expose an AWS hostname: lb1-1234.us-east-2.elb.amazonaws.com and you want myapp.mydomain.com
-->  CNAME:
      • Points a hostname to any other hostname. (app.mydomain.com => blabla.anything.com)
      • ONLY FOR NON ROOT DOMAIN (aka. something.mydomain.com)
-->  Alias:
      • Points a hostname to an AWS Resource (app.mydomain.com => blabla.amazonaws.com)
      • Works for ROOT DOMAIN and NON ROOT DOMAIN (aka mydomain.com)
      • Free of charge
      • Native health check

SIMPLE ROUTING POLICY :
• Maps a hostname to another hostname
• Use when you need to redirect to a single resource
• You can’t attach health checks to simple routing policy
• If multiple values are returned, a random one is chosen by the

WEIGHTED ROUTING POLICY ;
• Control the % of the requests that go to specific endpoint
• Helpful to test 1% of traffic on new app version for example
• Helpful to split traffic between two regions
• Can be associated with Health Checks

LATENCY ROUTING POLICY : Redirect to the server that has the least latency close to us
• Super helpful when latency of users is a priority
• Latency is evaluated in terms of user to designated AWS Region
• Germany may be directed to the US (if that’s the lowest latency)

FAILOVER ROUTING POLICY :
We have a primary and a secondary instances that are checked for their health contineously and the requests are routed accordingly.
// Say an instance is terminated etc;

GEOLOCATION ROUTING POLICY :
Different from Latency based!
• This is routing based on user location
• Here we specify: traffic from the UK should go to this specific IP
• Should create a “default” policy (in case there’s no match on location)

MULTI VALUE ROUTING POLICY : // Advanced Simple routing policy
• Use when routing traffic to multiple resources
• Want to associate a Route 53 health checks with records
• Up to 8 healthy records are returned for each Multi Value query
• Multi Value is not a substitute for having an ELB

--> If we have a domain registered using 3rd party say godaddy or Google, then we can change the nameservers to Route53 and create hosted zones and records.
Private hosted zones are meant to be used for internal network queries and are not publicly accessible. Public Hosted Zones are meant to be used for people requesting your website through the public internet.
You have purchased a domain on Godaddy and would like to use it with Route 53. What do you need to change to make this work? --> create a public hosted zone and update the 3rd party registrar NS records.

####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
:::: S3 ::::
####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
The key is the FULL path:
   • s3://my-bucket/my_file.txt                             --> my_file.txt
   • s3://my-bucket/my_folder1/another_folder/my_file.txt   --> my_folder1/another_folder/my_file.txt
   • The key is composed of prefix + object name
   • s3://my-bucket/my_folder1/another_folder/my_file.txt   --> my_folder1/another_folder/ + my_file.txt

• Object values are the content of the body:  Max Object Size is 5TB (5000GB), If uploading more than 5GB, must use “multi-part upload”
--> VERSIONING is done at the BUCKET level.
• Any file that is not versioned prior to enabling versioning will have version “null”
• Suspending versioning does not delete the previous versions

--> Once versioning is enabled and we upload a file with same name , a VERSION-ID is attached to the object that is of the form <fmhglafbglsugewlfgwlkfbef>.
    Deleting an entire object with versioning enabled, DELETE MARKER is tagged to the TYPE COLUMN of the Object and the objects version is of form <slhfgsnvlsgwflwfewfefbwelib>.
    Deleting a specific version of an object will be called a permanent delete .

ENCRYPTION :
---------------
There  are 4 methods of encrypting objects in S3 // SSE for SERVER SIDE ENCRYPTION
We can enable a default encryption for a bucket and all the objects will be encrypted automatically.
We can also encrypt an object after uploading it to the bucket.
SSE-C can only be done using CLI - SDK - S3 REST API

      • SSE-S3: encrypts S3 objects using keys handled & managed by AWS
          Encryption using keys handled & managed by Amazon S3
          Object is encrypted server side
          AES-256 encryption type
          Must set header: “x-amz-server-side-encryption": "AES256"

      • SSE-KMS: leverage AWS Key Management Service to manage encryption keys
          --> We have 3 options , AWS managed Key (AWS/S3), Choose from your KMS master keys, Enter AWS master key ARN (if you want to use a key from another account)
          SSE-KMS: encryption using keys handled & managed by KMS
          KMS Advantages: user control + audit trail
          Object is encrypted server side
          Must set header: “x-amz-server-side-encryption": ”aws:kms"

      • SSE-C: when you want to manage your own encryption keys // can only be done using CLI - SDK - S3 REST API
          User along with object sends keys that S3 has to use to encrypt. The encryption is happening at S3 not on client side, client only provides the key.
          server-side encryption using data keys fully managed by the customer outside of AWS
          Amazon S3 does not store the encryption key you provide
          HTTPS must be used, Encryption key must provided in HTTP headers, for every HTTP request made

      • Client Side Encryption
         customer encrypts data using an SDK , say using S3 ENCRYPTION SDK
         Clients must encrypt data themselves before sending to S3
         Clients must decrypt data themselves when retrieving from S3
         Customer fully manages the keys and encryption cycle

SECURITY :
-------------
An IAM principal can access an S3 object if the user IAM permissions allow it OR the resource policy ALLOWS it AND there’s no explicit DENY
User based     -->  IAM policies - which API calls should be allowed for a specific user from IAM console
Resource Based -->  Bucket Policies - bucket wide rules from the S3 console - allows cross account
Networking     -->  Supports VPC Endpoints (for instances in VPC without www internet)
Auditing       -->  S3 Access Logs can be stored in other S3 bucket && API calls can be logged in AWS CloudTrail
User Security  -->  • MFA Delete: MFA (multi factor authentication) can be required in versioned buckets to delete objects
                    • Pre-Signed URLs: URLs that are valid only for a limited time (ex: premium video service for logged in users)
                    // Example when we click on view a photo (object) while logged in to console to S3.

WEBSITES :
---------------
The website URL will be:  <bucket-name>.s3-website-<AWS-region>.amazonaws.com  OR   <bucket-name>.s3-website.<AWS-region>.amazonaws.com
If you get a 403 (Forbidden) error, make sure the bucket policy allows public reads!
Write an index.html ( not index.html.txt )  and error.html files --> Make the bucket public --> using policy generator , create a policy that allows GetObject for everyone --> Add the policy and save.
goto properties and get the URL for the STATIC WEBSITE HOSTING --> use the URL to access the site --> if a url is misspelt, then error.html is displyed else the website.

<html>
 <body>
  <p><img src="<object>" alt="parakeet"></p>
 </body>
</html>


{
    "Version": "2012-10-17",
    "Id": "Policy1613889700929",
    "Statement": [
        {
            "Sid": "Stmt1613889696074",
            "Effect": "Allow",
            "Principal": "*",
            "Action": "s3:GetObject",
            "Resource": "arn:aws:s3:::dileep147/*"
        }
    ]
}

CORS - Cross Origin Resource Sharing :
-----------------------------------------
To be able to load a webpage located in another bucket or on another server.
We need to add the CORS policy on the second bucket to allow 1st bucket to access its resources.
 --> Available in the Permissions tab at the bottom --> Add CORS permissions.

 An origin is a scheme (protocol), host (domain) and port
• E.g.: https://www.example.com (implied port is 443 for HTTPS, 80 for HTTP)
• Web Browser based mechanism to allow requests to other origins while visiting the main origin
• Same origin       : http://example.com/app1  -------------------  http://example.com/app2
• Different origins : http://www.example.com   -------------------  http://other.example.com
• The requests won’t be fulfilled unless the other origin allows for the requests, using CORS Headers (ex: Access-Control-Allow-Origin)

S3 CONSISTENCY MODEL : // • Note: there’s no way to request “strong consistency”
------------------------
--> Read after write consistency for PUTS of new objects
• As soon as a new object is written, we can retrieve it                           ex: (PUT 200 => GET 200)
• This is true, except if we did a GET before to see if the object existed         ex: (GET 404 => PUT 200 => GET 404) – eventually consistent

--> Eventual Consistency for DELETES and PUTS of existing objects
• If we read an object after updating, we might get the older version              ex: (PUT 200 => PUT 200 => GET 200 (might be older version))
• If we delete an object, we might still be able to retrieve it for a short time   ex: (DELETE 200 => GET 200)

S3 MFA-DELETE :
----------------
To use MFA-Delete, enable Versioning on the S3 bucket
--> You will need MFA to   --  permanently delete an object version -- suspend versioning on the bucket
--> You won’t need MFA for --  enabling versioning  -- listing deleted versions
• Only the bucket owner (root account) can enable/disable MFA-Delete
• MFA-Delete currently can only be enabled using the CLI

S3 ACCESS LOGS :
-------------------
USER >--- Requests ---> BUCKET >--- Access Logs ---> LOGGING BUCKET
NOTE : • Do not set your logging bucket to be the monitored bucket, It will create a logging loop, and your bucket will grow in size exponentially
In properties --> Enable Server logging and specify the bucket that will log.

S3 REPLICATION : // CRR - Cross region Replication && SRR - Same Region Replication
--------------------
Must enable versioning in source and destination -- Buckets can be in different accounts -- Copying is asynchronous -- Must give proper IAM permissions to S3
We find it under S3 --> Bucket --> Management , VERSION ID of source and destination are same for an object after its replication.
After activating, only new objects are replicated (not retroactive)
--> For DELETE operations:
    • If you delete without a version ID, it adds a delete marker, not replicated
    • If you delete with a version ID, it deletes in the source, not replicated
--> There is no “chaining” of replication
    • If bucket 1 has replication into bucket 2, which has replication into bucket 3
    • Then objects created in bucket 1 are not replicated to bucket 3

S3 PRE-SIGNED URLS :
-------------------------
--> Can generate pre-signed URLs using SDK or CLI
    • For downloads (easy, can use the CLI)
    • For uploads (harder, must use the SDK)
• Valid for a default of 3600 seconds, can change timeout with --expires-in [TIME_BY_SECONDS] argument
• Users given a pre-signed URL inherit the permissions of the person who generated the URL for GET / PUT
--> Examples :
    • Allow only logged-in users to download a premium video on your S3 bucket
    • Allow an ever changing list of users to download files by generating URLs dynamically
    • Allow temporarily a user to upload a file to a precise location in our bucket

S3 STORAGE :
--------------
--> Amazon Glacier – 3 retrieval options:
    • Expedited (1 to 5 minutes) for less than 250mb
    • Standard (3 to 5 hours)
    • Bulk (5 to 12 hours)
    • Minimum storage duration of 90 days
--> Amazon Glacier Deep Archive – for long term storage – cheaper:
    • Standard (12 hours)
    • Bulk (48 hours)
    • Minimum storage duration of 180 day

S3 LIFECYCLE RULES :
-----------------------
--> Transition actions: It defines when objects are transitioned to another storage class.
    • Move objects to Standard IA class 60 days after creation
    • Move to Glacier for archiving after 6 months
--> Expiration actions: configure objects to expire (delete) after some time
    • Access log files can be set to delete after a 365 days
    • Can be used to delete old versions of files (if versioning is enabled)
    • Can be used to delete incomplete multi-part uploads
• Rules can be created for a certain prefix (ex - s3://mybucket/mp3/*)
• Rules can be created for certain objects tags (ex - Department: Finance)

S3 PERFORMANCE :
-----------------------
Your application can achieve at least 3,500 PUT/COPY/POST/DELETE and 5,500 GET/HEAD requests per second per prefix in a bucket.
• There are no limits to the number of prefixes. //  anything between BUCKET and FILE is a prefix
• bucket/folder1/sub1/file => /folder1/sub1/
• bucket/folder1/sub2/file => /folder1/sub2/
• bucket/1/file => /1/
• bucket/2/file => /2/
• If you spread reads across all four prefixes evenly, you can achieve 22,000 requests per second for GET and HEAD

S3 performance is limited by KMS.
When you upload, it calls the GenerateDataKey KMS API and When you download, it calls the Decrypt KMS API
• Count towards the KMS quota per second (5500, 10000, 30000 req/s based on region)
• As of today, you cannot request a quota increase for KMS

To increase S3 Performance , use :
WRITING TO S3 :
    Multi-Part upload :  recommended for files > 100MB, must use for files > 5GB
    S3 Transfer Acceleration (upload only) :  Increase transfer speed by transferring file to an AWS edge location which will forward the data to the S3 bucket in the target region . //  Compatible with multi-part upload
READING FROM S3 :
    Parallelize GETs by requesting specific byte ranges,  Better resilience in case of failures , Can be used to speed up downloads
    We can also request for a partial data for example to read only the header of the file.

S3 SELECT & GLACIER SELECT :
--------------------------------
// used to send only a part of data to our application by automatically filtering data at server side .
• Retrieve less data using SQL by performing server side filtering
• Can filter by rows & columns (simple SQL statements), we can not do aggregations and joins
• Less network transfer, less CPU cost client-side

ATHENA :
-------------
Serverless service to perform analytics directly against S3 files . Before we query, we must specify the bucket to where results should be written to.
First we create a table using  a query, then specify the schema and then run queries in ATHENA.
• Uses SQL language to query the files
• Has a JDBC / ODBC driver
• Charged per query and amount of data scanned
• Supports CSV, JSON, ORC, Avro, and Parquet (built on Presto)
• Use cases: Business intelligence / analytics / reporting, analyze & query VPC Flow Logs, ELB Logs, CloudTrail trails, etc...
• Exam Tip: Analyze data directly on S3 => use Athena

S3 OBJECT LOCK & GLACIER VAULT LOCK :
---------------------------------------
S3 OBJECT LOCK :
• Adopt a WORM (Write Once Read Many) model
• Block an object version deletion for a specified amount of time

GLACIER VAULT LOCK :
• Adopt a WORM (Write Once Read Many) model
• Lock the policy for future edits (can no longer be changed)
• Helpful for compliance and data retention

####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
:::: CLI - SDK - IAM ROLES & POLICIES ::::
####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################

//  Linux instances have a default aws cli installed. So you can execute commands directly on the terminal.
Create an Ec2 Linux instance
create a role in IAM with attaching a full access to s3 policy.
Now in EC2 --> Actions --> Security --> Attach a role -> Attach the role created.
Now we can execute commands in Linux instance , say  "aws s3 ls" or " aws s3 mb s3://<bucket-name> "
We can only give one role to an EC2 at a time , but attach 10 policies to that role.

POLICIES :
---------------
Applied to Roles, users, Groups or Objects (ie S3)
Uses JSON format
Describe the level of access applied to an AWS resource by an AWS resource or user
To be more specific Policies either grant or deny the ability to call a specific method on a specific resource in the AWS API.

ROLES :
-----------
Used to delegate access to AWS Resources
Can be assigned to an AWS Resource (ie EC2 instance) or third party accounts (i.e.. another AWS account or SAML 2.0)
Up to 10 policies can be assigned to a role (this is new, it used to be 2)
Roles CANNOT be assigned to Users or Groups // But they can be assigned to users in directory service which is completely different.
Use AWSPolicySimulator to test your policies.

EC2 --> ROLES --> MyFirstRole ---> Policy 1 + Policy 2 + ............ + Policy 10

USERS & GROUPS :
------------------
Users are individual accounts with usernames & passwords; and Access Key ID's and Access Keys
Groups are collections of users accounts ie. Developers, Admins, DevOps
Policies can either be applied to Groups or to individual accounts
Up to 10 policies per group

• Developing and performing AWS tasks against AWS can be done in several ways
• Using the AWS CLI on our local computer
• Using the AWS CLI on our EC2 machines --> Not at all recommended . Use roles instead
• Using the AWS SDK on our local computer
• Using the AWS SDK on our EC2 machines
• Using the AWS Instance Metadata Service for EC2 -->  curl http://169.254.169.254/latest/meta-data/ (need not have a role assigned to get data)

EC2 METADATA : ( "/" at the end is important)
-------------------
• AWS EC2 Instance Metadata is powerful but one of the least known features to developers
• It allows AWS EC2 instances to ”learn about themselves” without using an IAM Role for that purpose.
• The URL is http://169.254.169.254/latest/meta-data
• You can retrieve the IAM Role name from the metadata, but you CANNOT retrieve the IAM Policy.
• Metadata = Info about the EC2 instance
• Userdata = launch script of the EC2 instance

Launch an instance and execute commands "curl http://169.254.169.254/latest/"  --> " curl http://169.254.169.254/latest/meta-data/ " ---> So on .... and explore the directory.
An entry is a value if it is a simple string, and it is a directory if it has an associated "/" at the end.

SDK :
------
• We have to use the AWS SDK when coding against AWS Services such as DynamoDB
• Fun fact… the AWS CLI uses the Python SDK (boto3)
• Good to know: if you don’t specify or configure a default region, then us-east-1 will be chosen by default
• It’s recommend to use the default credential provider chain
--> The default credential provider chain works seamlessly with:
    • AWS credentials at ~/.aws/credentials (only on our computers or on premise)
    • Instance Profile Credentials using IAM Roles (for EC2 machines, etc…)
    • Environment variables (AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY)

EXPONENTIAL BACKOFF :
-------------------------
An API call failure each time will increase the timeout for the next API call for not to overprovision calls to AWS service.
So if the 1st API call has 2ms and it failed, then the 2nd API call will have 4ms timeout and so on....

####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
:::: CLOUDFRONT & AWS-GLOBAL ACCELERATOR ::::
####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
Nearly 210 Point Of Presence Globally as of now.
The following can be acted as an origin for CLOUDFRONT :
    -->  S3 bucket             : • For distributing files and caching them at the edge • Enhanced security with CloudFront Origin Access Identity (OAI) • CloudFront can be used as an ingress (to upload files to S3)
    -->  Custom Origin (HTTP)  : • Application Load Balancer (can have private ec2 that talks to only ELB) • EC2 instance (must be public) • S3 website (must first enable the bucket as a static S3 website) • Any HTTP backend you want
// CloudFront accesses data from an S3 Bucket using the Origin Access Identity (OAI).

CloudFront Geo Restriction :
You can restrict who can access your distribution
    • Whitelist: Allow your users to access your content only if they're in one of the countries on a list of approved countries.
    • Blacklist: Prevent your users from accessing your content if they're in one of the countries on a blacklist of banned countries.

CloudFront vs S3 Cross Region Replication :
--> CloudFront:
    • Global Edge network
    • Files are cached for a TTL (maybe a day)
    • Great for static content that must be available everywhere
--> S3 Cross Region Replication:
    • Must be setup for each region you want replication to happen
    • Files are updated in near real-time
    • Read only
    • Great for dynamic content that needs to be available at low-latency in few regions

CLOOUDFRONT SIGNED URLS / SIGNED COOKIES :
---------------------------------------------
You want to distribute paid shared content to premium users over the world
We can use CloudFront Signed URL / Cookie. We attach a policy with:
        • Includes URL expiration
        • Includes IP ranges to access the data from
        • Trusted signers (which AWS accounts can create signed URLs)
How long should the URL be valid for?
        • Shared content (movie, music): make it short (a few minutes)
        • Private content (private to the user): you can make it last for years

• Signed URL = access to individual files (one signed URL per file)
• Signed Cookies = access to multiple files (one signed cookie for many files)

CloudFront Signed URL vs S3 Pre-Signed URL :
------------------------------------------------
CloudFront Signed URL:                                                             S3 Pre-Signed URL:
• Allow access to a path, no matter the origin                                     • Issue a request as the person who pre-signed the URL
• Account wide key-pair, only the root can manage it                               • Uses the IAM key of the signing IAM principal
• Can filter by IP, path, date, expiration                                         • Limited lifetime
• Can leverage caching features

Unicast IP: one server holds one IP address
Anycast IP: all servers hold the same IP address and the client is routed to the nearest one

AWS GLOBAL ACCELERATOR :
-------------------------------
• Leverage the AWS internal network to route to your application
• 2 Anycast IP are created for your application
• The Anycast IP send traffic directly to Edge Locations
• The Edge locations send the traffic to your application

• Works with Elastic IP, EC2 instances, ALB, NLB, public or private
• Global Accelerator performs a health check of your applications
• Helps make your application global (failover less than 1 minute for unhealthy)
• Great for disaster recovery (thanks to the health checks)
• only 2 external IP need to be whitelisted
• DDoS protection thanks to AWS Shield

AWS Global Accelerator vs CloudFront :
------------------------------------------
• They both use the AWS global network and its edge locations around the world
• Both services integrate with AWS Shield for DDoS protection.
--> CloudFront
      • Improves performance for both cacheable content (such as images and videos)
      • Dynamic content (such as API acceleration and dynamic site delivery)
      • Content is served at the edge
--> Global Accelerator
      • Improves performance for a wide range of applications over TCP or UDP
      • Proxying packets at the edge to applications running in one or more AWS Regions.
      • Good fit for non-HTTP use cases, such as gaming (UDP), IoT (MQTT), or Voice over IP
      • Good for HTTP use cases that require static IP addresses
      • Good for HTTP use cases that required deterministic, fast regional failover

####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
:::: STORAGE EXTRAS ::::
####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
• Snowball cannot import to Glacier directly
• You have to use Amazon S3 first, and an S3 lifecycle policy TO TRANSFER DATA TO GLACIER.
SNOW BALL EDGE = SNOW BALL + SOME COMPUTATIONAL CAPACITY .

FILE GATEWAY : // Backed by S3
• Configured S3 buckets are accessible using the NFS and SMB protocol
• Supports S3 standard, S3 IA, S3 One Zone IA
• Bucket access using IAM roles for each File Gateway
• Most recently used data is cached in the file gateway
• Can be mounted on many servers

     Application <-- NFS --> File Gateway <=== HTTPS ===> AWS

     NOTE :
     --------
     • Using a file gateway means you need Virtualization…
     • Otherwise, you can use a File Gateway Hardware Appliance
     • You can buy it on amazon.com • Helpful for daily NFS backups in small data centers which does not have a virtualization capability.

VOLUME GATEWAY : // Backed by S3 with EBS snaps
• Block storage using iSCSI protocol backed by S3
• Backed by EBS snapshots which can help restore on-premise volumes!
• Cached volumes: low latency access to most recent data
• Stored volumes: entire dataset is on premise, scheduled backups to S3

     Application <-- iSCSI --> Volume Gateway <=== HTTPS ===> AWS >--> EBS Snapshots

TAPE GATEWAY : // Backed by S3 and Glacier
• Some companies have backup processes using physical tapes (!)
• With Tape Gateway, companies use the same processes but in the cloud
• Virtual Tape Library (VTL) backed by Amazon S3 and Glacier
• Back up data using existing tape-based processes (and iSCSI interface)
• Works with leading backup software vendors

   Connect the backup applications to an AWS Storage Gateway using an iSCSI-virtual tape library (VTL)
   Backup Server <-- iSCSI --> | Tape Drive Or Media Changer to  Tape Gateway | <=== HTTPS ===> AWS >--> Archived Tapes stored in Glacier.

AMAZON FSx ( File Server ) is for WINDOWS.
AMAZON FSx for LUSTRE (Linux Cluster) is for Machine Learning, High Performance Computing (HPC) for linux machines.

####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
:::: AWS INTEGRATION & MESSAGING ::::
####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
There are two patterns of application communication
1) Synchronous communications --> application to application //  can be problematic if there are sudden spikes of traffic
2) Asynchronous / Event based --> application to queue to application
    • using SQS: queue model
    • using SNS: pub/sub model
    • using Kinesis: real-time streaming model

SQS : --> Standard & FIFO queues
-------------------------------------
  STANDARD : Limitation of 256KB per message sent
             Can have duplicate messages (at least once delivery, occasionally)
             Can have out of order messages (best effort ordering
  FIFO     : First-in -- First-out delivery
             Exactly one processing

    PRODUCER -----------------> [[ SQS QUEUE ]] --------------------> CONSUMER ------ Process & Delete --> Deleted in QUEUE
                                      ^                                v
                  (After Timeout) ->  |__ Only process & not delete ___|

The message is persisted in SQS until a consumer deletes it and Message retention: default 4 days, up to 14 days
consumers Poll SQS for messages (receive up to 10 messages at a time) and they should delete the message else they will be persisted in the queue.
Create a queue --> Click send/recieve messages --> send a message --> Below poll for a message !!!
until and unless you delete the message , the message keeps on appearing in the queue. so delete it after you read/process it.

SQS MESSAGE VISIBILITY TIMEOUT :
-----------------------------------
After a message is polled by a consumer, it becomes invisible to other consumers. By default, the “message visibility timeout” is 30 seconds.
That means the message has 30 seconds to be processed and after the message visibility timeout is over, the message is “visible” in SQS.
If a message is not processed within the visibility timeout, it will be processed twice so A consumer can call the ChangeMessageVisibility API to get more time to process.
• If visibility timeout is high (hours), and if the consumer crashes, re-processing will take time
• If visibility timeout is too low (seconds), we may get duplicates

SQS DEAD LETTER QUEUES :
------------------------------
If a consumer fails to process a message within the Visibility Timeout the message goes back to the queue!
We can set a threshold of how many times a message can go back to the queue and After the MaximumReceives threshold is exceeded, the message goes into a dead letter queue (DLQ).
• Useful for debugging!
• Make sure to process the messages in the DLQ before they expire : Good to set a retention of 14 days in the DLQ
--> Create a Queue and then specify this queue as a dead letter queue to all the other queues using their properties available at the bottom.

SQS DELAY QUEUES :
-----------------------
Delay a message (consumers don’t see it immediately) up to 15 minutes default is 0 seconds (message is available right away)
Can set a default at queue level  Can override the default on send using the DelaySeconds parameter .

SQS FIFO QUEUES :
------------------------
Exactly-once send capability (by removing duplicates) and  Messages are processed in order by the consumer.
The name should end with .fifo to work

SQS WITH AUTOSCALING :
------------------------
EC2 Instances poll for messages --> creates a custom metric in cloudwatch [ queue length / number of instances ] --> cloudwatch will create an alarm if a threshold is reached --> this alarm will trigger the autoscaling group accordingly.

SNS - SIMPLE NOTIFICATION SERVICE :
-----------------------------------------
The “event producer” only sends message to one SNS topic and  As many “event receivers” (subscriptions) as we want to listen to the SNS topic notifications and Each subscriber to the topic will get all the messages (note: new feature to filter messages)
Up to 10,000,000 subscriptions per topic, 100,000 topics limit
• Subscribers can be: SQS, HTTP / HTTPS (with delivery retries – how many times), Lambda, Emails, SMS messages, Mobile Notifications
SNS + SQS = Fanout --->   Buying Service --> SNS -> SQS queue 1 --> Fraud services
                                                    SQS Queue 2 --> Shipping service

// SQS , after data is consumed and deleted it is gone
// Kiness , we can replay or reprocess data

KINESIS :
-----------
Kinesis is a managed alternative to Apache Kafka --  Great for application logs, metrics, IoT, clickstreams --  Great for “real-time” big data -- • Great for streaming processing frameworks (Spark, NiFi, etc…)
• Data is automatically replicated to 3 AZ
• Kinesis Streams: low latency streaming ingest at scale
• Kinesis Analytics: perform real-time analytics on streams using SQL
• Kinesis Firehose: load streams into S3, Redshift, ElasticSearch…

[ STREAMS + IOT + LOGS ] ---> Kinesis Streams --> Kinesis Analytics --> Kinesis Firehose ---> [ S3 or RedShift ]

PRODUCERS ---> [shard1 + shard2 + .......] ----> Consumers . // One Consumer per shard.
Streams are divided in ordered Shards / Partitions, we increase or decrease shards as per demand . Once data is inserted in Kinesis, it can’t be deleted (immutability).
A SHARD can be a 1MB/s or 1000 messages/s at write PER SHARD and  2MB/s at read PER SHARD (THROUGHPUT) .  Records are ordered per shard
Billing is per shard provisioned, can have as many shards as you want--> More shards = more bill.

NOW HOW TO SEND DATA TO SHARD ?
Along with data, we send a key which will be hashed to find the right Shard. // Key is hashed to determine shard id
The same key goes to the same partition (helps with ordering for a specific key) and Messages sent get a “sequence number”
Choose a partition key that is highly distributed (helps prevent “hot partition”)
    • user_id if many users
    • Not country_id if 90% of the users are in one country then there will be a hot partition
KCL -- Kinesis Client Library can be used by consumers.  || KPL -- Kinesis Producer Library is used by producers.

Kinesis Firehose --> Near Real time , used to send into redshift - S3 - ElasticSearch - Splunk and only pay for data flowing through firehose and not for provisioning.
--> We can also transform data using LAMBDA before sending data.

Kinesis Data Streams vs Firehose :
--> Streams  -- Going to write custom code (producer / consumer) • Real time (~200 ms) • Must manage scaling (shard splitting / merging) • Data Storage for 1 to 7 days, replay capability, multi consumers
--> Firehose -- Fully managed, send to S3, Splunk, Redshift, ElasticSearch • Serverless data transformations with Lambda • Near real time (lowest buffer time is 1 minute) • Automated Scaling • No data storage

KINESIS vs SQS ORDERING :
-----------------------------
Let’s assume 100 trucks, 5 kinesis shards, 1 SQS FIFO
--> Kinesis Data Streams:
      • On average you’ll have 20 trucks per shard
      • Trucks will have their data ordered within each shard
      • The maximum amount of consumers in parallel we can have is 5
      • Can receive up to 5 MB/s of data
--> SQS FIFO
      • You only have one SQS FIFO queue
      • You will have 100 Group ID
      • You can have up to 100 Consumers (due to the 100 Group ID)
      • You have up to 300 messages per second (or 3000 if using batching)

SQS vs SNS vs KINESIS :
---------------------------
SQS:
• Consumer “pull data”
• Data is deleted after being consumed
• Can have as many workers (consumers) as we want
• No need to provision throughput
• No ordering guarantee (except FIFO queues)
• Individual message delay capability

SNS:
• Push data to many subscribers
• Up to 10,000,000 subscribers
• Data is not persisted (lost if not delivered)
• Pub/Sub
• Up to 100,000 topics
• No need to provision throughput
• Integrates with SQS for fan- out architecture pattern

Kinesis:
• Consumers “pull data”
• As many consumers as we want
• Possibility to replay data
• Meant for real-time big data, analytics and ETL
• Ordering at the shard level
• Data expires after X days
• Must provision throughput

AWS MQ : // managed Apache ActiveMQ -- OpenWire , AMQP, STOMP, MQTT, WSS
------------
• SQS, SNS are “cloud-native” services, and they’re using proprietary protocols from AWS.
• Traditional applications running from on-premise may use open protocols such as: MQTT, AMQP, STOMP, Openwire, WSS
• When migrating to the cloud, instead of re-engineering the application to use SQS and SNS, we can use Amazon MQ
• Amazon MQ doesn’t “scale” as much as SQS / SNS
• Amazon MQ runs on a dedicated machine, can run in HA with failover
• Amazon MQ has both queue feature (~SQS) and topic features (~SNS)

####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
:::: SERVERLESS ::::
####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################

A 3 TIER APPLICATION :
----------------------------
The web server is the presentation tier and provides the user interface. This is usually a web page or web site, such as an ecommerce site where the user adds products to the shopping cart, adds payment details or creates an account. The content can be static or dynamic, and is usually developed using HTML, CSS and Javascript .
The application server corresponds to the middle tier, housing the business logic used to process user inputs. To continue the ecommerce example, this is the tier that queries the inventory database to return product availability, or adds details to a customer's profile. This layer often developed using Python, Ruby or PHP and runs a framework such as e Django, Rails, Symphony or ASP.NET, for example.
The database server is the data or backend tier of a web application. It runs on database management software, such as MySQL, Oracle, DB2 or PostgreSQL, for example.

          USER <====> WEB TIER <====> APPLICATION TIER <=====> DATABASE TIER

Serverless in AWS : • AWS Lambda • DynamoDB • AWS Cognito • AWS API Gateway • Amazon S3 • AWS SNS & SQS • AWS Kinesis Data Firehose • Aurora Serverless • Step Functions • Fargate

LAMBDA : • Node.js (JavaScript) • Python • Java (Java 8 compatible) • C# (.NET Core) • Golang • C# / Powershell • Ruby • Custom Runtime API (community supported, example Rust)
          // Pay per calls + Pay per execution time
You can tell the Lambda runtime which handler method to invoke by setting the handler parameter on your function's configuration.
When you configure a function in Python, the value of the handler setting is the file name and the name of the handler module, separated by a dot.
For example, main.Handler calls the Handler method defined in main.py.

AWS Lambda Limits to Know - per region
• Execution:
    • Memory allocation: 128 MB – 3008 MB (64 MB increments)
    • Maximum execution time: 900 seconds (15 minutes)
    • Environment variables (4 KB)
    • Disk capacity in the “function container” (in /tmp): 512 MB
    • Concurrency executions: 1000 (can be increased)
• Deployment:
    • Lambda function deployment size (compressed .zip): 50 MB
    • Size of uncompressed deployment (code + dependencies): 250 MB
    • Can use the /tmp directory to load other files at startup
    • Size of environment variables: 4 KB

LAMBDA@edge :
--------------
                1                                    2
  USER <---------------------> LAMBDA@edge <----------------> ORIGIN
                4                                    3
  1.  After CloudFront receives a request from a viewer (viewer request)
  2.  Before CloudFront forwards the request to the origin (origin request)
  3.  After CloudFront receives the response from the origin (origin response)
  4.  Before CloudFront forwards the response to the viewer (viewer response)

DYNAMO Db :
----------------
Table must have provisioned read and write capacity units
--> Read Capacity Units (RCU): throughput for reads ($0.00013 per RCU)
    • 1 RCU = 1 strongly consistent read of 4 KB per second
    • 1 RCU = 2 eventually consistent read of 4 KB per second
--> Write Capacity Units (WCU): throughput for writes ($0.00065 per WCU)
    • 1 WCU = 1 write of 1 KB per second
• Option to setup auto-scaling of throughput to meet demand
• Throughput can be exceeded temporarily using “burst credit”, If burst credit are empty, you’ll get a “ProvisionedThroughputException”.
• It’s then advised to do an exponential back-off retry

DynamoDb Accelerator : DAx :
------------------------------
• DAX = DynamoDB Accelerator • Seamless cache for DynamoDB, no application re-write
• Writes go through DAX to DynamoDB • Micro second latency for cached reads & queries • Solves the Hot Key problem (too many reads) • 5 minutes TTL for cache by default
• Up to 10 nodes in the cluster • Multi AZ (3 nodes minimum recommended for production)

DynamoDb Streams :
-------------------
Changes in DynamoDB (Create, Update, Delete) can end up in a DynamoDB Stream
• This stream can be read by AWS Lambda, and we can then do:
• React to changes in real time (welcome email to new users) -- • Analytics -- • Create derivative tables / views -- • Insert into ElasticSearch

DyanmoDB Global Tables : // (cross region replication)
---------------------------
• Active Active replication, many regions
• Must enable DynamoDB Streams
• Useful for low latency, DR purposes

DynamoDB - New Features :
-----------------------------
--> Transactions (new from Nov 2018)
    • All or nothing type of operations
    • Coordinated Insert, Update & Delete across multiple tables
    • Include up to 10 unique items or up to 4 MB of data
--> On Demand (new from Nov 2018)
    • No capacity planning needed (WCU / RCU) – scales automatically
    • 2.5x more expensive than provisioned capacity (use with care)
    • Helpful when spikes are un-predictable or the application is very low throughput

####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
:::: METRICS - MONITORING - AUDIT ::::
####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################

CloudWatch provides metrics for every services in AWS. Metric is a variable to monitor (CPUUtilization, NetworkIn…) AND Metrics belong to namespaces
Dimension is an attribute of a metric (instance id, environment, etc…) and we can have Up to 10 dimensions per metric and Metrics have timestamps. we Can create CloudWatch dashboards of metrics
Dashboards are global and can include graphs from different regions
• EC2 instance metrics have metrics “every 5 minutes”, With detailed monitoring (for a cost), you get data “every 1 minute” . Use detailed monitoring if you want to more prompt scale your ASG!

Possibility to define and send your own custom metrics to CloudWatch, Ability to use dimensions (attributes) to segment metrics
cloudwatch is a Great way to setup dashboards for quick access to keys metrics. Dashboards are global and can include graphs from different regions.
You can change the time zone & time range of the dashboards, You can setup automatic refresh (10s, 1m, 2m, 5m, 15m)

--> 3 dashboards (up to 50 metrics) for free and $3/dashboard/month afterwards
Applications can send logs to CloudWatch using the SDK
• CloudWatch can collect log from: • Elastic Beanstalk: collection of logs from application • ECS: collection from containers
                                   • AWS Lambda: collection from function logs • VPC Flow Logs: VPC specific logs • API Gateway • CloudTrail based on filter
                                   • CloudWatch log agents: for example on EC2 machines • Route53: Log DNS queries
• CloudWatch Logs can go to: • Batch exporter to S3 for archival • Stream to ElasticSearch cluster for further analytics

CloudWatch Logs for EC2 :
--------------------------------
• By default, no logs from your EC2 machine will go to CloudWatch
• You need to run a CloudWatch agent on EC2 to push the log files you want
• Make sure IAM permissions are correct // create a role for ec2 to send logs to cloudwatch
• The CloudWatch log agent can be setup on-premises too

CloudWatch Logs Agent & Unified Agent :
-------------------------------------------
• For virtual servers (EC2 instances, on-premise servers…)
• CloudWatch Logs Agent -- Old version of the agent, Can only send to CloudWatch Logs
• CloudWatch Unified Agent --  Collect additional system-level metrics such as RAM, processes, etc… , • Collect logs to send to CloudWatch Logs , • Centralized configuration using SSM Parameter Store

CloudWatch EVENTS :
----------------------
An event indicates changes in your AWS environment. AWS resources can generate events when their state changes, or you can create rules that self-trigger on an automated schedule.
A rule matches incoming events and routes them to targets for processing. A single rule can route to multiple targets, all of which are processed in parallel.
A target processes events. Targets can include Amazon EC2 instances. AWS Lambda functions, Kinesis streams, Amazon ECS tasks, Amazon SNS topics, Amazon SQS queues, and more.

####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
:::: IDENTITY & ACCESS MANAGEMENT - IAM ::::
####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################

AWS STS – Security Token Service :
---------------------------------------
Allows to grant limited and temporary access to AWS resources. Token is valid for up to one hour (must be refreshed)
If a user wants to assume a ROLE in an account or Cross Account --> Then the user calls the " AssumeRole " api towards STS --> Which calls the IAM to validate --> if valid, STS sends temporary creds to the User.


######################################################
DISASTER RECOVERY :
-----------------------
• RPO: Recovery Point Objective // Data Loss
• RTO: Recovery Time Objective  // Down Time

1. Backup and Restore (High RPO)
2. Disaster Recovery – Pilot Light
    • A small version of the app is always running in the cloud
    • Useful for the critical core (pilot light)
    • Very similar to Backup and Restore
    • Faster than Backup and Restore as critical systems are already up
3. Warm Standby
    • Full system is up and running, but at minimum size
    • Upon disaster, we can scale to production load
4. Multi Site / Hot Site Approach
    • Very low RTO (minutes or seconds) – very expensive
    • Full Production Scale is running AWS and On Premise
5. All AWS Multi Region

On-Premise strategy with AWS :
------------------------------------
• Ability to download Amazon Linux 2 AMI as a VM (.iso format) --> • VMWare, KVM, VirtualBox (Oracle VM), Microsoft Hyper-V
• VM Import / Export  --> • Migrate existing applications into EC2 • Create a DR repository strategy for your on-premise VMs • Can export back the VMs from EC2 to on-premise
• AWS Application Discovery Service -->  • Gather information about your on-premise servers to plan a migration • Server utilization and dependency mappings • Track with AWS Migration Hub

AWS DATA SYNC :
------------------
• Move large amount of data from on- premise to AWS
• Can synchronize to: Amazon S3, Amazon EFS, Amazon FSx for Windows
• Move data from your NAS or file system via NFS or SMB
• Replication tasks can be scheduled hourly, daily, weekly
• Leverage the DataSync agent to connect to your systems

CloudFormation - StackSets :
-----------------------------------
• Create, update, or delete stacks across multiple accounts and regions with a single operation
• Administrator account to create StackSets
• Trusted accounts to create, update, delete stack instances from StackSets
• When you update a stack set, all associated stack instances are updated throughout all accounts and regions.


####################################################
####################################################
EC2 Auto Scaling does not support multiple regions.
The ALB (and NLB) supports IP addresses as targets as well as instance IDs as targets.
You cannot use instance ID based targets for on-premises servers and you cannot mix instance ID and IP address target types in a single target group.
When you create a target group, you specify its target type, which determines how you specify its targets. After you create a target group, you cannot change its target type. Target type is an INSTANCE/IP/LAMBDA
Using IP addresses as targets allows load balancing any application hosted in AWS or on-premises using IP addresses of the application back-ends as targets.
You must have a VPN or Direct Connect connection to enable this configuration to work.
A Classic load balancer can load based on ports only , not IP addresses.
A Network Load Balancer can be configured with a single static IP address (the other types of ELB cannot have static IP's. They have DNS names) for each AZ that can be used by applications as the front-end IP of the load balancer.

The ELB Application Load Balancer can route traffic based on data included in the request including the host name portion of the URL as well as the path in the URL.
Creating a rule to route traffic based on information in the path will work for this solution and ALB works well with Amazon ECS.

An Application Load Balancer is a type of Elastic Load Balancer that can use layer 7 (HTTP/HTTPS) protocol data to make forwarding decisions.
An ALB supports both path-based (e.g. /images or /orders) and host-based routing (e.g. example.com).
In this scenario a single EC2 instance is listening for traffic for each application on a different port.
You can use a target group that listens on a single port (HTTP or HTTPS) and then uses listener rules to selectively route to a different port on the EC2 instance based on the information in the URL path.
So you might have example.com/images going to one backend port and example.com/orders going to a different backend port.
Amazon Route 53 is a DNS service. It can be used to load balance however it does not have the ability to route based on information in the incoming request path.
You cannot use host-based or path-based routing with a CLB.

// STREAMING DATA --> KINESIS  // SQS is not best suited to streaming data
// With SQS FIFIO & No GROUP ID, we can have only one consumer and with sqs standard we do not have ordering.
// With SQS FIFO & GROUP ID, we can scale number of consumers.
Amazon Kinesis Firehose is used for streaming data. With Firehose the data is immediately loaded into a destination that can be Amazon S3, RedShift, Elasticsearch, or Splunk.
This is not an ideal use case for Firehose as this is not streaming data and there is no need to load data into an additional AWS service.
Amazon Kinesis streams allows up to 1 MiB of data per second or 1,000 records per second for writes per shard. There is no limit on the number of shards. and SQS FIFO can have upto 300 messages per second.
Shards in Kinesis Data Streams will have their data ordered within each shard.
SQS is pull-based, not push-based. EC2 instances must poll the queue to find jobs to process.
Using an SQS queue to store the data is not possible as the data needs to be stored long-term and SQS queues have a maximum retention time of 14 days.
Elastic Map Reduce (EMR) is a hosted Hadoop framework and is not used for analytics on streaming data.
Amazon RedShift Spectrum is a feature of Amazon Redshift that enables you to run queries against exabytes of unstructured data in Amazon S3, with no loading or ETL required. However, RedShift nodes run on EC2 instances, so for infrequent queries this will not minimize infrastructure costs.


An Amazon RDS Read Replica is being deployed in a separate region. The master database is not encrypted but all data in the new region must be encrypted. How can this be achieved?
You cannot create an encrypted Read Replica from an unencrypted master DB instance.
You also cannot enable encryption after launch time for the master DB instance.
Therefore, you must create a new master DB by taking a snapshot of the existing DB, encrypting it, and then creating the new DB from the snapshot.
You can then create the encrypted cross-region Read Replica of the master DB.

There are some limitations for encrypted Amazon RDS DB Instances:
you can't modify an existing unencrypted Amazon RDS DB instance to make the instance encrypted, and you can't create an encrypted read replica from an unencrypted instance.
However, you can use the Amazon RDS snapshot feature to encrypt an unencrypted snapshot that's taken from the RDS database that you want to encrypt.
Restore a new RDS DB instance from the encrypted snapshot to deploy a new encrypted DB instance. Finally, switch your connections to the new DB instance.


VPC <=== PEERING CONNECTION ===> VPC
Resource <==== PRIVATE LINK ====> Resource
Resource in a VPC <==== VPC ENDPOINTS [Interface/Gateway] <====> Resources // Gateway for S3 & DynamoDB, it can be added as a route in route table.

SCALING POLICIES :
-------------------------
SCHEDULED : Allows you to set your own scaling schedule for predictable load changes. To configure your Auto Scaling group to scale based on a schedule, you create a scheduled action. This is ideal for situations where you know when and for how long you are going to need the additional capacity.
            Scaling actions are performed automatically as a function of time and date. This will ensure that there are enough EC2 instances to serve the demand and prevent the application from slowing down.
STEP      : Increase or decrease the current capacity of your Auto Scaling group based on a set of scaling adjustments, known as step adjustments. The adjustments vary based on the size of the alarm breach. This is more suitable to situations where the load unpredictable.
            --> Increase or decrease the current capacity of the group based on a set of scaling adjustments, (Step)
SIMPLE    : After a scaling activity is started, the policy must wait for the scaling activity or health check replacement to complete and the cooldown period to expire before responding to additional alarms (in contrast to step scaling). Again, this is more suitable to unpredictable workloads.
            --> Increase or decrease the current capacity of the group based on a single scaling adjustment. (Simple)
            --> AWS recommend using step over simple scaling in most cases.
TRACKING  : If you are scaling based on a utilization metric that increases or decreases proportionally to the number of instances in an Auto Scaling group, we recommend that you use target tracking scaling policies.

EX : A web application is running on a fleet of Amazon EC2 instances using an Auto Scaling Group. It is desired that the CPU usage in the fleet is kept at 40%.
     This is a perfect use case for a target tracking scaling policy. With target tracking scaling policies, you select a scaling metric and set a target value. In this case you can just set the target value to 40% average aggregate CPU utilization.
     A simple scaling policy will add instances when 40% CPU utilization is reached, but it is not designed to maintain 40% CPU utilization across the group.
     The step scaling policy makes scaling adjustments based on a number of factors. The PercentChangeInCapacity value increments or decrements the group size by a specified percentage. This does not relate to CPU utilization.

The cooldown period is a configurable setting for your Auto Scaling group that helps to ensure that it doesn’t launch or terminate additional instances before the previous scaling activity takes effect.
After the Auto Scaling group dynamically scales using a simple scaling policy, it waits for the cooldown period to complete before resuming scaling activities.

An Elastic Fabric Adapter is an AWS Elastic Network Adapter (ENA) with added capabilities.
The EFA lets you apply the scale, flexibility, and elasticity of the AWS Cloud to tightly-coupled HPC apps. It is ideal for tightly coupled app as it uses the Message Passing Interface (MPI).
An Auto Scaling group can contain EC2 instances in one or more Availability Zones within the same Region. However, Auto Scaling groups cannot span multiple Regions.

An active/standby broker is comprised of two brokers in two different Availability Zones, configured in a redundant pair. These brokers communicate synchronously with your application, and with Amazon EFS.
Usually, only one of the broker instances is active at any time, while the other broker instance is on standby. If one of the broker instances malfunctions or undergoes maintenance, it takes Amazon MQ a short while to take the inactive instance out of service. This allows the healthy standby instance to become active and to begin accepting incoming communications. When you reboot a broker, the failover takes only a few seconds.
For an active/standby broker, Amazon MQ provides two ActiveMQ Web Console URLs, but only one URL is active at a time.

When a user requests your content, CloudFront typically serves the requested content regardless of where the user is located.
If you need to prevent users in specific countries from accessing your content, you can use the CloudFront geo restriction feature to do one of the following:
Allow your users to access your content only if they're in one of the countries on a whitelist of approved countries.
Prevent your users from accessing your content if they're in one of the countries on a blacklist of banned countries.

A new version of the AWS Web Application Firewall was released in November 2019.
With AWS WAF classic you create “IP match conditions”, whereas with AWS WAF (new version) you create “IP set match statements”. Look out for wording on the exam.
The IP match condition / IP set match statement inspects the IP address of a web request's origin against a set of IP addresses and address ranges.
Use this to allow or block web requests based on the IP addresses that the requests originate from.
AWS WAF supports all IPv4 and IPv6 address ranges. An IP set can hold up to 10,000 IP addresses or IP address ranges to check.
CloudFront does not sit within a subnet so network ACLs do not apply to it.

A signed URL includes additional information, for example, an expiration date and time, that gives you more control over access to your content.
You can also specify the IP address or range of IP addresses of the users who can access your content.
If you use CloudFront signed URLs (or signed cookies) to limit access to files in your Amazon S3 bucket, you may also want to prevent users from directly accessing your S3 files by using Amazon S3 URLs.
To achieve this you can create an origin access identity (OAI), which is a special CloudFront user, and associate the OAI with your distribution.retain information about each user session
You can then change the permissions either on your Amazon S3 bucket or on the files in your bucket so that only the origin access identity has read permission (or read and download permission).

The AWS Security Token Service (STS) is a web service that enables you to request temporary, limited-privilege credentials for AWS Identity and Access Management (IAM) users or for users that you authenticate (federated users).
AWS Certificate Manager is a service that lets you easily provision, manage, and deploy public and private Secure Sockets Layer/Transport Layer Security (SSL/TLS) certificates for use with AWS services and your internal connected resources.

Cross-origin resource sharing (CORS) is a browser security feature that restricts cross-origin HTTP requests that are initiated from scripts running in the browser.
If your REST API's resources receive non-simple cross-origin HTTP requests, you need to enable CORS support.
A cross-origin HTTP request is one that is made to:
• A different domain (for example, from example.com to amazondomains.com)
• A different subdomain (for example, from example.com to petstore.example.com)
• A different port (for example, from example.com to example.com:10777)
• A different protocol (for example, from https://example.com to http://example.com)

To support CORS, therefore, a REST API resource needs to implement an OPTIONS method that can respond to the OPTIONS preflight request with at least the following response headers mandated by the Fetch standard:
• Access-Control-Allow-Methods
• Access-Control-Allow-Headers
• Access-Control-Allow-Origin


Amazon EKS is a managed service that can be used to run Kubernetes on AWS.
Kubernetes is an open-source system for automating the deployment, scaling, and management of containerized applications.
Applications running on Amazon EKS are fully compatible with applications running on any standard Kubernetes environment, whether running in on-premises data centers or public clouds.
This means that you can easily migrate any standard Kubernetes application to Amazon EKS without any code modification.
This solution ensures that the same open-source software is used for automating the deployment, scaling, and management of containerized applications both on-premises and in the AWS Cloud.

// STICKY SESSIONS -- ELASTICACHE for retain information about each user session
// Amazon CloudFront, API Gateway, Amazon S3, AWS Lambda, DynamoDB --> SERVERLESS PACK
In order to address scalability and to provide a shared data storage for sessions that can be accessible from any individual web server, you can abstract the HTTP sessions from the web servers themselves.
A common solution to for this is to leverage an In-Memory Key/Value store such as Redis and Memcached.
Sticky sessions, also known as session affinity, allow you to route a site user to the particular web server that is managing that individual user’s session.
The session’s validity can be determined by a number of methods, including a client-side cookie or via configurable duration parameters that can be set at the load balancer which routes requests to the web servers. You can configure sticky sessions on Amazon ELBs.

Amazon DynamoDB global tables provide a fully managed solution for deploying a multi-region, multi-master database.
This is the only solution presented that provides an active-active configuration where reads and writes can take place in multiple regions with full bi-directional synchronization.
Amazon Aurora Global Database provides read access to a database in multiple regions – it does not provide active-active configuration with bi-directional synchronization (though you can failover to your read-only DBs and promote them to writable).

AWS Global Accelerator is a service in which you create accelerators to improve availability and performance of your applications for local and global users.
Global Accelerator directs traffic to optimal endpoints over the AWS global network. This improves the availability and performance of your internet applications that are used by a global audience.
Global Accelerator is a global service that supports endpoints in multiple AWS Regions, which are listed in the AWS Region Table.
By default, Global Accelerator provides you with two static IP addresses that you associate with your accelerator.
(Or, instead of using the IP addresses that Global Accelerator provides, you can configure these entry points to be IPv4 addresses from your own IP address ranges that you bring to Global Accelerator.)

// GA will direct users to the closest edge location and then use the AWS global network.
AWS Global Accelerator uses the vast, congestion-free AWS global network to route TCP and UDP traffic to a healthy application endpoint in the closest AWS Region to the user.
This means it will intelligently route traffic to the closest point of presence (reducing latency). Seamless failover is ensured as AWS Global Accelerator uses anycast IP address which means the IP does not change when failing over between regions so there are no issues with client caches having incorrect entries that need to expire.
A Route 53 failover routing policy uses a primary and standby configuration. Therefore, it sends all traffic to the primary until it fails a health check at which time it sends traffic to the secondary. This solution does not intelligently route traffic for lowest latency.
Amazon CloudFront cannot be configured with “a pair of static IP addresses”.

AWS Transit Gateway connects VPCs and on-premises networks through a central hub.
With AWS Transit Gateway, you can quickly add Amazon VPCs, AWS accounts, VPN capacity, or AWS Direct Connect gateways to meet unexpected demand, without having to wrestle with complex connections or massive routing tables. This is the operationally least complex solution and is also cost-effective.

AWS WAF : A rate-based rule tracks the rate of requests for each originating IP address, and triggers the rule action on IPs with rates that go over a limit.
You set the limit as the number of requests per 5-minute time span.
you can use this type of rule to put a temporary block on requests from an IP address that's sending excessive requests.
By default, AWS WAF aggregates requests based on the IP address from the web request origin, but you can configure the rule to use an IP address from an HTTP header, like X-Forwarded-For, instead.
you cannot use AWS WAF with a network load balancer.
To control access at the CloudFront layer the AWS Web Application Firewall (WAF) can be used. With WAF you must create an ACL that includes the IP restrictions required and then associate the web ACL with the CloudFront distribution.


Burstable performance instances provide a baseline of CPU performance with the ability to burst to a higher level when required.
However, the issue in this scenario is disk wait time, not CPU performance, therefore we need to improve I/O not CPU performance.
// CPU Performance --> Burstable Ec2.
// IPOS --> SSD [gp2 -- IO1]

The permissions boundary for an IAM entity (user or role) sets the maximum permissions that the entity can have.
This can change the effective permissions for that user or role. The effective permissions for an entity are the permissions that are granted by all the policies that affect the user or role.
Within an account, the permissions for an entity can be affected by identity-based policies, resource-based policies, permissions boundaries, Organizations SCPs, or session policies.

A VPC automatically comes with a modifiable default network ACL. By default, it allows all inbound and outbound IPv4 traffic. Custom network ACLs deny everything inbound and outbound by default
The most resilient solution is to configure Direct connect connections at multiple Direct connection locations. This ensures that any issues impacting a single DX location do not affect availability of the network connectivity to AWS.

You can only apply one IAM role to a Task Definition so you must create a separate Task Definition.
A Task Definition is required to run Docker containers in Amazon ECS and you can specify the IAM role (Task Role) that the task should use for permissions.
With the EC2 launch type you can apply IAM roles at the container and task level, whereas with Fargate you can only apply at the task level.
IAM roles for ECS tasks enabled you to secure your infrastructure by assigning an IAM role directly to the ECS task rather than to the EC2 container instance.
This means you can have one task that uses a specific IAM role for access to S3 and one task that uses an IAM role to access DynamoDB.

After creating a file system, by default, only the root user (UID 0) has read-write-execute permissions.
For other users to modify the file system, the root user must explicitly grant them access.
One common use case is to create a “writable” subdirectory under this file system root for each user you create on the EC2 instance and mount it on the user’s home directory.
All files and subdirectories the user creates in their home directory are then created on the Amazon EFS file system
--> Create a subdirectory for each user and grant read-write-execute permissions to the users. Then mount the subdirectory to the users’ home directory

AWS DataSync is an online data transfer service that simplifies, automates, and accelerates copying large amounts of data between on-premises systems and AWS Storage services, as well as between AWS Storage services.
DataSync can copy data between Network File System (NFS) shares, or Server Message Block (SMB) shares, self-managed object storage, AWS Snowcone, Amazon Simple Storage Service (Amazon S3) buckets, Amazon Elastic File System (Amazon EFS) file systems, and Amazon FSx for Windows File Server file systems

Amazon FSx for Lustre is ideal for high performance computing (HPC) workloads running on Linux instances.
FSx for Lustre provides a native file system interface and works as any file system does with your Linux operating system.
When linked to an Amazon S3 bucket, FSx for Lustre transparently presents objects as files, allowing you to run your workload without managing data transfer from S3.
This solution provides all requirements as it enables Linux workloads to use the native file system interfaces and to use S3 for long-term and cost-effective storage of output files.
Amazon FSx for Windows File Server service should be used with Windows instances and does not integrate with S3.

AWS Global Accelerator is a service that improves the availability and performance of applications with local or global users.
You can configure the ALB as a target and Global Accelerator will automatically route users to the closest point of presence.
Failover is automatic and does not rely on any client side cache changes as the IP addresses for Global Accelerator are static anycast addresses.
Global Accelerator also uses the AWS global network which ensures consistent performance.

You can specify one of the following when initiating a job to retrieve an archive based on your access time and cost requirements.
Expedited — Expedited retrievals allow you to quickly access your data when occasional urgent requests for a subset of archives are required. For all but the largest archives (250 MB+), data accessed using Expedited retrievals are typically made available within 1–5 minutes. Provisioned Capacity ensures that retrieval capacity for Expedited retrievals is available when you need it.
Standard — Standard retrievals allow you to access any of your archives within several hours. Standard retrievals typically complete within 3–5 hours. This is the default option for retrieval requests that do not specify the retrieval option.
Bulk — Bulk retrievals are S3 Glacier’s lowest-cost retrieval option, which you can use to retrieve large amounts, even petabytes, of data inexpensively in a day. Bulk retrievals typically complete within 5–12 hours.
Vault Lock allows you to easily deploy and enforce compliance controls on individual Glacier vaults via a lockable policy (Vault Lock policy).

AUTHENTICATE : To check if you are a valid user
AUTHORISE    : To check if you can access a particular service
A key-value database is a type of nonrelational (NoSQL) database that uses a simple key-value method to store data. A key-value database stores data as a collection of key-value pairs in which a key serves as a unique identifier. Amazon DynamoDB is a fully managed NoSQL database service that provides fast and predictable performance with seamless scalability
A VPG is used to setup an AWS VPN which you can use in combination with Direct Connect to encrypt all data that traverses the Direct Connect link. This combination provides an IPsec-encrypted private connection that also reduces network costs, increases bandwidth throughput, and provides a more consistent network

POLICY :
---------------
Only 1 policy block.
only 1 statement block but can have multiple arrays
Effect can have only 1 value , either allow or deny.
version can be either 2008-10-17 or 2012-10-17 ,  by default it is 2008-10-17 if not specified.

{
 VERSION   : 2008-10-17  // or 2012-10-17
 STATEMENT : [
               {
                 EFFECT   : Allow //  or Deny
                 ACTION   : [            ]
                 RESOURCE : [            ]
               }
               {
                 EFFECT   : Allow //  or Deny
                 ACTION   : [            ]
                 RESOURCE : [            ]

                 PRINCIPAL    : [        ] // The below are available with latest policy version , along with policy variables
                 NOTPRINCIPAL : [        ]
                 NOTACTION    : [        ]
                 CONDITION    : [        ] // To specufy when can this policy take effect.

               }
 ]
}

Aurora Replicas are independent endpoints in an Aurora DB cluster, best used for scaling read operations and increasing availability. Up to 15 Aurora Replicas can be distributed across the Availability Zones that a DB cluster spans within an AWS Region. To increase availability, you can use Aurora Replicas as failover targets. That is, if the primary instance fails, an Aurora Replica is promoted to the primary instance.
An Amazon Aurora DB cluster consists of a DB instance, compatible with either MySQL or PostgreSQL, and a cluster volume that represents the data for the DB cluster, copied across three Availability Zones as a single, virtual volume. The DB cluster contains a primary instance and, optionally, up to 15 Aurora Replicas. A DB cluster does not necessarily scale read operations as it is optional to deploy Aurora Replicas
A cluster volume manages the data for DB instances in a DB cluster and does not provide read scaling.
Amazon Aurora Global Database is not suitable for scaling read operations within a region. It is a new feature in the MySQL-compatible edition of Amazon Aurora, designed for applications with a global footprint. It allows a single Aurora database to span multiple AWS regions, with fast replication to enable low-latency global reads and disaster recovery from region-wide outages.

Amazon CloudTrail can be used to log activity on the reports.
The key difference between the two answers that include CloudTrail is that one references data events whereas the other references management events.
Data events provide visibility into the resource operations performed on or within a resource. These are also known as data plane operations. Data events are often high-volume activities.
Example data events include:
    • Amazon S3 object-level API activity (for example, GetObject, DeleteObject, and PutObject API operations).
    • AWS Lambda function execution activity (the Invoke API).
Management events provide visibility into management operations that are performed on resources in your AWS account. These are also known as control plane operations. Example management events include:
    • Configuring security (for example, IAM AttachRolePolicy API operations)
    • Registering devices (for example, Amazon EC2 CreateDefaultVpc API operations).

Both S3 and EFS offer concurrent access from many EC2 Linux instances, but EFS is not the most cost-effective solution.
S3 is object storage while EFS is file storage .

Aurora cluster volumes automatically grow as the amount of data in your database increases.
An Aurora cluster volume can grow to a maximum size of 64 tebibytes (TiB). Table size is limited to the size of the cluster volume. That is, the maximum table size for a table in an Aurora DB cluster is 64 TiB.
Aurora Replicas are independent endpoints in an Aurora DB cluster, best used for scaling read operations and increasing availability.
Up to 15 Aurora Replicas can be distributed across the Availability Zones that a DB cluster spans within an AWS Region.
The DB cluster volume is made up of multiple copies of the data for the DB cluster.
However, the data in the cluster volume is represented as a single, logical volume to the primary instance and to Aurora Replicas in the DB cluster.
As a result, all Aurora Replicas return the same data for query results with minimal replica lag—usually much less than 100 milliseconds after the primary instance has written an update.
Replica lag varies depending on the rate of database change. That is, during periods where a large amount of write operations occur for the database, you might see an increase in replica lag.
Amazon Aurora Serverless is an on-demand, auto-scaling configuration for Amazon Aurora. The database automatically starts up, shuts down, and scales capacity up or down based on application needs.
This is an ideal database solution for infrequently-used applications.


EC2 STATUS CHECKS ON EBS :
----------------------------
The possible values are ok, impaired, warning, or insufficient-data.
If all checks pass, the overall status of the volume is ok.
If the check fails, the overall status is impaired. If the status is insufficient-data, then the checks may still be taking place on your volume at the time.

You can use Route 53 to check the health of your resources and only return healthy resources in response to DNS queries. There are three types of DNS failover configurations:
1. Active-passive: Route 53 actively returns a primary resource. In case of failure, Route 53 returns the backup resource. Configured using a failover policy.
2. Active-active: Route 53 actively returns more than one resource. In case of failure, Route 53 fails back to the healthy resource. Configured using any routing policy besides failover.
3. Combination: Multiple routing policies (such as latency-based, weighted, etc.) are combined into a tree to configure more complex DNS failover.

// Customer vpc [[ Instances + Endpoint Network Interface ]] -----------------> Your VPC [[ API gateway --> API's ]]
You can create your own application in your VPC and configure it as an AWS PrivateLink-powered service (referred to as an endpoint service).
Other AWS principals can create a connection from their VPC to your endpoint service using an interface VPC endpoint.
You are the service provider, and the AWS principals that create connections to your service are service consumers.
This configuration is powered by AWS PrivateLink and clients do not need to use an internet gateway, NAT device, VPN connection or AWS Direct Connect connection, nor do they require public IP addresses.
Another option is to use a VPC Peering connection.
A VPC peering connection is a networking connection between two VPCs that enables you to route traffic between them using private IPv4 addresses or IPv6 addresses.
Instances in either VPC can communicate with each other as if they are within the same network. You can create a VPC peering connection between your own VPCs, or with a VPC in another AWS account.


You can use MFA with a Cognito user pool (not in IAM) .
A user pool is a user directory in Amazon Cognito. With a user pool, your users can sign in to your web or mobile app through Amazon Cognito.
Your users can also sign in through social identity providers like Facebook or Amazon, and through SAML identity providers.

Amazon FSx for Lustre provides a high-performance file system optimized for fast processing of workloads such as machine learning, high performance computing (HPC), video processing, financial modeling, and electronic design automation (EDA).
Amazon FSx works natively with Amazon S3, letting you transparently access your S3 objects as files on Amazon FSx to run analyses for hours to months.
Amazon FSx for Windows File Server provides a fully managed native Microsoft Windows file system so you can easily move your Windows-based applications that require shared file storage to AWS.
This solution integrates with Windows file shares, not with Amazon S3.
EFS and EBS are not good use cases for this solution. Neither storage solution is capable of presenting Amazon S3 objects as files to the application.

Uploading using a pre-signed URL allows you to upload the object without having any AWS security credentials/permissions. '
Pre-signed URLs can be generated programmatically and anyone who receives a valid pre-signed URL can then programmatically upload an object. This solution bypasses the web server avoiding any performance bottlenecks.

BASTON HOST :
----------------
A bastion host is a server whose purpose is to provide access to a private network from an external network, such as the Internet.
Because of its exposure to potential attack, a bastion host must minimize the chances of penetration.
Say if we have an instance in a private subnet, then we can use a baston host in a public subnet to get access to the instance in the private subnet

NAT Instance  (OLD)                                            vs     NAT Gateway  (LATEST)
---------------------------------------------------------------------------------------------------------
Managed by you (e.g. software updates)                                 Managed by AWS
Scale up (instance type) manually and use enhanced networking          Elastic scalability up to 45 Gbps

No high availability – scripted/auto-scaled HA possible                Provides automatic high availability within an AZ and can be
using multiple NATs in multiple subnets                                placed in multiple AZs

Need to assign Security Group                                          No Security Groups
Can use as a bastion host                                              Cannot access through SSH
Use an Elastic IP address or a public IP address with a                Choose the Elastic IP address to associate with a NAT gateway at creation
NAT instance

Can implement port forwarding through manual                           Does not support port forwarding
customisation

When to use ENI: This is the basic adapter type for when you don’t have any high-performance requirements. Can use with all instance types.
When to use ENA: Good for use cases that require higher bandwidth and lower inter-instance latency. Supported for limited instance types (HVM only).
When to use EFA: High Performance Computing. MPI and ML use cases. Tightly coupled applications. Can use with all instance types.
