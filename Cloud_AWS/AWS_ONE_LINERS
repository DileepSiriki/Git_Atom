CLOUD :  [ Define 4 lines - Uses - Types - Virtualization - Hypervisor & Types]
---------
  DEFINE :
    On-demand - IT resources compute DB n/w apps - public / private - pay as you go -- Buy maintain own - from vendors like AWS & Azure
    Platform independent - no downloaded software
    Small to large - backup / recovery / email / VM / web apps / big data analytics
    hospitals - finance teams - gaming companies

  USES :
    A agile - c cost effective - D deploy global in sec - E elastic - F trade capital expense for variable expenses - G no engineers for maintenance
    Guessing capacity - no running data centres - pay as you go - serverless - scalable , shared responsibility.

  ADVANTAGES :
  Trade capital for variable expense, Benefit from massive economies of scale, Stop guessing capacity, Increase speed and agility, Stop spending money on running and maintaining data centers, Go global in minutes.

  TYPES          : DEPLOYMENT MODELS :  Public Private Hybrid Community --- SERVICE MODELS : IAAS Admins, PAAS Developers, SAAS Users & Diagram
  VIRTUALIZATION : Technique of sharing 1 resource among many by assigning a logical name to resource and providing a pointer to physical resource when demanded.
                   Server, Application, Hardware virtualization
  HYPERVISOR     : Guest machines, Type-1 & Type-2 [ Bare metal & Hosted ] - Diagram

*****************************************************************************************************************************************************************************************
*****************************************************************************************************************************************************************************************

AMAZON WEB SERVICES : ~25 regions & ~80 AZ's --  175 services over 190 countries - 5 pillars P R O C S  [security/shared responsibility ]
-----------------------
Secure cloud service offering a broad set of global cloud-based products. - ACDEFGS from Cloud usages

  5 PILLARS - PROCS :
  -----------------------

    Performance Efficiency - >   // Horizontal & Vertical Scaling & Serverless architecture -- Experiment more often
    The performance efficiency pillar includes the ability to use computing resources efficiently to meet system requirements.
    Key topics include selecting the right resource types and sizes based on workload requirements, monitoring performance, and making informed decisions to maintain efficiency as business needs evolve.
    On how you can run services efficiently and scalably.

    Reliability            - >  // Availability Zone & Region -- Stop guessing about capacity
    The reliability pillar includes the ability of a system to recover from infrastructure or service disruptions, dynamically acquire computing resources to meet demand, and mitigate disruptions such as misconfigurations or transient network issues.
    A resilient workload quickly recovers from failures to meet business and customer demand.
    Key topics include distributed system design, recovery planning, and how to handle change.
    On how you can build services that are resilient to both service and infrastructure disruptions.

    Operational excellence - >   // Automation to escape human errors - Cloud formation to automate -- Anticipate failure -- Make frequent, small, reversible changes
    The operational excellence pillar includes the ability to run and monitor systems to deliver business value and to continually improve supporting processes and procedures.
    Key topics include automating changes, responding to events, and defining standards to manage daily operations.
    On how you can continuously improve your ability to run systems, create better procedures, and gain insights.

    Cost optimization      - >   // cost explorer - Budgets - Cost usage
    On the ability to avoid or eliminate unneeded cost or sub-optimal resources. CAPEX to OPEX

    Security               - >  Shared Model // Policies (IAM) , Network Security (VPC) , Encryption (KMS), IAM, KMS, MFA, Cloud Trail, Cloud Watch, SNS, Email. -- Enable traceability
    The security pillar includes the ability to protect information, systems, and assets while delivering business value through risk assessments and mitigation strategies.
    Key topics include confidentiality and integrity of data, identifying and managing who can do what with privilege management, protecting systems, and establishing controls to detect security events.

[ DIAGRAM ] --> Regions - Availability Zones - Local Zone (Similar to regions but closer to users), available only in 3 places - Outposts (Cloud in On-premises)- Wavelengths

*****************************************************************************************************************************************************************************************
*****************************************************************************************************************************************************************************************

-------------------------------------------------------------------------------------------------------------------------------------------------
SERVICES COVERED :
-------------------------------------------------------------------------------------------------------------------------------------------------
Analytics                 |  Compute             |   Data Base              |  Management & Governance       |  Satellite
Application Integration   |  Containers          |   Developer Tools        |  Migration & Transfer          |  Security, Identity & Compliance
AR & VR                   |  Cryptography & PKI  |   End User Computing     |  Networking & Content Delivery |  Storage
Billing & Cost Management |  Customer Enablement |   Front-End Web & Mobile |  Quantum Computing             |
Business Applications     |  Customer Engagement |   Game Development       |  Robotics                      |
Blockchain                |                      |                          |                                |

-------------------------------------------------------------------------------------------------------------------------------------------------
SERVICES NOT COVERED :
-------------------------------------------------------------------------------------------------------------------------------------------------
Internet of Things (IoT)  |
Machine Learning          |
Media Services            |


**********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************
**********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************
:::: ANALYTICS ::::
**********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************
**********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************

1. APP-FLOW       : Bidirectional data transfer from SAAS Applications ( Slack or Sales Force ) to AWS ( S3 or Redshift ) for analytics and archiving and automatic workflow.
2. DATA EXCHANGE  : To securely exchange file-based data sets in the AWS Cloud from qualified data providers and then use the data across a variety of AWS analytics and machine learning service. AWS Data Exchange scans all data published by providers before it is made available to subscribers. Used by colleges, hospitals , scientists to get data from various data lakes .
3. DATA PIPELINE  : To automate the movement and transformation of data across AWS compute and storage resources, as well as your on-premises resources. You upload your pipeline definition to the pipeline, and then activate the pipeline and it schedules and runs tasks by creating Amazon EC2 instances to perform the defined work activities. For example, Task Runner could copy logfiles to Amazon S3 and launch Amazon EMR clusters.
                    Task Runner polls for tasks in pipeline and then performs those tasks. You can define data-driven workflows, so that tasks can be dependent on the successful completion of previous tasks.

4. AWS GLUE            : Fully managed ETL (extract, transform, and load) service used to categorize your data, clean it, enrich it, and move it reliably between various data stores and data streams.
                         It is used to combine data [ Prepare a data lake ] from various sources and clean , normalize and prepare data to one common syntax and save to S3. EXTRACT from various DB's [CSV/JSON/XML]-------------> [[ TRANSFORM to common DB ]] ----------------> LOAD to another DB .
                         Automatically identify type of data in S3/Redshift --> then provides a unified view of data as catalog that can be used with EMR/ATHENA. Glue automatically generates Scala or Python code for your ETL jobs that you can further customize.
5. AWS LAKE FORMATION  : The data lake is your persistent data that is stored in Amazon S3 and managed by Lake Formation using a Data Catalog. AWS Lake Formation is a fully managed service that makes it easier for you to build, secure, and manage data lakes.
                         Collect -> Clean -> Move to S3 -> make data available for Analytics. // Lake Formation simplifies and automates many of the complex manual steps that are usually required to create data lakes. These steps include collecting, cleansing, moving, and cataloging data, and securely making that data available for analytics and machine learning. Once a datalake is formed we can use EMR/Athena/Redshift for analytics.
6. AWS QUCIK SIGHT     : Combines AWS data + 3rd party data + SaaS data --> Combine --> generate catalogs. // It is a cloud-scale business intelligence (BI) service that you can use to deliver easy-to-understand insights to the people who you work with, wherever they are. It connects to your data in the cloud and combines data from many different sources.
                         In a single data dashboard, QuickSight can include AWS data, third-party data, big data, spreadsheet data, SaaS data, B2B data, and more.

7. ATHENA              : Query service that makes it easy to analyze Structured, Unstructured, Semi Structured data (CSV, columnar, standard or JSON format) in Amazon S3 using standard SQL then store results from queries directly into another bucket in S3 or download then to local. Login to console, define your schema, and start querying directly on S3 buckets.
8. AWS KINESIS         : To collect, process, and analyze real-time, streaming data such as video, audio, application logs, website clickstreams, and IoT telemetry data for machine learning, analytics, and other applications. You can use Amazon Kinesis services for real-time application monitoring, fraud detection, and live leader-boards.
                         KINESIS STREAMS --> Collect and store data streams     // KINESIS FIREHOSE --> Process and deliver data streams //  KINESIS ANALYTICS --> Analyze streaming data and get insights.
9. ELASTIC MAP REDUCE  : Apache Spark and apache Hadoop are big data frameworks used to store data from a datalake [ purchase info - social media - server logs ] similar to S3 ad process those logs. Maintaining them is costly so EMR does this for  us . Master node - Core Node - Task Node . Amazon EMR launches clusters in minutes.
    // EMR               Upload data to S3 --> EMR launches cluster with specified EC2 instances --> pull data from S3 into EC2 --> Store output in S3 --> Delete cluster. // EMR uses Hadoop engine and runs Hadoop software on the instances. The central component of Amazon EMR is the cluster. A cluster is a collection of Amazon Elastic Compute Cloud (Amazon EC2) instances. Each instance in the cluster is called a node. Each node has a role within the cluster, referred to as the node type.
                         The customer implements their algorithm in terms of map() and reduce() functions. Clusters comprises of one master and multiple other nodes. The master node divides input data into blocks, and distributes the processing of the blocks to the other nodes. Each node runs MAP and REDUCE functions to break and join the data at the end.
10. AMAZON REDSHIFT    : Petabyte-scale cloud data warehouse for only relational data that analyses all your data using standard SQL.
    // SPECTRUM          It is specifically designed for online analytic processing (OLAP) and business intelligence (BI) applications, which require complex queries against large datasets.
                         Amazon Redshift also includes Amazon Redshift Spectrum, allowing you to run SQL queries directly against exabytes of unstructured data in Amazon S3 data lakes.

You should use Amazon EMR if you use custom code to process and analyze extremely large datasets with big data processing frameworks such as Apache Spark, Hadoop, Presto, or Hbase. Amazon EMR gives you full control over the configuration of your clusters and the software you install on them.
Data warehouses like Amazon Redshift are designed for a different type of analytics altogether.
Data warehouses are designed to pull together data from lots of different sources, like inventory, financial, and retail sales systems.
In order to ensure that reporting is consistently accurate across the entire company, data warehouses store data in a highly structured fashion.
structure builds data consistency rules directly into the tables of the database.
Amazon Redshift is the best service to use when you need to perform complex queries on massive collections of structured and semi-structured data and get fast performance.

11. CLOUD SEARCH        : Add search capabilities to your website or application. Upload all data and documents and it will create a search index. Autocomplete suggestions - Geospatial search - Highlighting - Support for 34 languages.
12. ELASTIC SEARCH      : Log stash is an open source tool for collecting, parsing, and storing logs for future use. Kibana 3 is a web interface that can be used to search and view the logs that Logstash has indexed. Both of these tools are based on Elasticsearch. Elasticsearch, Logstash, and Kibana, when used together is known as an ELK stack.
    ( ELK Stash)          Used to analyse and get real time  insights on machine generated data at a peta byte scale by deploying, operating, and scaling Elasticsearch clusters in the AWS Cloud.
13. AMAZON  MANAGED STREAMING FOR KAFKA - MSK : Managed Streaming for Apache Kafka that enables you to build and run applications that use Apache Kafka to process streaming data.

// BIGDATA HADOOP SPARK - ELASTIC SEARCH, LOG STATSH, KIBANA - KAFKA

**********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************
**********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************
:::: CONTAINERS ::::
**********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************
**********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************

DOCKER FILE  is a text document that contains commands that are used to assemble an image.  Docker builds images automatically by reading the instructions from the Dockerfile. DOCKER image is a read-only template with instructions for creating a Docker container.
DOCKER packages software into standardized units called CONTAINERS . They have everything your software needs to run including Libraries , system tools , code and run time .
DOCKER FILE --> docker build ---> DOCKER IMAGE -- docker run ---> DOCKER CONTAINER.
$ docker build  /path/to/a/Dockerfile  --> To build an image.
$ docker run /path/to/docker_image     --> To create a container.

Elastic container Registry - Elastic container Service -
1. ECR  : It is a fully managed Docker container registry that makes it easy for developers to store, manage, and deploy DOCKER CONTAINER IMAGES. Supports both private / public container image repositories.
          Amazon ECR integrates with Amazon ECS, Amazon EKS, AWS Fargate, AWS Lambda, and the Docker CLI, allowing you to simplify your development and production workflows. Uses S3 for storing.
2. ECS  : Container service that makes it easy to run, stop, and manage containers on a cluster. You can create Amazon ECS clusters within a new or existing VPC.
          After a cluster is up and running, you can create task definitions that define which container images run across your clusters. The task definition is a text file (in JSON format) that describes one or more containers ( up to a maximum of 10 ) that form your application.
          The task definition can be thought of as a blueprint for your application that indicate which containers should be used, which ports should be opened for your application, and what data volumes should be used with the containers in the task.
3. EKS  : KUBERNETIS also known as K8s, is an open-source system for automating deployment, scaling, and management of containerized applications. It groups containers that make up an application into logical units for easy management and discovery.
          It s a managed service that you can use to run Kubernetes on AWS without needing to install, operate, and maintain your own Kubernetes control plane or nodes. Amazon EKS automatically detects and replaces unhealthy control plane instances, and it provides automated version upgrades and patching for them.

// EC2          : Deploy and manage your own cluster of EC2 instances for running the containers - For consistent demand of CPU cores.
                  With the EC2 launch type billing is based on the cost of the underlying EC2 instances. This allows you to optimize price by taking advantage of billing models such as spot instances (bid a low price for an instance), or reserved instances .
// FARGATE      : Run containers directly, without any EC2 instance - Need not manage compute - for tiny and infrequent workloads.
                  If your workload is small with the occasional burst, such as a website that has traffic during the day but low traffic at night, then AWS Fargate is a fantastic choice.

4. FARGATE       : A serverless compute engine for containers that works with both ECS and EKS. Fargate allocates the right amount of compute, eliminating the need to choose instances and scale cluster capacity unlike ECS.
                   Fargate removes the need to provision and manage servers, lets you specify and pay for resources per application, and improves security through application isolation by design.
5. APP2CONTAINER : It is a command-line tool for modernizing .NET and Java applications into containerized applications. Using A2C simplifies your migration tasks by performing inventory and analysis of your existing applications, creating Docker containers that include your application dependencies, and generating deployment templates.
                   After you have reviewed your templates, A2C helps you register your containers to Amazon ECR, deploy to Amazon ECS or Amazon EKS, and build CI/CD pipelines using AWS CodeStar.


**********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************
**********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************
:::: STORAGE :::: // Object S3 - Block EBS  - File EFS, FSx
**********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************
**********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************
1. AWS S3 : 11 9's Durable - Backup Storage for internet - Static websites, FTP apps - virtually unlimited storage - media hosting - software delivery.... // Permissions - Versioning - Replication - Server side & client side encryption - Macie
            Amazon S3 stores data as objects within buckets. An object is a file and any optional metadata that describes the file. Buckets are containers for objects. Max 100 buckets per account, max 5tb per object and unlimited objects per bucket. // OBJECT : Photos/puppy.jpg -- BUCKET : Dileep --> URL : http://Dileep.s3.aws.com/photos/puppy.jpg . Data is stored a key value pairs. Key is the name of the object and the value is the actual content of file.
            S3 Cross-Region Replication (CRR) to replicate buckets into same or different regions using S3 Replication Time Control (S3 RTC) to replicate within 15minutes.
            1. FOR FREQUENTLY ACCESSED OBJECTS   -->  S3 standard ( A-99.99% )& Reduced Redundancy (for reproduceable data, least durable of all classes ) --> Ideal for cloud applications, dynamic websites, content distribution, mobile and gaming applications, and big data analytics.
            2. FOR AUTOMATICALLY OPTIMIZING DATA WITH CHANGING PATTERNS --> S3 Intelligent Tiering ( A-99.9%, min 128kb ) --> 30 (to infrequent), 90 (to archive), 180 (to deep archive) --> Ideal for new applications and data lakes
            3. FOR INFREQUENTLY ACCESSED OBJECTS --> S3 Standard IA ( A-99.9% ) & S3 One zone IA ( A-99.5% , stores data in a single AZ and costs 20% less, not resilient to AZ destruction ) --> Ideal for long-term storage, backups, and as a data store for disaster recovery files.
            4. FOR ARCHIVING OBJECTS --> S3 Glacier (A-99.99%, 1-5 mnt retrieval, 90days) & S3 Glacier Deep Archive (A-99.99%, 12 hrs retrieval, 180 days) --> Ideal for long-term archive. and the Glacier deep is Ideal for those in highly-regulated industries, such as the Financial Services, Healthcare, and Public Sectors — that retain data sets for 7-10 years or longer to meet regulatory compliance requirements.
            PRICE : Standard [ 0.02300 0.02200 0.02100 ] - Infrequent [ 0.0125 0.01000 ] - glacier [ 0.00400 0.00099]
            S3 TRANSFER ACCELERATION - Transferring your data to popular AWS storage platform S3 over long distances, AWS S3 Transfer Acceleration helps you do it faster 171% faster over the public Internet.

2. ELASTIC BLOCK STORE : It is a hard drive to EC2 instances - Persists its data even after termination of an instance - EBS can be connected to only one Instance at a time - But one instance can be connected more than one EBS .
                         As a primary storage device for data that requires frequent and granular updates.
    ( HDD - SDD )        Detach and attach to different instance - change configuration any time - independent of lifecycle of instance - can encrypt and take periodic incremental snapshots.

3. STORAGE GATEWAY     : Hybrid cloud storage service that seamlessly connect on-premises applications to cloud storage, caching data locally for low-latency access. This gives you on-premises access to virtually unlimited cloud storage.
                         Download the virtual appliance gateway or purchase the hardware appliance, configure required storage, and manage and monitor the service. Once configured, you start using the gateway to write and read data to and from AWS storage. You can monitor the status of your data transfer and your storage interfaces through the AWS Management Console.
                         Storage GW can be a --> VM on premises - EC2 instance - Hardware device.
                         1. FILE GW   -> retrieve objects in S3 using Network File System - NFS (linux) / Server Messaging protocol- SMB (Windows), GW deployed on a VM running on Hyper-V / ESXi / KVM
                         2. VOLUME GW -> On-premises applications can access these as Internet Small Computer System Interface (iSCSI)targets --> Cached (Store in s3 and retain freq. data as cache in on-premises)- Stored (Store in on-premises and save backups to S3)
                         3. TAPE GW   -> Cost-effectively and durably archive backup data in GLACIER or DEEP_ARCHIVE -- GW deployed on a VM running on Hyper-V / ESXi / KVM

4. SNOW FAMILY         : Snow cone      -- 8 TB storage   --> 1 SMALL BOX [ uses opshub application for data transfer ]
                         Snow Ball      -- 80 TB storage  --> 1 SUITCASE  [ Snowball is a petabyte-scale data transport solution by cascading snow balls ]
                         Snow Ball edge -- 100 TB storage --> 1 SUITCASE  [ Embedded Ec2 - Lambda for running jobs in remote areas like ships , hills for an immediate results. ]
                         Snow Mobile    -- 1 Exabyte      --> 1 Truck     [ 100Pb per snow mobile --  AWS Snowmobile is the exabyte-scale data migration service ]
    // For edge computing applications, to collect data, process the data to gain immediate insight, and then transfer the data online to AWS by shipping the device to AWS, or online by using AWS DataSync.
    // The Snow cone device supports data transfer from on-premises Windows, Linux, and macOS servers and file-based applications through the NFS interface.
    // Using Snowball addresses common challenges with large-scale data transfers including high network costs, long transfer times, and security concerns.
    // S3 buckets, data, and EC2 AMIs that you choose are automatically configured, encrypted, and pre-installed on your devices. The AWS DataSync agent is also pre-installed before your devices are shipped to you. you connect it to your on-premises network and set the IP address either manually or automatically with DHCP. You must download and install AWS OpsHub for Snow Family, on a windows or mac laptop.

5. AWS BACKUP : Automatic backups of our AWS resources using BAKUP PLANS. It automates and consolidates backup tasks that were previously performed service-by-service, and removes the need to create custom scripts and manual processes.
                These features include Amazon Elastic Block Store (Amazon EBS) snapshots, Amazon Relational Database Service (Amazon RDS) snapshots, Amazon DynamoDB backups, AWS Storage Gateway snapshots, and others.
6. AWS EFS    : Only for LINUX -- simple, scalable, fully managed elastic NFS file system for use with AWS Cloud services and on-premises resources. you can create a file system, mount the file system on your EC2 instances, and then read and write data from your EC2 instances to and from your file system.
                Multiple Amazon EC2 instances can access an Amazon EFS file system at the same time, providing a common data source for workloads and applications running on more than one instance or server.

                EC2-a --
                EC2-b   | <---------->  |||||||||  <---------->  On-Premises
                EC2-c --       NFS         EFS          NFS

7. AWS FSX    : For WINDOWS -- Fully managed third-party file systems with the native compatibility and feature sets for workloads such as Microsoft Windows–based storage, high-performance computing, machine learning, and electronic design automation.
                LUSTURE &  WINDOWS FILE SERVER ---- With file storage on Amazon FSx, the code, applications, and tools that Windows developers and administrators use today can continue to work unchanged.


**********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************
**********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************
:::: CRYPTOGRAPHY AND PKI [ Public Key Infrastructure ]  SERVICES  ::::
**********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************
**********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************

AWS provides multiple services that you can use to help protect your data at rest or in transit.
Cryptography is the practice of protecting information through the use of coded algorithms, hashes, and signatures. Encryption -> Cipher text -> Decryption .
Symmetric [ Share Key - Advanced Encryption Standard (AES) & Triple DES (3DES) ] -  Asymmetric [ Public key - Rivest Shamir Adleman (RSA) & Elliptic Curve Cryptography (ECC) ] - Client side - Server Side

#### AWS OWNED CMK _ AWS MANAGED CMK _ CUSTOMER MANAGED CMK
A customer master key (CMK) is a logical representation of a master key.
The CMK includes metadata, such as the key ID, creation date, description, and key state.
The CMK also contains the key material used to encrypt and decrypt data.
AWS KMS supports symmetric and asymmetric CMKs.

1. CLOUD HSM : HARDWARE SECURITY MODULE is a computing device that performs cryptographic operations and provides secure storage for cryptographic keys.
               Used to Generate, store, import, export, and manage cryptographic keys, including symmetric keys and asymmetric key pairs
               -- Use symmetric and asymmetric algorithms to encrypt and decrypt data -- Use cryptographic hash functions to compute message digests and hash-based message authentication codes (HMACs) -- Cryptographically sign data (including code signing) and verify signatures.
               // If you need to secure your encryption keys in a service backed by FIPS-validated HSMs, but you do not need to manage the HSM, try AWS Key Management Service.
2. AWS KMS   : KEY MANAGEMENT SERVICE  provides tools for generating master keys and other data keys and also interacts with many other AWS services to encrypt their service-specific data.
               The customer master keys (CMKs) that you create in AWS KMS are protected by FIPS 140-2 validated cryptographic modules. They never leave AWS KMS unencrypted. To use or manage your CMKs, you interact with AWS KMS.
               CMK is a 256bit AES symmetric key used to encrypt and decrypt the DATA KEYS that encrypt the actual data. You can also create asymmetric RSA or elliptic curve (ECC) CMKs backed by asymmetric key pairs. The public key in each asymmetric CMK is exportable, but the private key remains within AWS KMS.
               AWS KMS does not store or manage data keys, and you cannot use KMS to encrypt or decrypt with data keys. To use data keys to encrypt and decrypt, use the AWS Encryption SDK.
               KMS -- Generate --> CMK -- Encrypt & Decrypt --> DATA KEYS -- Encrypt & Decrypt --> Actual Data
               |____________________ KMS_________________________________| |_________ ENRYPTION SDK __________|


3. DYNAMO DB ENC. CLIENT : A client-side encryption library that helps you to protect your table data before you send it to Amazon DynamoDB. INTEROPERABLE b/w java and python.
                           It encrypts the attribute values in each table item using a unique encryption key. It then signs the item to protect it against unauthorized changes, such as adding or deleting attributes or swapping encrypted values. It also verifies and decrypts them when you retrieve them.
4. S3 CLIENT SIDE ENCRYP : 1 -->  Using a CMK stored in AWS KMS : Client requests KMS for a CMK to generate Data keys. KMS sends two versions of plain and encrypted data key. Client encrypts data using plain key and embeds encrypted key as meta data.
                                  Upon transfer , S3 gets plain key from encrypted key using KMS using CMK and decrypts the object . ONE KEY PER ONE OBJECT
                           2 -->  Using a master key stored within your application  : You provide a client-side master key to the Amazon S3 encryption client. The client uses the master key only to encrypt the data encryption key that it generates randomly.
                                  S3 client generates a plain text data key , encrypts the data and now the client-side master key encrypts the plain text data key and store it as meta data in object . while downloading first decrypt the data key using client-side master key and use it to decrypt the data.
                                  It's important that you safely manage your client-side master encryption keys. If you lose them, you can't decrypt your data.

// PKI : Public key Infrastructure is a system of hardware, software, people, policies, documents, and procedures.
// It includes the creation, issuance, management, distribution, usage, storage, and revocation of digital certificates.

5. AWS CERTIFICATE MANAGER : ACM - Service that lets you easily provision, manage, and deploy public and private Secure Sockets Layer/Transport Layer Security (SSL/TLS) certificates for use with AWS services and your internal connected resources.
                             These public certificates verify the identity and authenticity of your web server and the ownership of your public keys. In doing so, public certificates initiate a trusted, encrypted connection between you and your users.
6. AWS ACM PRIVATE CERTIFICATE AUTHORITY : ACM PCAIs service is for enterprise customers building a public key infrastructure (PKI)inside the AWS cloud and intended for private use within an organization.
                             With private certificates you can authenticate resources inside an organization. Private certificates allow entities like users, web servers, VPN users, internal API endpoints, and IoT devices to prove their identity and establish encrypted communications channels.

                             Plain text     --> [[ cryptographic hash function - SHA (Secure HASH) ]]  --> Original Message Digest also called a HASH
                             Message Digest --> [[ Plain text private key + Signing Algorithm      ]]  --> SIGNATURE also called DIGITAL SIGNATURE also called a CERTIFICATE

                             ** We need to send both the Original Message Digest also called a HASH and the signature along with the message to the client .

                             Signature   --> [[ Public key  + Decryption Algorithm ]]    --> Generated Message Digest also called a generated HASH
                             Generated Message Digest + Original Message Digest  --> [[ Verification Algorithm ]] --> TRUE or FALSE

7. ENCRYPTION SDK          : A client-side encryption library to help you implement best-practice encryption and decryption in any application even if you're not a cryptography expert.
                             Every successful call to encrypt returns a single portable, formatted encrypted message that contains metadata and the message ciphertext.
                             All implementations are interoperable. For example, you can encrypt your data with the Java library and decrypt it with the Python library. Or you can encrypt data with the C library and decrypt it with the CLI.
8. AWS SECRET MANAGER      : Provides encryption and rotation of encrypted secrets used with AWS-supported databases. To protect all other types of data at their source, use the AWS Encryption SDK.
9. AWS SIGNER              : Fully managed code-signing service to ensure the trust and integrity of your code. Organizations validate code against a digital signature to confirm that the code is unaltered and from a trusted publisher.
                             For example With Code Signing for AWS Lambda, you can ensure that only trusted code runs in your Lambda functions.
10. AWS CRYPTO TOOLS       : Part of the AWS ENCRYPTION SDK . The AWS Crypto Tools libraries are designed to help everyone do cryptography right, even without special expertise.


**********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************
**********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************
:::: DATABASE  ::::
**********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************
**********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************

Structured Query language (SQL) pronounced as "S-Q-L" or sometimes as "See-Quel" is the standard language for dealing with Relational Databases.
A relational database defines relationships in the form of tables. Ex : MySQL Database, Oracle, Ms SQL Server, Sybase, PostgreSQL etc.
ACID --> Atomicity (Either all of its operations are executed or none)                  - Consistency (The database must remain in a consistent state after any transaction)
         Isolation (No transaction will affect the existence of any other transaction)  - Durability  (The database should be durable enough to hold all its latest updates even if the system fails or restarts.)

Not only Structured Query language (NO-SQL) pronounced as "Not only SQL" or sometimes as "Not SQL" is the standard language for dealing with Non-Relational/Distributed  Databases.
NoSQL is a non-relational DMS, that does not require a fixed schema, avoids joins, and is easy to scale. NoSQL database is used for distributed data stores with humongous data storage needs. NoSQL is used for Big data and real-time web apps
Ex : MongoDB, Redis, , Neo4j, Cassandra, Hbase.
BASE : Basically Available - Soft State - Eventually Consistent

DATABASE TYPE     USE CASE                                                      AWS SERVICE
================================================================================================================================================
Relational        Traditional applications, ERP, CRM, e-commerce                Amazon Aurora - Amazon RDS -  Amazon Redshift
================================================================================================================================================
Key-value         High-traffic web apps, e-commerce systems,                    Amazon DynamoDB
(JSON-like)       gaming applications
================================================================================================================================================
In-memory         Caching, session management, gaming leader boards,            Amazon ElastiCache for Memcached -  Amazon ElastiCache for Redis
                  geospatial applications
================================================================================================================================================
Document          Content management, catalogs, user profiles                   Amazon DocumentDB (with MongoDB compatibility)
================================================================================================================================================
Wide column       High scale industrial apps for equipment maintenance,         Amazon Keyspaces (for Apache Cassandra)
                  fleet management, and route optimization
================================================================================================================================================
Graph             Fraud detection, social networking, recommendation engines    Amazon Neptune
================================================================================================================================================
Time series       IoT applications, DevOps, industrial telemetry                Amazon Timestream
================================================================================================================================================
Ledger            Systems of record, supply chain, registrations,               Amazon QLDB
                  banking transactions
================================================================================================================================================

REDSHIFT vs RDS :
Both Amazon Redshift and Amazon RDS enable you to run traditional relational databases in the cloud while offloading database administration.
Customers use Amazon RDS databases primarily for online-transaction processing (OLTP) workload while Redshift is used primarily for reporting and analytics.
OLTP workloads require quickly querying specific information and support for transactions like insert, update, and delete and are best handled by Amazon RDS.
Amazon Redshift harnesses the scale and resources of multiple nodes and uses a variety of optimizations to provide order of magnitude improvements over traditional databases for analytic and reporting workloads against very large data sets.

1. RDS : It is a web service that makes it easier to set up, operate, and scale a relational database in the cloud. DB instances use Amazon Elastic Block Store (Amazon EBS) volumes for database and log storage.
         Amazon RDS provides you with six widely-used database engines to choose from, including Amazon Aurora, PostgreSQL, MySQL, MariaDB, Oracle Database, and SQL Server.
         The basic building block of Amazon RDS is the DB instance. Each DB instance runs a DB engine. The computation and memory capacity of a DB instance is determined by its DB instance class [ Magnetic, General Purpose (SSD), Provisioned IOPS(PIOPS) ] .
         Your primary DB instance is synchronously replicated across Availability Zones to the secondary instance. A security group controls the access to a DB instance. It does so by allowing access to IP address ranges or Amazon EC2 instances that you specify. When you use Amazon RDS, you can choose to use on-demand DB instances or reserved DB instances.
         You can have up to 40 Amazon RDS DB instances. Each DB instance has a DB instance identifier. The identifier is used as part of the DNS hostname allocated to your instance by RDS. For example, if you specify db1 as the DB instance identifier, then RDS will automatically allocate a DNS endpoint for your instance, such as db1.123456789012.us-east-1.rds.amazonaws.com

    READ REPLICAS :
    -----------------
    Amazon RDS Read Replicas enable you to create one or more read-only copies of your database instance within the same AWS Region or in a different AWS Region.
    Updates made to the source database are then asynchronously copied to your Read Replicas. In addition to providing scalability for read-heavy workloads, Read Replicas can be promoted to become a standalone database instance when needed.
    Amazon RDS Multi-AZ deployments provide enhanced availability for database instances within a single AWS Region.
    With Multi-AZ, your data is synchronously replicated to a standby in a different Availability Zone (AZ). In the event of an infrastructure failure, Amazon RDS performs an automatic failover to the standby, minimizing disruption to your applications.

2. AURORA : Fully managed relational database engine that's compatible with MySQL(5 times throughput) and PostgreSQL(3 times throughput).
            The only RDS database that can scale instances automatically is Amazon Aurora. For RDS databases other than Aurora, they only support storage auto-scaling, NOT instance auto-scaling.
            If you want to scale Amazon RDS instances (other than Aurora), you have two options:
                  1- Manual horizontal scaling (by adding read replicas)
                  2- Manual vertical scaling (by upgrading/downgrading an existing instance).
    // AURORA is AWS OWNED --> operates on clusters rather than a single instance.

// DB Instance - Horiz/Verti scaling - DB identifier (DNS endpoint) -- Read replicas -- Multi AZ
// Table - item - attributes - Dynamo Db Streams -- DynamoDB Accelerator (DynamoDB DAX) -- DynamoDB point-in-time recovery (PITR)

3. DYNAMO DB : Fast and flexible NoSQL database service for all applications that need consistent, single-digit millisecond latency at any scale.
               With POINT IN TIME RECOVERY  feature, you can restore a table to any point in time during the last 35 days. Point-in-time recovery helps protect your tables from accidental write or delete operations.
               A table is a collection of items, and each item is a collection of attributes. DynamoDB uses primary keys to uniquely identify each item in a table and secondary indexes to provide more querying flexibility.
               You can use DYNAMO DB STREAMS to capture data modification events in DynamoDB tables.
               TABLE --> ITEM --> ATTRIBUTES . //For example, an item in a People table contains attributes called PersonID, LastName, FirstName, and so on.

               DynamoDB Streams is an optional feature that captures data modification events in DynamoDB tables.
               The data about these events appear in the stream in near-real time, and in the order that the events occurred.
               Each event is represented by a stream record. Event can be an ADDITION - UPDATION - DELETION of an item.

4. AMAZON ELASTIC CACHE FOR REDIS & MemCached :  // Set up in-memory cache environments in cloud .
// Queries that involve joins, we have to pay every time we query so we can cache such data and pay once. --> Caching data and providing it to users quickly rather than getting it from Disks. // example for applications in mobile .
Amazon ElastiCache makes it easy to set up, manage, and scale distributed in-memory cache environments in the AWS Cloud.
It provides a high performance, resizable, and cost-effective in-memory cache, while removing complexity associated with deploying and managing a distributed cache environment.
ElastiCache works with both the Redis and Memcached engines.
The service improves the performance of web applications by allowing you to retrieve information from fast, managed, in-memory data stores, instead of relying entirely on slower disk-based databases.

The most frequently accessed data be stored in elasticache so that the application’s response time is optimal.
The primary purpose of an in-memory data store is to provide ultrafast (sub millisecond latency) and inexpensive access to copies of data.

**********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************
**********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************
:::: DEVELOPER TOOLS  ::::
**********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************
**********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************

1. CLOUD-9       : We get an IDE to work with online. The AWS Cloud9 IDE offers a rich code-editing experience with support for several programming languages and runtime debuggers, and a built-in terminal.
                   // Can connect code and EC2. Store your project's files locally on the instance or server --> Clone a remote code repository — such as a repo in AWS CodeCommit — into your environment --> Work with a combination of local and cloned files in the environment.
2. CLOUD-SHELL   : We get a shell to work with online similar to AWS CLI that you can use to manage AWS services. We need not download a separate CLI.

3. CODE-ARTIFACT : Service that makes it easy for organizations to securely store, publish and share software packages used for application development. Artifacts are produced by some actions and consumed by others.
4. CODE-COMMIT   : Similar to GITHUB and Versioning in S3. It is a version control service that enables you to privately store and manage Git repositories in the AWS Cloud.
                   You can use AWS Cloud9 to make code changes in a CodeCommit repository. CodeCommit is optimized for team software development. It manages batches of changes across multiple files, which can occur in parallel with changes made by other developers.
5. CODE-STAR     : It lets you quickly develop, build, and deploy applications on AWS.
                   Depending on your choice of AWS CodeStar project template, that toolchain might include source control, build, deployment, virtual servers or serverless resources, and more.
                   Go to AWS -> Code Star -> Select a Template -> Add team members -> create a repo for code -> Create a pipeline -> Deploy Project . // wiki tile or a issue-tracking software  can be added for recommendations.

6. X-RAY         : Makes it easy for developers to analyze the behaviour of their distributed applications (Micro Services) by providing request tracing, exception collection, and profiling capabilities.
                   Is a service that collects data about requests that your application serves, and provides tools you can use to view, filter, and gain insights into that data to identify issues and opportunities for optimization.

7. CODE-BUILD    : Compiles your source code, runs unit tests, and produces artifacts that are ready to deploy. It provides pre packaged build environments for popular programming languages and build tools such as Apache Maven, Gradle, and more.
                   BUILD PROJECT = Instructions on how to run Build + Source code repo + Build env. + Build commands + Output repo
                   Source Code + BUILD PROJECT [ BUILD SPEC -> OS + Language + Run time + Build env + Build Tools + where to put output ] -> CODE BUILD creates a BUILD ENVIRONMENT  -> Send output to S3 ->  Send SNS if specified .
8. CODE-DEPLOY   : A deployment service that automates application deployments to Amazon EC2 instances, on-premises instances, serverless Lambda functions, or Amazon ECS services.
                   CodeDeploy can deploy application content that runs on a server and is stored in Amazon S3 buckets, GitHub repositories, or Bitbucket repositories. It can deploy in one or 1000 instances.
                   Deployable content [ from CODE BUILD ] + APPSPEC File -> upload to S3 or GITHUB -> Tell CODE DEPLOY [ s3 url / guthub url && No; of instances ] -> CODE DEPLOY Agent in each instance pulls the revision .
9. CODE-PIPELINE : A continuous delivery [ CI-CD ] service that enables you to model, visualize, and automate [ automates the building, testing, and deployment of your software into production ] the steps required to release your software.
                   CodePipeline can deploy applications to EC2 instances by using CodeDeploy, AWS Elastic Beanstalk, or AWS OpsWorks Stacks. CodePipeline can also deploy container-based applications to services by using Amazon ECS.
                   You can trigger an execution when you change your source code (like in code commit) or manually start the pipeline. You can also trigger an execution through an Amazon CloudWatch Events rule that you schedule.
                   [[ SOURCE STAGE ]] -> Code change/cloud watch alarm --> Code Build --> Virtual env (EC2 / ECS) --> Unit Tests --> Code Deploy --> Integration tests --> [[ PROD STAGE ]]
                   TO STOP A PIPELINE : Stop & Wait OR Stop & Abandon .

**********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************
**********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************
:::: MIGRATION & TRANSFER  ::::
**********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************
**********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************

1. APPLICATION DISCOVERY SERVICE :  Deploy Discovery Agent on resources  - To Migrate to cloud // Integrated with MIGRATION HUB //
                   It helps you plan your migration to the AWS cloud quickly and reliably plan application migration projects by automatically identifying applications running in on-premises data centres, their associated dependencies, and their performance profile. Integrates with application discovery solutions from AWS Partner Network(APN) partners.
                   The AWS Discovery Agent is AWS software that you install on on-premises servers and VMs targeted for discovery and migration. Agents capture system configuration, system performance, running processes, and details of the network connections between systems.
                   After registration, it pings the service at 15-minute intervals for configuration information.

2. MIGRATION-HUB : Provides a single location to track [ Database / Server ] migration tasks across multiple AWS tools and partner solutions. With Migration Hub, you can choose the AWS and partner migration tools that best fit your needs while providing visibility into the status of your migration projects.
                   Also provides key metrics and progress information for individual applications, regardless of which tools are used to migrate them.

3. SERVER MIGRATION SERVICE : Migrate from On-Premises or Microsoft or Azure Servers to AWS. AWS SMS incrementally replicates your server VMs as cloud-hosted Amazon Machine Images (AMIs) ready for deployment on Amazon EC2.
                   The Server Migration Connector is a FreeBSD VM that you install in your on-premises virtualization environment. We need to install this on the on premises / virtual VM. Now open the Server Migration Service on AWS and do the transfer.

4. DATABASE MIGRATION SERVICE - DMS : Service to migrate relational databases, data warehouses, NoSQL databases, and other types of data store bidirectionally from on-premises to cloud.
                   you can perform one-time migrations, and you can replicate ongoing changes to keep sources and targets in sync. If you want to change database engines, you can use the AWS Schema Conversion Tool (AWS SCT) to translate your database schema to the new platform.
                   And then use AWS DMS to migrate to cloud . Any data that can not be converted is marked for review later . The only requirement to use AWS DMS is that one of your endpoints must be on an AWS service. You can't use AWS DMS to migrate from an on-premises database to another on-premises database.
                   A task can consist of three major phases:   --> • The full load of existing data   --> • The application of cached changes   --> • Ongoing replication
                   While the replication is going on , all the transactions are cached and the cached data is migrated at the end to not disturb the flow of business or miss any data transaction.
// DMS is for DATA BASE migration -- // DATA SYNC is for DATA migration

5. AWS SCHEMA CONVERSION TOOL : // USED BY AWS-DMS. Makes heterogeneous database migrations easy by automatically converting the source database schema and a majority of the custom code to a format compatible with the target database.
                   Any code that the tool cannot convert automatically is clearly marked so that you can convert it yourself. Your converted schema is suitable for an Amazon Relational Database Service (Amazon RDS) MySQL, MariaDB, Oracle, SQL Server, PostgreSQL DB, an Amazon Aurora DB cluster, or an Amazon Redshift cluster.
                   The converted schema can also be used with a database on an Amazon EC2 instance or stored as data on an Amazon S3 bucket.

6. AWS DATA-SYNC : Online Transfer of data. It is an online data transfer service that simplifies, automates, and accelerates moving data between on-premises storage systems and AWS storage services, and also among  AWS storage services themselves .
                   DataSync can copy data between Network File System (NFS), Server Message Block (SMB) file servers, self-managed object storage, AWS Snow cone, Amazon Simple Storage Service (Amazon S3)buckets, Amazon EFS file systems, and Amazon FSx for Windows File Server file systems.

                     ON_PREMISIS                                                          AWS CLOUD
                                                              TLS
    Shared File system <----> AWS Data Sync agent  <========================> AWS Data Sync <----> AWS S3 / EFS / FSx


7. SNOW FAMILY         : Snow cone      -- 8 TB storage   --> 1 SMALL BOX [ uses opshub application for data transfer ]
                         Snow Ball      -- 80 TB storage  --> 1 SUITCASE  [ Snowball is a petabyte-scale data transport solution by cascading snow balls ]
                         Snow Ball edge -- 100 TB storage --> 1 SUITCASE  [ Embedded Ec2 - Lambda for running jobs in remote areas like ships , hills for an immediate results. ]
                         Snow Mobile    -- 1 Exabyte      --> 1 Truck     [ 100Pb per snow mobile --  AWS Snowmobile is the exabyte-scale data migration service ]

8. AWS TRANSFER FAMILY : // To set up a sftp server for customers. AWS Transfer Family is a secure transfer service that stores your data in Amazon S3 and simplifies the migration of the following workflows to AWS:
                         --> Secure Shell File Transfer Protocol (SFTP),   --> File Transfer Protocol Secure (FTPS), and   --> File Transfer Protocol (FTP) .
                         It is a secure transfer service that enables you to transfer files into and out of AWS storage services. Supports transferring data from the following :
                                  --> Amazon Simple Storage Service (Amazon S3) storage.
                                  --> Amazon Elastic File System (Amazon EFS) Network File System (NFS) file system

**********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************
**********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************
:::: FRONT END WEB & MOBILE  ::::
**********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************
**********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************

1. AMPLIFY  : Set of SDKs, libraries, tools, and documentation to develop mobile & Web-apps. Provides two main services, hosting and the Admin UI.
              Provides a git-based workflow for hosting fullstack serverless web apps with continuous deployment. The Admin UI is a visual interface for frontend web and mobile developers to create and manage app backends outside the AWS console.

2. APP-SYNC : GraphQL is a query language and server-side runtime for application programming interfaces (APIs) that prioritizes giving clients exactly the data they request and no more.
              As an alternative to REST, GraphQL lets developers construct requests that pull data from multiple data sources in a single API call. App-Sync is an Enterprise level, fully managed GraphQL service with real-time data synchronization and offline programming features.

3. AWS DEVICE FARM  : Performing iOS, Android and Fire OS apps on real mobiles -- Live customer support using remote connection --> Remote access of devices onto which you can load, run, and interact with apps in real time. -> Remotely work on someone's phone
                      Because testing is performed in parallel, tests on multiple devices begin in minutes. As tests are completed, a test report that contains high-level results, low-level logs, pixel-to-pixel screenshots, and performance data is updated.
                      Remote access allows you to swipe, gesture, and interact with a device through your web browser in real time. Install apps for them and explain them on how to use apps and solve any issues they are facing.

4. LOCATION SERVICE : Add maps, points of interest, geocoding, geofences, and tracking to your applications. AWS uses high-quality data from global, trusted providers Esri and HERE.
                      Tracking data and geofencing location information, such as facility, asset, and personnel locations, never leaves your AWS account at all. With Amazon Location, neither Amazon nor third parties have rights to sell your data or use it for advertising.


5. AMAZON PINPOINT : Send promotions, transactional (Order confirmations and password resets) and campaign messages via  email, SMS and voice messages, and push notifications to customers and get campaign statistics.
                     Add customer contact information and then create segments that target certain customers. Next, you have to create your messages and schedule your campaigns. Finally, you can use the analytics dashboards see how well the campaigns performed.
6. AMAZON SIMPLE NOTIFICATION SERVICE - SNS : It is a web service that enables applications, end-users, and devices to instantly send and receive notifications from the cloud. We create topics and subscribe to the topic for messages .
                     Clients can subscribe to the SNS topic and receive published messages using a supported protocol, such as Amazon Kinesis Data Firehose, Amazon SQS, AWS Lambda, HTTP, email, mobile push notifications, and mobile text messages (SMS).

    • Application-to-application messaging  -- HTTP/S - Lambda - SQS
    • Application-to-person notifications   -- SMS - Notifications - SES
    • Standard and FIFO topics
    • Message delivery retry               • Message attributes -- we can provide arbitrary metadata .
    • Message security                     • Message filtering based on policies
    • Dead-letter queues -- an Amazon SQS queue for messages that can't be delivered successfully due to client errors or server error.

SNS : IS A TOPIC Service.  // publishers to subscribers OR Producers to Consumers
SQS : IS A QUEUE Service.
      Unlimited Throughput , minimum one delivery , Best effort ordering - STANDARD QUEUE
      High Throughput , Exactly one delivery , First in First OUT - FIFO QUEUES


**********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************
**********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************
:::: END USER COMPUTING  ::::
**********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************
**********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************
// The following are used if you have a corporate N/W or if you are an enterprise and you want to provide services to your end users. All the services are monitored by an admin and he provides / revokes accesses to users .
   Mainly used by employees working remotely. This also makes sure that the main organizational information is not stored on users local machine.

// software developers in an organization can use Amazon WorkSpaces to access all desktop resources from any computer or tablet.
   Engineers can use AppStream 2.0 to stream GPU intensive apps.
   And sales leaders can use Amazon WorkLink to access internal web-based content, such as sales data, from their mobile devices.

1. WORKSPACES : Enables you to provision virtual, cloud-based Microsoft Windows or Amazon Linux desktops for your users, known as WorkSpaces. You can quickly add or remove users as your needs change. Users can access their virtual desktops from multiple devices or web browsers.
                As an admin you have the rights to remove add users and assign applications to users using WAM. As a user you pick up from right where you left off with a persistent desktop experience.
                you can bring your own licenses and applications, or purchase them from the AWS Marketplace for Desktop Apps. From any Operating system , use any other operating system

2. WORKSPACES APPLICATION MANAGER - WAM : Offers a fast, flexible, and secure way for you to deploy, manage, update, patching, and retire Microsoft Windows desktop applications into virtual containers that run as though they are installed natively for Amazon WorkSpaces  with Windows.
                VERSION-1 : With Amazon WAM Lite, you can manage and deliver applications from the AWS Marketplace free of charge. You pay only for the applications that your users activate.
                VERSION-2 : With Amazon WAM Standard, you can build your application catalog with line-of-business applications, third-party applications for which you own licenses, and applications from the AWS Marketplace for Desktop Apps.

3. APP-STREAM 2.0 : Add your existing desktop applications to AWS and enable your users to instantly stream them - Always-on & On-Demand .
                A fully managed application streaming service that provides users with instant access to their desktop applications from anywhere. Your applications run on AWS compute resources, and data is never stored on users' devices, which means they always get a high performance, secure experience.
                • Always-On — Your instances run all the time, even when no users are streaming applications. Use an Always-On fleet to provide your users with instant access to their applications.
                • On-Demand — Your instances run only when users are streaming applications. Idle instances that are available for streaming are in a stopped state. Use an On-Demand fleet to optimize your streaming charges and provide your users with access to their applications after a 1-2 minute wait.

4. WORK-DOCS :  --> STORE - SYNC - SHARE FILES from anywhere to anywhere. --> Directly edit docs online. Users can share their files with other members of your organization for collaboration or review.
                Files are stored in the cloud, safely and securely. Your user's files are only visible to them, and their designated contributors and viewers. Other members of your organization do not have access to other user's files unless they are specifically granted access.

5. WORK-LINK : Similar to UX-Apps TCS . Used to give secure access to internal websites and web apps through mobile phones , since using a VPN is not a feasible solution. Users need to install the work-link application .
               In a single step, your users, such as employees, can access internal websites as efficiently as they access any other public website. In addition, all cached content is deleted from AWS when users end their browsing session.
               users enter a URL in their web browser, or choose a link to an internal website in an email. Because website data is never stored or cached locally on mobile browsers, Amazon WorkLink reduces the risk of information loss or theft.
               After initial setup, the Amazon WorkLink app works in the background while employees browse internal websites using Safari on iOS phones and Google Chrome on Android phones.


**********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************
**********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************
:::: APPLICATION INTEGRATION  ::::
**********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************
**********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************
1. APP-FLOW       : Bidirectional data transfer from SAAS Applications ( Slack or Sales Force ) to AWS ( S3 or Redshift ) for analytics and archiving and automatic workflow.

2. EVENT-BRIDGE   : // Formerly CloudWatch Events -- Bridge b/w your apps - SAAS - AWS apps.
                    serverless event bus service that makes it easy to connect your applications with data from a variety of sources to targets such as AWS Lambda.
                    You can set up routing rules to determine where to send your data to build application architectures that react in real time to all of your data sources.

3. AMAZON MANAGED MESSAGE BROKER - MQ : A message broker is an intermediary computer program module that translates a message from the formal messaging protocol of the sender to the formal messaging protocol of the receiver.
                    Amazon MQ is a managed message broker service that makes it easy to set up and operate message brokers in the cloud.

4. SIMPLE NOTIFICATION SERVICE - SNS  : It is a web service that enables applications, end-users, and devices to instantly send and receive notifications from the cloud. We create topics and subscribe to the topic for messages .
                     Clients can subscribe to the SNS topic and receive published messages using a supported protocol, such as Amazon Kinesis Data Firehose, Amazon SQS, AWS Lambda, HTTP, email, mobile push notifications, and mobile text messages (SMS).

    • Application-to-application messaging  -- HTTP/S - Lambda - SQS
    • Application-to-person notifications   -- SMS - Notifications - SES
    • Standard and FIFO topics
    • Message delivery retry               • Message attributes -- we can provide arbitrary metadata .
    • Message security                     • Message filtering based on policies
    • Dead-letter queues -- an Amazon SQS queue for messages that can't be delivered successfully due to client errors or server error.

5. AMAZON SQS     : Fully managed message queuing service that makes it easy to decouple and scale microservices, distributed systems, and serverless applications.
                    You can send, store, and receive messages between software components at any volume, without losing messages or requiring other services to be available.
                    Multiple copies of every message are stored redundantly across multiple availability zones so that they are available whenever needed.

6. STEP FUNCTIONS : Makes it easy to coordinate the components of distributed applications as a series of steps in a visual workflow. Service that lets you combine AWS Lambda functions and other AWS services to build business-critical applications.
                    Through Step Functions' graphical console, you see your application’s workflow as a series of event-driven steps.
                    Step Functions is based on state machines and tasks. A state machine is a workflow. A task is a state in a workflow that represents a single unit of work that another AWS service performs. Each step in aworkflow is a state.
                    With Step Functions' built-in controls, you examine the state of each step in your workflow to make sure that your application runs in order and as expected.
                    CHECK 6 USECASES AND PICTURES .

7. SIMPLE WORK FLOW - SWF : Makes it easy to build applications that coordinate work across distributed components. A Task represents a logical unit of work that is performed by a component of your application.
                    Coordinating tasks across the application involves managing inter task dependencies, scheduling, and concurrency in accordance with the logical flow of the application.
                    SWF gives you full control over implementing tasks and coordinating them without worrying about underlying complexities such as tracking their progress and maintaining their state.
                    When using Amazon SWF, you implement workers to perform tasks. These workers can run either on cloud infrastructure, such as Amazon Elastic Compute Cloud (Amazon EC2), or on your own premises.
                    Amazon SWF stores tasks and assigns them to workers when they are ready, tracks their progress, and maintains their state, including details on their completion.
                    To coordinate tasks, you write a program that gets the latest state of each task from Amazon SWF and uses it to initiate subsequent tasks.
                    Amazon SWF maintains an application's execution state durably so that the application is resilient to failures in individual components.
                    With Amazon SWF, you can implement, deploy, scale, and modify these application components independently.

**********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************
**********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************
:::: BUISENES APPLICATION  ::::
**********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************
**********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************
1. ALEXA FOR BUSINESS : Similar to your Alexa in house , you can set up 100's of alexa in an organization and sync data between users.
                        schedule  meetings , check available rooms for booking , join meetings , get sales reports for your specific company .
2. CHIME              : Similar MS Teams (Download Amazon Chime APP). Communications service that transforms meetings by making them more efficient and easier to conduct.
                        You can use Amazon Chime for online meetings, video conferencing, calls, and chat. You can also share content inside and outside of your organization.
                        You pay only for the users with Pro permissions that host meetings, and only on the days that those meetings are hosted. There is no charge for users with Basic permissions. Basic users cannot host meetings, but they can attend meetings and use chat.
3. HONEY-CODE         : Fully managed service that allows you to quickly build mobile and web apps for teams—without programming. Build a custom managed app using a visual editor . First setup data in tables , tables work like spread sheets but have database capabilities.
                        Next build the apps layout . Now link app and the tables, set personalization for specific users to see what they can see or not see. Set specific actions to be done on an event such as when data is updated .
4. WORK-MAIL          : Business email and calendar service. Maintaining personal email service is tough !! -> Buy dedicated resources [ h/w and s/w ] -> hire staff to run the resources -> regular maintenance -> upgrades, security patches, backups , disaster recovery .
                        support for existing desktop and mobile clients. Can access mail and calendar from multiple devices mobile - tablet - desktop - all OS - windows - IOS and has compatibility with OUTLOOK and other popular services .
                        Amazon WorkMail has a web-based client that you use to access your Amazon WorkMail account from a web browser.


**********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************
**********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************
:::: CUSTOMER ENABLEMENT SERVICES  ::::
**********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************
**********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************
1. AWS MANAGED SERVICES      : 3Rd party partners providing full life cycle solutions for ENTERPRISE Customers . An enterprise service that provides ongoing management of your AWS infrastructure operations.
                Provides full-lifecycle services to provision, run, and support your infrastructure, and automates common activities such as change requests, monitoring, patch management, security, and backup services.
                Enterprises want to adopt AWS at scale but often the skills that have served them well in traditional IT do not always translate to success in the cloud. AMS enables them to migrate to AWS at scale more quickly, reduce their operating costs, improve security and compliance and focus on their differentiating business priorities.

2. AWS PROFESSIONAL SERVICES : A global team of experts that can help you realize your desired business outcomes when using the AWS Cloud.
                We work together with your team and your chosen member of the AWS Partner Network (APN) to execute your enterprise cloud computing initiatives.

3. AWS IQ     : Submit request -> review responses -> select expert -> work securely -> Pay directly from AWS billing. Enables customers to quickly find, engage, and pay AWS Certified third-party experts for on-demand project work.
                AWS IQ enables AWS Certified experts to help customers and get paid for their AWS Certification expertise. AWS IQ is a good choice when you're stuck on one or more tasks and need someone to do the work for you.

4. AWS SUPPORT :  There are multiple SUPPORT PLANS available. All AWS customers automatically have 24/7 access to features of the Basic support plan.
                  --> Basic         - Included
                  --> Developer     - Starting at $29 per month       - https://aws.amazon.com/premiumsupport/plans/
                  --> Business      - Starting at $100 per month      - https://aws.amazon.com/premiumsupport/plans/
                  --> Enterprise    - Starting at $15,000 per month   - INFRASTRUCTURE EVENT MANAGEMENT - MIGRATION ACCELERATION PROGRAM - TECHNICAL ACCOUNT MANAGER -  TOTAL COST OF OWNERSHIP
                                      TAM - MAP - IEM - TCO

4. AWS TRAINING & CERTIFICATION : TRAINING : https://aws.amazon.com/training/?id=docs_gateway
                                  DOCS     : https://docs.aws.amazon.com/

**********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************
**********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************
:::: CUSTOMER ENGAGEMENT  ::::
**********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************
**********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************
1. AMAZON CONNECT : Set up a Customer care centre. Add agents who are located anywhere, and start engaging with your customers.
                    Voice and Chat channels and support tickets tracking . All support channel data is logged in S3. It uses Machine Learning (ML) and Artificial Intelligence (AI) for analytics .

2. AMAZON PINPOINT : Send promotions, transactional (Order confirmations and password resets) and campaign messages via  email, SMS and voice messages, and push notifications to customers and get campaign statistics.
                     Add customer contact information and then create segments that target certain customers. Next, you have to create your messages and schedule your campaigns. Finally, you can use the analytics dashboards see how well the campaigns performed.

3. AMAZON SIMPLE EMAIL SERVICE - SES : Add email-sending capabilities to any application. If your application runs in EC2, you can use Amazon SES to send 62,000 emails every month at no additional charge.
                    Use AWS Elastic Beanstalk to create an email-enabled application such as a program that uses Amazon SES to send a newsletter to customers.  Set up SNS to notify you of your emails that bounced, produced a complaint, or were successfully delivered to the recipient's mail server.
                    When you use Amazon SES to receive emails, your email content can be published to Amazon SNS topics.

**********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************
**********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************
:::: BILLING & COST MANAGEMENT  ::::
**********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************
**********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************
1. BILLING & COST MANAGEMENT : Service that you use to pay your AWS bill, monitor your usage, and analyze and control your costs.
   AWS COST & USAGE REPORT   : one-stop shop for accessing the most detailed information available about your AWS costs and usage. Lists AWS usage for each service category used by an account and its IAM users in hourly or daily line items, as well as any tags that you have activated for cost allocation purposes.
   AWS CONSOLIDATED BILLING  : Enables an organization to consolidate payments for multiple Amazon Web Services (AWS) accounts within a single organization by making a single paying account.
   AWS COST EXPLORER         : Is a free tool that you can use to view your costs and usage up to the last 13 months, forecast how much you are likely to spend for the next twelve months (used to be three months), and get recommendations for what Reserved Instances to purchase..
   AWS BUDGETS               : Gives you the ability to set custom budgets that alert you when your costs or usage exceed (or are forecasted to exceed) your budgeted amount.
                                   Cost budgets – Plan how much you want to spend on a service.
                                   Usage budgets – Plan how much you want to use one or more services.
                                   RI utilization budgets – Define a utilization threshold and receive alerts when your RI usage falls below that threshold.
   ORDERS & INVOICES         : To see your past payments and transactions.
   KNOWLEDGE CENTRE          : You can find answers to your questions quickly by visiting the AWS Knowledge Center.
   COST ANAMOLY DETECTION    : Analyze and determine the root cause of the anomaly, such as account, service, Region, or usage type that is driving the cost increase.

2. AWS PRICING CALCULATOR    : Lets you explore AWS services and create an estimate for the cost of your use cases on AWS. You can model your solutions before building them, explore the price points and calculations behind your estimate, and find the available instance types and contract terms that meet your needs.

3. AWS SAVINGS PLAN          : Flexible pricing model that helps you save up to 72 percent on Amazon EC2, AWS Fargate, and AWS Lambda usage. Provides you lower prices in exchange for a commitment to a consistent usage amount (measured in $/hour)for a one or three year term.
                               Every type of compute usage has an On-Demand rate and a Savings Plans rate. If you commit to $10/hour of compute usage, your usage is charged at your Savings Plans rate up to $10. Any usage beyond your Savings Plans commitment is charged at your regular On-Demand rates.


**********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************
**********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************
:::: TOTAL SERVICES :::: // Total = 27 <--> Left-Out = 3 <--> Remaining = 24 <--> [ 4 Important + 14 Regular + 6 Singles ]
**********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************
**********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************
1. ANALYTICS  [ 13  -->  3 + 3 + 3 + 4 ]
   APP-FLOW         DATA-EXCHANGE         DATA-PIPELINE
   GLUE             LAKE-FORMATION        QUCIK-SIGHT
   CLOUD-SEARCH     ELASTIC-SEARCH        MANAGED-STREAMING-FOR-KAFKA
   ATHENA           KINESIS               EMR -- ELASTIC-MAP-REDUCE  / EMR-SPECTRUM       REDSHIFT

2. CONTAINERS = 5
   //  DOCKER FILE --> docker build ---> DOCKER IMAGE -- docker run ---> DOCKER CONTAINER.
   ECR -- ELASTIC-CONTAINER-REGISTERY    ECS -- ELASTIC-CONTAINER-SERVICES      EKS -- ELASTIC-KUBERNETIS-SERVICE     FARGATE     APP2CONTAINER

3. STORAGE  [ 7 --> 3 + 4 ]
   SIMPLE-STORAGE-GATEWAT       EBS -- ELASTIC-BLOCK-STORE                    STORAGE-GATEWAY
   SNOW-FAMILY                  FSx -- MICROSOFT-FLIGHT-SIMULATORE x          EFS -- ELASTIC-FILE-SYSTEM       BACKUP

4. CRYPTOGRAPHY AND PKI - Public Key Infrastructure SERVICES : [ 10 --> 2 + 2 + 2 + 4 ]
   // Symmetric [ Share Key - Advanced Encryption Standard (AES) & Triple DES (3DES) ] -  Asymmetric [ Public key - Rivest Shamir Adleman (RSA) & Elliptic Curve Cryptography (ECC) ] - Hashing [ Secure hash Algorithm (SHA) ]
   HSM -- HARDWARE-SECURITY-MODELS     KMS -- KEY-MANAGEMENT-SERVICE
   DYNAMO-DB-ENCRYPTION-CLIENT         S3-CLIENT-SIDE-ENCRYPTION
   ACM -- AWS-CERTIFICATE-MANAGER      ACM-PCA --  ACM-PRIVATE-CERTIFICATE-AUTHORITY
   ENCRYPTION-SDK                      SECRETS-MANAGER                                       CRYPTO-TOOLS                   SIGNER

5. DATABASE [ 10 --> 5 + 5 ]
   // ACID -- BASE
   RDS -- RELATIONAL-DATABASE-SERVICES   AURORA    REDSHIFT    DYNAMO-DB     ELASTICAHE-FOR-(MEMCAHCED/REDIS)
   DOCUMENT-DB                           NEPTUNE   KEYSPACES   TIME-STREAM   QLDB -- QUANTUM-LEDGER-DATABASE

6. DEVELOPER TOOLS   [ 10 --> 2 + 3 + 3 + 1 ]
   CLOUD-9         CLOUD-SHELL
   CODE-ARTIFACT   CODE-COMMIT   CODE-STAR
   CODE-BUILD      CODE-DEPLOY   CODE-PIPELINE
   X-RAY

7. MIGRATION & TRANSFER  [ 8 --> 6 + 2 ]
   APPLICATION-DISCOVERY-SERVICE    MIGRATION-HUB     SERVER-MIGRATION-SERVICE    DATABASE-MIGRATION-SERVICE     SCHEMA-CONVERSION-TOOL     DATA-SYNC
   SNOW-FAMILY                      TRANSFER-FAMILY

8. FRONT END WEB & MOBILE [ 6 --> 4 + 2 ]
   AMPLIFY   APP-SYNC   DEVICE-FARM    LOCATION-SERVICES
   PINPOINT  SNS -- SIMPLE-NOTIFICATION-SERVICES

9. END USER COMPUTING
   WORKSPACES    WAM -- WORKSPACES-APPLICATION-MANAGER     WORK-DOCS    WORK-LINK    APP-STREAM-2.0

10. APPLICATION INTEGRATION  [ 7 --> 4 + 3 ]
    APP-FLOW                         EVENT-BRIDGE                          STEP-FUNCTIONS                 SWF -- SIMPLE-WORK-FLOW
    MQ -- MANAGED-MESSAGE-BROKER     SNS -- SIMPLE-NOTIFICATION-SERVICES   SQS -- SIMPLE-QUEUE-SERVICE

11. BUISENES APPLICATION
    ALEXA-FOR-BUSINESS     CHIME     HONEY-CODE     WORK-MAIL

12. CUSTOMER ENABLEMENT SERVICES
    AWS-MANAGED-SERVICES    AWS-PROFESSIONAL-SERVICES     AWS-IQ      AWS-SUPPORT      TRAINING-&-CERTIFICATION
    https://aws.amazon.com/premiumsupport/plans/

13. CUSTOMER ENGAGEMENT
    AMAZON-CONNECT          AMAZON-PINPOINT       SES -- SIMPLE-EMAIL-SERVICE

14.  BILLING & COST MANAGEMENT
     BILLING & COST MANAGEMENT --> [ COST-&-USAGE-REPORT --  CONSOLIDATED-BILLING  -- COST-EXPLORER -- BUDGETS -- ORDERS-&-INVOICES -- KNOWLEDGE-CENTRE -- COST-ANAMOLY-DETECTION -- Simple Monthly Calculator  ]
     PRICING CALCULATOR           SAVINGS PLAN

// [[ BAGS-QR ]]
15. SATELLITE            -- AWS GROUND STATION
16. ROBOTICS             -- AWS ROBO MAKER
17. QUANTUM COMPUTING    -- AMAZON BRAKET
18. BLOCKCHAIN           -- AMAZON MANAGED BLOCK CHAIN
19. AR & VR              -- AMAZON SUMERIAN
20. GAME DEVELOPMENT     -- AMAZON GAME-LIFT  &&  AMAZON LUMBER-YARD


21. COMPUTE   [ 8 --> 6 + 2 ]
    ELASTIC-COMPUTE-CLOUD       ELASTIC-BEANSTALK        BATCH [parallel-Clusters HPC(1000 cpu's)]    EC2-IMAGE-BUILDER        LAMBDA          LIGHT-SAIL [launch-wizard]
    OUTPOSTS                    WAVELENGTHS
 // LAUNCH-WIZARD               PARALLEL-CLUSTERS        SAM -- SERVERLESS-APPLICATION-MODEL          SERVERLESS-APPLICATION-REPOSITORY
                                                         (Open-Source framework - cloud-Formation)

22. NETWORKING & CONTENT DELIVERY   [ 10 --> 6 + 4 ]
    CLOUD-FRONT                  DIRECT-CONNECT(Interfaces + Gw's + Dedicated/Hosted)      VIRTUAL-PRIVATE-CLOUD
    VIRTUAL-PRIVATE-NETWORK      ROUTE-53(DOmain + Routing + Health + 8 STEPS)             ELASTIC-LOAD-BALANCING( 3-Types & Cross Zone Load Balancing)
    API-GATEWAY                  APP-MESH                                                  CLOUD-MAP                                                              GLOBAL-ACCELERATOR

23. SECURITY - IDENTITY & COMPLIANCE  [ 19 --> 2 + 2 + 2 + 2 + 4 + 4 + 3 ]
    IDENTITY-ACCESS-MANAGEMENT (Identity [Managed[aws, customer], Inline] & Resource)    COGNITO (fb, insta // authentication -- user pools and  authorization -- identity pools)
    RESOURCE-GROUPS-&-TAG-EDITOR (group resources & perform bulk actions )               RAM --  RESOURCE-ACCESS-MANAGER ( share resources across principles such as AWS accounts, organizational units, or an entire organization from AWS Organizations.)
    ARTIFACT (Sec, compliance reports and online agreements)                             AUDIT-MANAGER (audit your AWS usage)
    DIRETORY-SERVICE (To use Microsoft Active Directory - SSO)                           CLOUD-DIRECTORY (generate a conceptual relationship between the objects & Control access)

    WEB-APPLICATION-FIREWALL         SHIELD (Standard(free)/Advanced)         FIREWALL-MANAGER          NETWORK-FIREWALL
    AWS-DETECTIVE                    AWS-GUARD-DUTY                           AWS-INSPECTOR             AWS-MACIE
    SECRETS-MANAGER                  SECURITY-HUB                             SINGLE-SIGN-ON (Service provider <---- TRUST ----> Identity provider // SAML // 8 STEPS)

    // services AWS WAF - AWS SHIELD - AWS FIREWALL MANAGER - AWS NETWORK FIREWALL are interlinked. So which one to use ??
       --> It all starts with AWS WAF. You can automate and then simplify AWS WAF management using AWS Firewall Manager. // (DDOS, HTTP/S to cloud front, SQL injection, cross-site scripting)
           can be deployed on Amazon CloudFront, Application Load Balancer, Amazon API Gateway, AWS AppSync
           AWS WAF is a web application firewall that helps protect web applications from attacks by allowing you to configure rules that allow, block, or monitor (count) web requests based on conditions that you define. These conditions include IP addresses, HTTP headers, HTTP body, URI strings, SQL injection and cross-site scripting.
           AWS WAF is a web application firewall that lets you monitor the HTTP and HTTPS requests that are forwarded to CloudFront, and lets you control access to your content.
           When you use AWS WAF on Amazon CloudFront, your rules run in all AWS Edge Locations, located around the world close to your end-users. This means security doesn’t come at the expense of performance. Blocked requests are stopped before they reach your web servers.
       --> Shield offers free standard services and  Shield Advanced which adds additional features on top of AWS WAF, such as dedicated support from the DDoS Response Team (DRT) and advanced reporting, this requires a business or enterprise support plan.
       --> To use the services of the DRT, you must be subscribed to the Business Support plan or the Enterprise Support plan.
       --> If you want granular control over the protection that is added to your resources, AWS WAF alone is the right choice.
       --> If you want to use AWS WAF across accounts, accelerate your AWS WAF configuration, or automate protection of new resources, use Firewall Manager with AWS WAF.  Network firewall manager manages all netwrok firewalls , vpc's , subnets etc; Network firewall manages a particular VPC . You can create one for each VPC .
       --> Finally, if you own high visibility websites or are otherwise prone to frequent DDoS attacks, you should consider purchasing the additional features that Shield Advanced provides.

   // All AWS customers benefit from the automatic protections of AWS Shield Standard, at no additional charge. AWS Shield Standard defends against most common, frequently occurring network and transport layer DDoS attacks that target your web site or applications.
      When you use AWS Shield Standard with Amazon CloudFront and Amazon Route 53, you receive comprehensive availability protection against all known infrastructure (Layer 3 and 4) attacks.
      For higher levels of protection against attacks targeting your applications running on Amazon Elastic Compute Cloud (EC2), Elastic Load Balancing (ELB), Amazon CloudFront, AWS Global Accelerator and Amazon Route 53 resources, you can subscribe to AWS Shield Advanced.
      In addition to the network and transport layer protections that come with Standard, AWS Shield Advanced provides additional detection and mitigation against large and sophisticated DDoS attacks, near real-time visibility into attacks, and integration with AWS WAF, a web application firewall.


       // Layer 7 : AWS WAF is a web application firewall that lets you monitor the HTTP and HTTPS requests that are forwarded to an Amazon API Gateway API, Amazon CloudFront or an Application Load Balancer.
                    HTTP and HTTPS requests are part of the Application layer, which is layer 7.
          Layer 3 - Layer 3 is the Network layer and this layer decides which physical path data will take when it moves on the network.
                    AWS Shield offers protection at this layer. WAF does not offer protection at this layer.
          Layer 4 - Layer 4 is the Transport layer and this layer data transmission occurs using TCP or UDP protocols.
                    AWS Shield offers protection at this layer. WAF does not offer protection at this layer.

    /// AWS DETECTIVE - AWS GUARD DUTY - AWS INSPECTOR - AWS MACIE are similar in concepts.
        --> AWS DETECTIVE is mainly for the security findings in login attempts , API calls from AWS CloudTrail and Amazon VPC flow logs. It also ingests findings detected by GuardDuty.
            Amazon Detective can analyze trillions of events from multiple data sources such as Virtual Private Cloud (VPC) Flow Logs, AWS CloudTrail, and Amazon GuardDuty, and automatically creates a unified, interactive view of your resources, users, and the interactions between them over time.
        --> AWS GUARD DUTY does DETETIVE duty plus reports compromised EC2 instances , compromised s3 instances and compromised AWS credentials from VPC Flow Logs, AWS CloudTrail management event logs, Cloudtrail S3 data event logs, and DNS logs.
            It also monitors AWS account access behaviour for signs of compromise, such as unauthorized infrastructure deployments, like instances deployed in a Region that has never been used, or unusual API calls, like a password policy change to reduce password strength.
            GuardDuty is a threat detection service that monitors malicious activity and unauthorized behavior to protect your AWS account. GuardDuty analyzes billions of events across your AWS accounts from AWS CloudTrail (AWS user and API activity in your accounts), Amazon VPC Flow Logs (network traffic data), and DNS Logs (name query patterns). Security findings are retained and made available through the Amazon GuardDuty console and APIs for 90-days.
        --> AWS INSPECTOR does an vulnerability assessment based on common security standards and vulnerability definitions updated frequently by AWS . Offers predefined software called an agent that you can install in EC2. Examples of built-in rules include checking for remote root login being enabled, or vulnerable software versions installed.
        --> AWS MACIE is for protecting personal and financial data stored  in S3 across your organization and reports if a bucket is unencrypted .
        --> All the services are free for 30 days .Pay from the next month!! During the 30-day free trial period, the cost estimation projects what your estimated costs will be after the free trial period.


24. MANAGEMENT & GOVERNANCE :
    AWS BACKINT AGENT FOR SAP HANA    AMAZON MANAGED SERVICE FOR GRAFANA        MANAGED SERVICE FOR PROMETHEUS            TOOLS FOR POWER-SHELL     DATA LIFE CYCLE MANAGER      WELL-ARCHITECTURED TOOL
    COMMAND LINE INTERFACE            AWS MANAGEMENT CONSOLE                    CONSOLE MOBILE APPLICATION                COMPUTE OPTIMIZER


    CLOUD-FORMATION
    CLOUD-TRAIL              -- past 90 days of activity in your AWS account, save trail to S3.
    CLOUD-WATCH              -- collect and track metrics, collect and monitor log files, set alarms, and automatically react to changes in your AWS resources. Create custom dashboards
    CONFIG                   -- To assess , audit and evaluate the configuration of your resources . And to make sure resources are compliant with industry standards. We get notified if a resource is non-compliant or matches a rule against compliance. This includes how the resources are related to one another and how they were configured in the past so that you can see how the configurations and relationships change over time.
    OPSWORKS                 -- configuration management service that helps you configure and operate applications in a cloud enterprise by using Puppet or Chef.
    TRUSTED ADVISOR          -- online tool that provides you real time guidance to help you provision your resources following AWS best practices.
    AWS HEALTH               -- PERSONALIZED VIEW OF SERVICE HEALTH -- PROACTIVE NOTIFICATIONS DETAILED -- TROUBLESHOOTING GUIDANCE // Personal Health Dashboard + Service Health Board. While the Service Health Dashboard displays the general status of AWS services, Personal Health Dashboard gives you a personalized view into the performance and availability of the AWS services underlying your AWS resources.
    AUTO-SCALING             -- enabled by cloud-watch -- auto scaling groups (min/max)

    AWS CHATBOT              -- Slack or Amazon Chime -- AWS Chatbot processes AWS service notifications from Amazon Simple Notification Service (Amazon SNS), and forwards them to Slack or Amazon Chime chat rooms so teams can analyze and act on them.
    SERVICE QUOTAS / LIMITS  --  Service Quotas is a service for viewing and managing your quotas easily and at scale as your AWS workloads grow.  Quotas, also referred to as limits, are the maximum number of resources that you can create in an AWS account.
    SERVICE CATALOG          -- enables IT administrators to create, manage, and distribute portfolios of approved products to end users, who can then access the products they need in a personalized portal.
    LICENSE MANAGER          --  Bring Your Own License - BYOL // Sends SNS if a license is expiring. streamlines the process of bringing software vendor licenses ( for example, Microsoft, SAP, Oracle, and IBM) to the cloud.

    AWS CONTROL TOWER        -- Organizations + SSO + Service Catalog // a service that enables you to enforce and manage governance rules for security, operations, and compliance at scale across all your organizations and accounts in the AWS Cloud.
                                provides the easiest way to set up and govern a secure, compliant, multi-account AWS environment based on best practices established by working with thousands of enterprises.
    AWS ORGANIZATIONS        -- Account management service that lets you consolidate multiple AWS accounts into an organization that you create and centrally manage.
                                Includes account management and consolidated billing capabilities that enable you to better meet the budgetary, security, and compliance needs of your business.
                                Basic organization that consists of "n" accounts can be  organized into "m" organizational units (OUs) under the root.
                                The organization also has several policies that are attached to some of the OUs or directly to accounts.

    AWS PROTON               -- As an administrator, you can create service templates to provide standardized infrastructure and deployment tooling for serverless and container-based applications.
                                Then developers on your team, in turn, can select from the available service templates to automate their application or service deployments.
                                platform teams can use AWS Proton to define and manage standard application stacks that contain the architecture, infrastructure resources, and the CI/CD software deployment pipeline.
    APP-CONFIG               -- Automation -- Capability of AWS Systems Manager, to create, manage, and quickly deploy application configurations, includes built-in validation checks and monitoring.

    AWS SYSTEMS MANAGER      -- Use AWS Systems Manager to organize, monitor, and automate management tasks on your AWS resources and also use to view and control your infrastructure on AWS. AWS Systems Manager gives you visibility and control of your infrastructure on AWS.
                                Systems Manager provides a unified user interface so you can view operational data from multiple AWS services and enables you to automate operational tasks across your AWS resources.
                                After tagging the required resources in a region , you can view a consolidated dashboard in Systems Manager that reports the status of all the resources that are part of the region. If a problem arises with any of these resources, you can take corrective action immediately.
                                --> Operations Management, Application Management, Change Management, Node Management, and Shared Resources
                                Group AWS resources together by any purpose or activity you choose, such as application, environment, region, project, campaign, business unit, or software lifecycle.





##################################################################################################################
##################################################################################################################
ADDITIONAL RESOURCES  :
##################################################################################################################
##################################################################################################################

1. AWS MARKET-PLACE :  https://www.youtube.com/watch?v=RxCRHsAqiWs
---------------------------------------------------------------------
// Buy & Sell - Paid or Free AWS Software Solutions .
AWS Marketplace is an online store where you can buy or sell software that runs on Amazon Web Services (AWS).
AWS Marketplace is a curated digital catalog customers can use to find, buy, deploy, and manage third-party software, data, and services that customers need to build solutions and run their businesses.
AWS Marketplace includes thousands of software listings from popular categories such as security, networking, storage, machine learning, business intelligence, database, as well as related professional services to help you manage and support those solutions.
AWS Marketplace also simplifies software licensing and procurement with flexible pricing options and multiple deployment methods.
Customers can quickly launch pre-configured software with just a few clicks, and choose software solutions in AMI and SaaS formats, as well as other formats.
Flexible pricing options include free trial, hourly, monthly, annual, multi-year, and BYOL, and get billed from one source, AWS.
Partners use AWS Marketplace to be up and running in days and offer their software products to customers around the world.
The product can be free to use or it can have an associated charge. AWS handles billing and payments, and charges appear on customers’ AWS bill.

SELLER : seller can be an independent software vendor (ISV), consulting partner, managed services provider (MSP), or individual who has something to offer that works with AWS products and services.
One way of delivering your products to buyers is with Amazon Machine Images (AMIs) - Docker Images - SAAS Apps.
An AMI provides the information required to launch an Amazon Elastic Compute Cloud (Amazon EC2) instance.
You create a custom AMI for your product, and buyers can use it to create EC2 instances with your product already installed and ready to use.

The AWS Marketplace provides value to buyers in several ways:
    1- It simplifies software licensing and procurement with flexible pricing options and multiple deployment methods. Flexible pricing options include free trial, hourly, monthly, annual, multi-year, and BYOL.
    2- Customers can quickly launch pre-configured software with just a few clicks, and choose software solutions in AMI and SaaS formats, as well as other formats.
    3- It ensures that products are scanned periodically for known vulnerabilities, malware, default passwords, and other security-related concerns.

2. AWS GOVCLOUD - United States :
----------------------------------
The AWS GovCloud (US) Regions are isolated AWS Regions designed to enable U.S. government agencies and customers to move sensitive workloads into the cloud by addressing their specific regulatory and compliance requirements.
The AWS GovCloud (US) Regions adhere to U.S. International Traffic in Arms Regulations (ITAR) requirements.


3. AWS MANAGED SERVICES & CUSTOMER MANAGED SERVICES & SHARED CONTROLS :
-----------------------------------------------------------------------
AWS MANAGED :
-----------------
--> AWS Lambda, Amazon RDS, Amazon Redshift, Amazon CloudFront, Elastic Map Reduce, Dynamo DB
For managed services shown above, AWS is responsible for performing all the operations needed to keep the service running.
The AWS-managed services automate time-consuming administration tasks such as hardware provisioning, software setup, patching and backups. The AWS-managed services free customers to focus on their applications so they can give them the fast performance, high availability, security and compatibility they need.
Examples of AWS-managed services include Amazon RDS, Amazon DynamoDB, Amazon Redshift, Amazon WorkSpaces, Amazon CloudFront, Amazon CloudSearch, and several other services.
AWS is responsible for patching the underlying hosts, upgrading the firmware, and fixing flaws within the infrastructure for all services, including Amazon EC2.

Amazon EMR launches clusters in minutes.
You don’t need to worry about node provisioning, infrastructure setup, Hadoop configuration, or cluster tuning.
Amazon EMR takes care of these tasks so you can focus on analysis.
DynamoDB is serverless with no servers to provision, patch, or manage and no software to install, maintain, or operate.
DynamoDB automatically scales tables up and down to adjust for capacity and maintain performance.
Availability and fault tolerance are built in, eliminating the need to architect your applications for these capabilities.

CUSTOMER MANAGED :
------------------------
On the other hand, customer-managed services are services that are completely managed by the customer.
For example, a service such as Amazon Elastic Compute Cloud (Amazon EC2), Amazon VPC, and Amazon S3 are categorized as Infrastructure as a Service (IaaS) and, as such, requires the customer to perform all of the necessary security configuration and management tasks.
Customers that deploy an Amazon EC2 instance are responsible for the management of the guest operating system (including updates and security patches), any application software or utilities installed by the customer on the instances, and the configuration of the AWS-provided firewall (called a security group) on each instance.
The customer is responsible for securing their network by configuring Security Groups, Network Access control Lists (NACLs), and Routing Tables. The customer is also responsible for setting a password policy on their AWS account that specifies the complexity and mandatory rotation periods for their IAM users' passwords.
Examples of customer-managed services include Amazon Elastic Compute Cloud (Amazon EC2), Amazon Virtual Private Cloud (Amazon VPC), and AWS Identity And Access Management (AWS IAM).
Patching the guest operating system is the responsibility of AWS for the managed services only (such as Amazon RDS). The customer is responsible for patching the guest OS for other services (such as Amazon EC2).
Data encryption is the responsibility of the customer. Both SERVER SIDE & CLIENT SIDE encryption comes under client responsibility.
  NOTE :
  The AWS managed services we mentioned above are different than the AWS Managed Services (AMS) service.
  AMS is an AWS service that operates AWS on behalf of enterprise customers and partners.
  Enterprises want to adopt AWS at scale but often the skills that have served them well in traditional IT do not always translate to success in the cloud.
  AWS Managed Services (AMS) enables them to migrate to AWS at scale more quickly, reduce their operating costs, improve security and compliance and focus on their differentiating business priorities.

AWS is responsible for physical controls and environmental controls. Customers inherit these controls from AWS.
Inherited Controls are controls which a customer fully inherits from AWS such as physical controls and environmental controls.


4. AWS SHARED CONTROLS - AWS & CUSTOMER:
--------------------------------------------------
Responsibilities depends on the service used .
For example, when using Amazon EC2, you are responsible for applying operating system and application security patches regularly. However, such patches are applied automatically when using Amazon RDS.

Shared Controls are controls which apply to both the infrastructure layer and customer layers, but in completely separate contexts or perspectives.
In a shared control, AWS provides the requirements for the infrastructure and the customer must provide their own control implementation within their use of AWS services.
Examples include:
** Patch Management         – AWS is responsible for patching the underlying hosts and fixing flaws within the infrastructure, but customers are responsible for patching their guest OS and applications.
** Configuration Management – AWS maintains the configuration of its infrastructure devices, but a customer is responsible for configuring their own guest operating systems, databases, and applications.
** Awareness & Training     - AWS trains AWS employees, but a customer must train their own employees.

Patching the guest operating system is the responsibility of AWS for the managed services only (such as Amazon RDS).
The customer is responsible for patching the guest OS for other services (such as Amazon EC2).
AWS is responsible for patching the underlying hosts, upgrading the firmware, and fixing flaws within the infrastructure for all services, including Amazon EC2.

Both client side and server side encryption is a responsibility of customer.
AWS offers a lot of services and features that help AWS customers protect their data in the cloud.
Customers can protect their data by encrypting it in transit and at rest.
They can use Cloudtrail to log API and user activity, including who, what, and from where calls were made.
They can also use the AWS Identity and Access Management (IAM) to control who can access or edit their data.

5. AWS MICROSERVICES vs MONOLITHIC :
--------------------------------------
AWS recommends adopting microservices architecture, not monolithic architecture. With monolithic architectures, application components are tightly coupled and run as a single service.
With a microservices architecture, an application is built as loosely coupled components.
Benefits of microservices architecture include:
  1- Microservices allow each service to be independently scaled to meet demand for the application feature it support.
  2- Teams are empowered to work more independently and more quickly.
  3- Microservices enable continuous integration and continuous delivery, making it easy to try out new ideas and to roll back if something doesn’t work.
  4- Service independence increases an application’s resistance to failure.
     In a monolithic architecture, if a single component fails, it can cause the entire application to fail.
     With microservices, applications handle total service failure by degrading functionality and not crashing the entire application.

6.  AWS PENETRATION TESTING : https://aws.amazon.com/security/penetration-testing/
-------------------------------
Penetration testing is the practice of testing a network or web application to find security vulnerabilities that an attacker could exploit.
AWS customers are welcome to carry out security assessments and penetration tests against their AWS infrastructure without prior approval for 8 services:
So Penetration testing can be done by customers on their own instances without prior authorization from AWS.

      PERMITTED ACTIVITIES :                        PROHIBITED ACTIVITIES :
  ---------------------------------------------------------------------------------------------------------------------------------------------
      1- Amazon EC2 instances, NAT Gateways,        1. DNS zone walking via Amazon Route 53 Hosted Zones
         and Elastic Load Balancers.
      2- Amazon RDS.                                2. Denial of Service (DoS), Distributed Denial of Service (DDoS), Simulated DoS, Simulated DDoS
      3- Amazon CloudFront.                         3. Port flooding
      4- Amazon Aurora.                             4. Protocol flooding
      5- Amazon API Gateways.                       5. Request flooding  (login request flooding, API request flooding)
      6- AWS Lambda and Lambda Edge functions.
      7- Amazon Lightsail resources.
      8- Amazon Elastic Beanstalk environments.

The AWS customers are responsible for performing penetration tests against their AWS infrastructure.
AWS customers are allowed to perform penetration tests against their AWS infrastructure, but they must ensure that their activities are aligned with AWS policies.
AWS customers are allowed to perform penetration testing on both AWS-managed services such as Amazon RDS and customer-managed services such as Amazon EC2.

Note: Customers are not permitted to conduct any security assessments of AWS infrastructure, or the AWS services themselves.
      If you discover a security issue within any AWS services in the course of your security assessment, please contact AWS Security immediately.

7. HORIZONTAL & VERTICAL SCALING :
-------------------------------------
HORIZONTAL -> Adding more EC2 instances
Scaling horizontally takes place through an increase in the number of resources (e.g., adding more hard drives to a storage array or adding more servers to support an application).
This is a great way to build Internet-scale applications that leverage the elasticity of cloud computing.

VERTICAL -> Add specifications to a single resource.
Scaling vertically takes place through an increase in the specifications of an individual resource (e.g., upgrading a server with a larger hard drive, adding more memory, or provisioning a faster CPU).
On Amazon EC2, this can easily be achieved by stopping an instance and resizing it to an instance type that has more RAM, CPU, I/O, or networking capabilities. This way of scaling can eventually hit a limit and it is not always a cost efficient or highly available approach. However, it is very easy to implement and can be sufficient for many use cases especially as a short term solution.
A "vertically scalable" system is constrained to running its processes on only one computer; in such systems, the only way to increase performance is to add more resources into one computer in the form of faster (or more) CPUs, memory, or storage.
Vertical scaling may improve performance, but not fault-tolerance; because if this "one computer" fails, the whole system will fail.

Vertical-scaling is often limited to the capacity constraints of a single machine, scaling beyond that capacity often involves downtime and comes with an upper limit.
With horizontal-scaling it is often easier to scale dynamically by adding more machines in parallel. Hence, in most cases, horizontal-scaling is recommended over vertical-scaling.
Horizontally scalable systems are oftentimes able to outperform vertically scalable systems by enabling parallel execution of workloads and distributing those across many different computers.

8. AWS WELL ARCHITECTURED FRAMEWORK : https://d1.awsstatic.com/whitepapers/architecture/AWS_Well-Architected_Framework.pdf
---------------------------------------
The Well-Architected Framework identifies a set of general design principles to facilitate good design in the cloud:

  --> STOP GUESSING YOUR CAPACITY NEEDS :
      Eliminate guessing about your infrastructure capacity needs.
      When you make a capacity decision before you deploy a system, you might end up sitting on expensive idle resources or dealing with the performance implications of limited capacity.
      With cloud computing, these problems can go away.
      You can use as much or as little capacity as you need, and scale up and down automatically.

  --> TEST SYSTEMS AT PRODUCTION SCALE :
      In the cloud, you can create a production-scale test environment on demand, complete your testing, and then decommission the resources.
      Because you only pay for the test environment when it's running, you can simulate your live environment for a fraction of the cost of testing on premises.

  --> AUTOMATE TO MAKE YOUR ARCHIECTURAL EXPERIMENTATION EASIER :
      Automation allows you to create and replicate your systems at low cost and avoid the expense of manual effort.
      You can track changes to your automation, audit the impact, and revert to previous parameters when necessary.

  --> ALLOW FOR EVOLUTIONARY ARCHITECTURES :
      In a traditional environment, architectural decisions are often implemented as static, one-time events, with a few major versions of a system during its lifetime.
      As a business and its context continue to change, these initial decisions might hinder the system's ability to deliver changing business requirements.
      In the cloud, the capability to automate and test on demand lowers the risk of impact from design changes.
      This allows systems to evolve over time so that businesses can take advantage of innovations as a standard practice.

  --> DRIVE ARCHITECTURES USING DATA :
      In the cloud you can collect data on how your architectural choices affect the behaviour of your workload.
      This lets you make fact-based decisions on how to improve your workload.
      Your cloud infrastructure is code, so you can use that data to inform your architecture choices and improvements over time.

  --> IMPROVE THROUGH GAME DAYS :
      Test how your architecture and processes perform by regularly scheduling game days to simulate events in production.
      This will help you understand where improvements can be made and can help develop organizational experience in dealing with events.

The AWS Well-Architected Tool helps customers review the state of their workloads and compares them to the latest AWS architectural best practices.
The tool is based on the AWS Well-Architected Framework, developed to help cloud architects build secure, high-performing, resilient, and efficient application infrastructure.

################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
To ensure that the application has the highest level of availability :
-> Deploy across multiple regions and availability zones , not across availability zones & Subnets.
In addition to replicating applications and data across multiple data centers in the same Region using Availability Zones, you can also choose to increase redundancy and fault tolerance further by replicating data between geographic Regions (especially if you are serving customers from all over the world).
You can do so using both private, high speed networking and public internet connections to provide an additional layer of business continuity, or to provide low latency access across the globe.

####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
SECURITY is a shared responsibility.
Most (not all) data and network security are taken care of for you.
When we talk about the data/network security, AWS has a “shared responsibility model” where AWS and the customer share the responsibility of securing them.
For example, the customer is responsible for creating rules to secure their network traffic using the security groups and is also responsible for protecting data with encryption.

Gaining complete control over the physical infrastructure" is incorrect.
The Physical infrastructure is a responsibility of AWS, not the customer.

The principle of least privilege is one of the most important security practices and it means granting users the required permissions to perform the tasks entrusted to them and nothing more.
The security administrator determines what tasks users need to perform and then attaches the policies that allow them to perform only those tasks.
You should start with a minimum set of permissions and grant additional permissions when necessary. Doing so is more secure than starting with permissions that are too lenient and then trying to tighten them down.

####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
Elastic Load Balancers do not scale resources.
Elastic Load Balancers distributes incoming application traffic across multiple targets, such as Amazon EC2 instances, containers, IP addresses, and Lambda functions.

Scaling resources is a function of AWS AUTO SCALING.
AWS Auto Scaling is the feature that automates the process of adding/removing server capacity (based on demand).
Autoscaling allows you to reduce your costs by automatically turning off resources that aren’t in use.
On the other hand, Autoscaling ensures that your application runs effectively by provisioning more server capacity if required.
Auto Scaling helps you ensure that you have the correct number of Amazon EC2 instances available to handle the load for your application. You create collections of EC2 instances, called Auto Scaling groups.
You can specify the minimum number of instances in each Auto Scaling group, and Amazon EC2 Auto Scaling ensures that your group never goes below this size.

####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
What does AWS provide to deploy popular technologies - such as IBM MQ - on AWS with the least amount of effort and time?

AWS Quick Start Reference Deployments outline the architectures for popular enterprise solutions on AWS and provide AWS CloudFormation templates to automate their deployment.
Each Quick Start launches, configures, and runs the AWS compute, network, storage, and other services required to deploy a specific workload on AWS, using AWS best practices for security and availability.
Quick Starts are built by AWS solutions architects and partners to help you deploy popular technologies on AWS, based on AWS best practices.
These accelerators reduce hundreds of manual installation and configuration procedures into just a few steps, so you can build your production environment quickly and start using it immediately.
Each Quick Start includes AWS CloudFormation templates that automate the deployment and a guide that discusses the architecture and provides step-by-step deployment instructions.

AWS Quick Start Reference Deployments outline the architectures for popular enterprise solutions on AWS and provide AWS CloudFormation templates to automate their deployment.
Each Quick Start launches, configures, and runs the AWS compute, network, storage, and other services required to deploy a specific workload on AWS, using AWS best practices for security and availability.
Quick Starts are built by AWS solutions architects and partners to help you deploy popular technologies on AWS, based on AWS best practices.
These accelerators reduce hundreds of manual installation and configuration procedures into just a few steps, so you can build your production environment quickly and start using it immediately.


####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
Your customers complain that sometimes they can’t reach your application. Which AWS service allows you to monitor the performance of your EC2 instances to assist in troubleshooting these issues?
ANS : CLOUD WATCH
Amazon CloudWatch is a service that monitors AWS cloud resources and the applications you run on AWS.
You can use Amazon CloudWatch to collect and track metrics, collect and monitor log files, set alarms, and automatically react to changes in your AWS resources.
Amazon CloudWatch can monitor AWS resources such as Amazon EC2 instances, Amazon DynamoDB tables, and Amazon RDS DB instances, as well as custom metrics generated by your applications and services, and any log files your applications generate.
You can use CloudWatch to detect anomalous behavior in your environments, take automated actions, troubleshoot issues, and discover insights to keep your applications running smoothly.

AWS CloudTrail is an AWS service that can be used to monitor all user interactions with the AWS environment.

####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
IN MEMEMORY vs ELASTICACHE :
--------------------------------
The most frequently accessed data be stored in elasticache so that the application’s response time is optimal.
The primary purpose of an in-memory data store is to provide ultrafast (sub millisecond latency) and inexpensive access to copies of data.
Querying a database is always slower and more expensive than locating a copy of that data in a cache. Some database queries are especially expensive to perform.
An example is queries that involve joins across multiple tables or queries with intensive calculations. By caching (storing) such query results, you pay the price of the query only once.
Then you can quickly retrieve the data multiple times without having to re-execute the query.

#################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
--> NETWORK BASED SERVICES : Private Network Connectors to AWS            - AWS DIRECT CONNECT
                             Edge Locations for S3 Enabled applications   - AMAZON S3 TRANSFER ACCELERATION

--> ONLINE  DATA TRANSFER :  Load stream Data into S3                     - AMAZON KINESIS
                             Online transfer of active data               - AWS DATA SYNC
                             SFTP transfer to S3                          - AWS TRANSFER FOR SFTP
                             Database Transfer                            - AWS DATABSE MIGRATION
                             VM Image transfer - Live applications        - AWS SERVER MIGRATION SERVICES  // ENDURE

  // AWS ENDURE : CloudEndure Migration is a highly automated solution that simplifies the process of migrating applications from physical, virtual, and cloud-based infrastructure, ensuring that they are fully operational in any AWS Region without compatibility issues.

--> OFFLINE DATA TRANSFER :  Ship Static data in and out of AWS           - AWS SNOW FAMILY [ Snow cone / Snow Ball / Snow Mobile ]

--> HYBRID DATA STORAGE   :  Access AWS storage from On-premisis          - AWS STORAGE GATEWAY

####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################

To achieve High Availability, you need the ability to redirect traffic in the case of instance failure. There are several options:

1. USE ELASTIC LOAD BALANCER :
   This is the preferred way to provide High Availability.
   Run multiple Amazon EC2 instances, preferably in different Availability Zones (AZs).
   Users connect to the ELB (via the supplied DNS name), which redirects traffic to the EC2 instances.
   If an instance fails, ELB notices this via regular Health Checks, and will only direct traffic to the healthy instances.
   Auto Scaling can be used to create these multiple instances across multiple Availability Zones, and it can also update the Load Balancing service when it adds/removes instances.

2. REDIRECT AN ELASTIC IP ADDRESS :
   Run multiple instances (preferably across multiple Availability Zones).
   Point an Elastic IP address to the instance you desire. Users connect via the Elastic IP address and are directed to the instance.
   If the instance fails, re-associate the Elastic IP address to a different instance, which will then start receiving the traffic immediately.
   This method is not recommended because only one instance is receiving all the traffic while the other instance(s) are sitting idle.
   It also requires a mechanism to detect failure and re-associate the Elastic IP (which you must do yourself).

3. REASSIGN AN ELASTIC NETWORK INTERFACE :
   All EC2 instances have a primary ENI. They can optionally have additional ENIs.
   It is possible to direct traffic to a secondary ENI and then move that secondary ENI to another instance. This is similar to reassigning an Elastic IP address.
   This method is not recommended for the same reason as re-associating an Elastic IP address (above), but also because an ENI can only be reassigned within the same AZ. It cannot be used to direct traffic to an EC2 instance in a different AZ.

Bottom line: Use an Elastic Load Balancer. It provides true High Availability and can do it automatically.

####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
CONFIG vs CLOUD TRAIL - Change Management Tools :
Change management is defined as “the Process responsible for controlling the Lifecycle of all Changes.
The primary objective of Change Management is to enable beneficial changes to be made, with minimum disruption to IT Services.
Despite all of the investments in software and hardware, an erroneous configuration or misstep in a process can frequently undo these efforts and lead to failure.
AWS Config and AWS CloudTrail are change management tools that help AWS customers audit and monitor all resource and configuration changes in their AWS environment
Customers can use AWS Config to answer “What did my AWS resource look like?” at a point in time. Customers can use AWS CloudTrail to answer “Who made an API call to modify this resource?” For example, a customer can use the AWS Management Console for AWS Config to detect that the security group “Production-DB” was incorrectly configured in the past. Using the integrated AWS CloudTrail information, they can pinpoint which user misconfigured the “Production-DB” security group. In brief, AWS Config provides information about the changes made to a resource, and AWS CloudTrail provides information about who made those changes. These capabilities enable customers to discover any misconfigurations, fix them, and protect their workloads from failures.

####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
BYOL - Dedicated HOSTS :
You have a variety of options for using new and existing Microsoft software licenses on the AWS Cloud. By purchasing Amazon Elastic Compute Cloud (Amazon EC2) or Amazon Relational Database Service (Amazon RDS) license-included instances, you get new, fully compliant Windows Server and SQL Server licenses from AWS.
The BYOL model enables AWS customers to use their existing server-bound software licenses, including Windows Server, SQL Server, and SUSE Linux Enterprise Server.
Your existing licenses may be used on AWS with Amazon EC2 Dedicated Hosts, Amazon EC2 Dedicated Instances or EC2 instances with default tenancy using Microsoft License Mobility through Software Assurance.
Dedicated Hosts provide additional control over your instances and visibility into Host level resources and tooling that allows you to manage software that consumes licenses on a per-core or per-socket basis, such as Windows Server and SQL Server. This is why most BYOL scenarios are supported through the use of Dedicated Hosts, while only certain scenarios are supported by Dedicated Instances.

####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
TOTAL COST OF OWNERSHIP - TCO :
-----------------------------------
Which of the following should be considered when performing a TCO analysis to compare the costs of running an application on AWS instead of on-premises?  --> PHYSICAL HARDWARE

Weighing the financial considerations of owning and operating a data center facility versus employing a cloud infrastructure requires detailed and careful analysis.
The Total Cost of Ownership (TCO) is often the financial metric used to estimate and compare costs of a product or a service.
When comparing AWS with on-premises TCO, customers should consider all costs of owning and operating a data center. Examples of these costs include facilities, physical servers, storage devices, networking equipment, cooling and power consumption, data center space, and Labor IT cost.

“Application development” is incorrect. Application development is the process of creating a program or a set of programs to perform the different tasks that a business requires.
Application development is a separate process that customers need to perform regardless of whether they will be using AWS or an on-premises data center. Application development is not part of the total cost of owning and operating a data center (TCO), and thus is an incorrect answer.

“Market Research” is incorrect. Market research is an organized effort to gather information about target audience and customers to determine how viable a product or service might be.
Market research is a separate process that customers need to perform regardless of whether they will be using AWS or an on-premises data center.

“Business analysis” is incorrect. Business analysis is a multistage process aimed at identifying business needs and determining solutions to business problems.
Business analysis is a separate process that customers need to perform regardless of whether they will be using AWS or an on-premises data center.

The AWS TCO (Total Cost of Ownership) Calculator is a free tool that provides directional guidance on possible realized savings when deploying AWS.
This tool is built on an underlying calculation model, that generates a fair assessment of value that a customer may achieve given the data provided by the user which includes the number of servers migrated to AWS, the server type, the number of processors and so on.

####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
"Save when you Reserve" For Customers that can commit to using EC2 over a 1 or 3-year term, it is better to use Amazon EC2 Reserved Instances. Reserved Instances provide a significant discount (up to 75%) compared to On-Demand instance pricing.
"Pay as you go" . Reserved Instances provide a significant discount (up to 75%) compared to On-Demand (pay-as-you-go) instance pricing.
"Pay less as AWS grows" . Pay less as AWS grows refers to the discounts that you get over time as AWS grows. This sometimes called “AWS Economies of Scale”. For example, AWS has reduced the per GB storage price of S3 by 80% since the service was first introduced in 2006.
"Pay less by using more". “Pay less by using more” means that you get volume based discounts and as your usage increases.  For services such as S3, pricing is tiered, meaning the more you use, the less you pay per GB.

####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
AWS MIGRATION ACCELERATION PROGRAMME - MAP:
--------------------------------------------
The AWS Migration Acceleration Program (MAP) is a comprehensive and proven cloud migration program based upon AWS’s experience of migrating a thousand enterprise customers to the cloud.
The program packages best practices, tools, expertise, financial incentives, and a partner ecosystem to make cloud adoption easier.
MAP consists of a three-phase journey that helps you achieve your migration goals by reducing migration complexity and costs.
Leverage the performance, security, and reliability of the cloud

  Step 1: Assess your readiness
  Conduct a migration readiness assessment to identify and evaluate strengths, opportunities for improvement, and business benefits.
  Create a TCO model to build business justification for your migration project.

  Step 2: Mobilize your resources
  Create a migration plan and detailed business case. Then, gain experience by migrating pilot workloads.
  Build your capabilities and experience how easy it is to migrate to AWS.

  Step 3: Migrate and modernize your workloads
  Execute the large-scale migration plan developed during the mobilize phase with the help of migration-certified AWS Partners and the AWS ProServe team.

####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
To enable HTTPS connections to your website or application in AWS, you need an SSL/TLS server certificate.
You can use a server certificate provided by AWS Certificate Manager (ACM) or one that you obtained from an external provider.
You can use ACM or IAM to store and deploy server certificates. Use IAM as a certificate manager only when you must support HTTPS connections in a region that is not supported by ACM.
IAM supports deploying server certificates in all regions, but you must obtain your certificate from an external provider for use with AWS.
Amazon Route 53 is used to register domain names or use your own domain name to route your end users to Internet applications.
Route 53 is not responsible for creating SSL certifications.

####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
AWS recommends some practices to help organizations avoid unexpected charges on their bill. Which of the following is NOT one of these practices?
"Deleting unused AutoScaling launch configuration" will not help, and thus is the correct choice.
The AutoScaling launch configuration does not incur any charges. Thus, it will not make any difference whether it is deleted or not.

AWS will charge the user once the AWS resource is allocated (even if it is not used). Thus, it is advised that once the user's work is completed they should:
1- Delete all Elastic Load Balancers.
2- Terminate all unused EC2 instances.
3- Delete the attached EBS volumes that they don’t need.
4- Release any unused Elastic IPs.

####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
If you suspect that your account has been compromised, or if you have received a notification from AWS that the account has been compromised, perform the following tasks:
1- Change your AWS root account password and the passwords of any IAM users.
2- Delete or rotate all root and AWS Identity and Access Management (IAM) access keys.
3- Delete any potentially compromised IAM users.
4- Delete any resources on your account you didn’t create, such as EC2 instances and AMIs, EBS volumes and snapshots, and IAM users.
5- Respond to any notifications you received from AWS Support through the AWS Support Center.

####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
AWS Serverless Services include:

Compute: AWS Lambda, AWS Fargate
Messaging: Amazon SNS, Amazon SQS
Database: Amazon DynamoDB, Amazon Aurora Serverless
Orchestration: AWS Step Functions

####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
How does AWS notify customers about security and privacy events pertaining to AWS services? -- Security Bulletins page -- https://aws.amazon.com/security/security-bulletins/
AWS supports several MFA device options including Virtual MFA devices, Universal 2nd Factor (U2F) security key, and Hardware MFA devices.

####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
Amazon Cloud Directory and AWS Directory Service are two different services.
AWS Directory Service is the service that provides single sign-on (SSO) to applications and services on AWS.
AWS Directory Service uses secure Windows trusts to enable users to sign in to the AWS Management Console and the AWS Command Line Interface (CLI) using their existing corporate Microsoft Active Directory credentials

Amazon Cloud Directory is a cloud-native, highly scalable, high-performance directory service that provides web-based directories to make it easy for you to organize and manage all your application resources such as users, groups, locations, devices, and policies, and the rich relationships between them.
Unlike existing traditional directory systems, Cloud Directory does not limit organizing directory objects in a single fixed hierarchy.
In Cloud Directory, you can organize directory objects into multiple hierarchies to support multiple organizational pivots and relationships across directory information.
For example, a directory of users may provide a hierarchical view based on reporting structure, location, and project affiliation.
Similarly, a directory of devices may have multiple hierarchical views based on its manufacturer, current owner, and physical location. With Cloud Directory, you can create directories for a variety of use cases, such as organizational charts, course catalogs, and device registries.

####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
PLOT LIGHT :
-----------
A pilot light scenario is a disaster recover / business continuity scenario wherein a minimal amount of services are kept running in a failover location to enable the business to meet their Recovery Time Objective (RTO) and Recovery Point Objective (RPO) in the event of a disaster.
By nature, a pilot light scenario will take some time to spin up and promote to production (as opposed to an active-active DR scenario) and will therefore not mitigate the per-minute losses that will be experienced by the company in the event of an outage.
Additional information: Recovery time objective (RTO) and recovery point objective (RPO) are two key metrics to consider when developing a disaster recover (DR) plan. RTO represents how many hours it takes customers to return to a working state after a disaster. RPO, which is also expressed in hours, represents how much data customers could lose when a disaster happens.
For example, an RPO of 1 hour means that customers could lose up to 1 hour’s worth of data when a disaster occurs.

####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
DynamoDB global tables are ideal for massively scaled applications with globally dispersed users. Global tables provide automatic replication to AWS Regions world-wide. They enable you to deliver low-latency data access to your users no matter where they are located.
DynamoDB Accelerator (DynamoDB DAX) is an in-memory cache for DynamoDB that reduces response times from milliseconds to microseconds.
DynamoDB point-in-time recovery (PITR) is used to back up your data with per-second granularity and restore to any single second from the time it was enabled up to the prior 35 days. DynamoDB PITR works as additional insurance against accidental loss of data.
DynamoDb Streams is an optional feature that captures data modification events in DynamoDB tables.

Amazon RDS Read Replicas enable you to create one or more read-only copies of your database instance within the same AWS Region or in a different AWS Region.
Updates made to the source database are then asynchronously copied to your Read Replicas. In addition to providing scalability for read-heavy workloads, Read Replicas can be promoted to become a standalone database instance when needed.
Amazon RDS Multi-AZ deployments provide enhanced availability for database instances within a single AWS Region.
With Multi-AZ, your data is synchronously replicated to a standby in a different Availability Zone (AZ). In the event of an infrastructure failure, Amazon RDS performs an automatic failover to the standby, minimizing disruption to your applications.
For RDS, You can use Read Replicas for both improved read performance as well as Disaster Recovery (using cross-region read replicas), however, by itself, multi-AZ cannot be used for disaster recovery.


When you provision a Multi-AZ DB Instance, Amazon RDS automatically creates a primary DB Instance and synchronously replicates the data to a standby instance in a different Availability Zone (AZ).
In case of an infrastructure failure, Amazon RDS performs an automatic failover to the standby (or to a read replica in the case of Amazon Aurora), so that you can resume database operations as soon as the failover is complete.
Since the endpoint for your DB Instance remains the same after a failover, your application can resume database operation without the need for manual administrative intervention.
For RDS, Read replicas allow you to create read-only copies that are synchronized with your master database.  There is no standby available while using read replicas.
In case of infrastructure failure, you have to manually promote the read replica to be its own standalone DB Instance, which means that the database endpoint would change.

####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
Which of the following factors should be considered when determining the region in which AWS Resources will be deployed?
--> Per AWS Best Practices, proximity to your end users, regulatory compliance, data residency constraints, and cost are all factors you have to consider when choosing the most suitable AWS Region.

The other options are incorrect:
The planned number of VPCs" is incorrect. The number of VPCs a customer can have in a given region is the same irrespective of which AWS Region the customer is using.
The AWS Region’s security level" is incorrect. The level of security is almost identical for all AWS regions.
​Geographic proximity to the company's location" is incorrect.
To achieve the lowest network latency and the quickest response, the best practice is to choose the closest AWS region to the end-users (not to the company's location).
For example, if an application is developed in Japan but is primarily accessed by users in North America, the customers will have a better experience (lower application latency) if the application is deployed to AWS Regions in North America than if it were deployed to the Tokyo Region.

####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
LOCAL ZONES vs OUTPOSTS :
--------------------------
Local zones are availability zones closure to large population, industry or an IT centre.
AWS Local Zones are a type of AWS infrastructure deployment that places AWS compute, storage, database, and other select services close to large population, industry, and IT centers.
With AWS Local Zones, you can easily run applications that need single-digit millisecond latency closer to end-users in a specific geography.
AWS Local Zones are ideal for use cases such as media & entertainment content creation, real-time gaming, live video streaming, and machine learning inference.

Even though we can use cloud , some applications still need to be on premises for latency issues . Hence OUTPOSTS.
So instead of changing applications to meet both cloud and on premises apps , we can use same API's to access resources in both.
AWS Technician comes and set up all the resources needed to connect resources in cloud and resources in on-premises.
Now we connect the outpost to the nearest REGION using a DIRECT CONNECT or VPN or use a local GW to connect to on premises resources.
Also create a VPC and enable communication between your instances.
An Outpost is a pool of AWS compute and storage capacity deployed at a customer site.
AWS Outposts brings native AWS services, infrastructure, and operating models to virtually any data centre, co-location space, or on-premises facility.
You can use the same services, tools, and partner solutions to develop for the cloud and on premises .

RESERVED INSTANCES : For EC2, FARGATE, RDS, REDSHIFT, ELASTICAHE
SAVINGS PLAN       : Only for EC2 & FARGATE

####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
BEANSTALK vs CLOUD FORMATION : https://stackoverflow.com/questions/14422151/what-is-the-difference-between-elastic-beanstalk-and-cloudformation-for-a-net-p
---------------------------------------
They're actually pretty different.
Elastic Beanstalk is intended to make developers' lives easier.
Cloud Formation is intended to make systems engineers' lives easier.

Elastic Beanstalk is a PaaS-like layer on top of AWS's IaaS services which abstracts away the underlying EC2 instances, Elastic Load Balancers, auto scaling groups, etc.
This makes it a lot easier for developers, who don't want to be dealing with all the systems stuff, to get their application quickly deployed on AWS.
It's very similar to other PaaS products such as Heroku, EngineYard, Google App Engine, etc.
With Elastic Beanstalk, you don't need to understand how any of the underlying magic works.

CloudFormation, on the other hand, doesn't automatically do anything.
It's simply a way to define all the resources needed for deployment in a huge JSON file.
So a CloudFormation template might actually create two ElasticBeanstalk environments (production and staging), a couple of Elastic Cache clusters, a DyanmoDB table, and then the proper DNS in Route53.
I then upload this template to AWS, walk away, and 45 minutes later everything is ready and waiting.
Since it's just a plain-text JSON file, I can stick it in my source control which provides a great way to version my application deployments.
It also ensures that I have a repeatable, "known good" configuration that I can quickly deploy in a different region.

Behind the scenes, Elastic Beanstalk uses CloudFormation to create and maintain resources.
If your application requirements dictate more custom control, the additional functionality of CloudFormation gives you more options to control your workloads.

AWS CloudFormation supports Elastic Beanstalk application environments as one of the AWS resource types.
This allows you, for example, to create and manage an AWS Elastic Beanstalk–hosted application along with an RDS database to store the application data.
In addition to RDS instances, any other supported AWS resource can be added to the group as well.

Tools used for quicker app deployments --> [[ lightsail - quick start references - cloud formation - bean stalk - serverless application models - launch wizard - Code Star]

####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
VPC Peering vs PrivateLink:

These 2 developed separately, but have more recently found themselves intertwined.
VPC Peering - applies to VPC
PrivateLink - applies to Application/Service

With VPC Peering you connect your VPC to another VPC. Both VPC owners are involved in setting up this connection.
When one VPC, (the visiting) wants to access a resource on the other (the visited), the connection need not go through the internet.

PrivateLink provides a convenient way to connect to applications/services by name with added security.
You configure your application/service in your VPC as an AWS PrivateLink-powered service (referred to as an endpoint service).
AWS generates a specific DNS hostname for the service. Other AWS principals can create a connection to your endpoint service after you grant them permission.

VPC Peering + PrivateLink :
As of March 7, 2019, applications in a VPC can now securely access AWS PrivateLink endpoints across VPC peering connections.
AWS PrivateLink endpoints can now be accessed across both intra-region and inter-region VPC peering connections

####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################

AMAZON COMPREHEND :
----------------------
It is a natural language processing (NLP) service that uses machine learning to find insights and relationships in text. No machine learning experience required.
There is a treasure trove of potential sitting in your unstructured data. Customer emails, support tickets, product reviews, social media, even advertising copy represents insights into customer sentiment that can be put to work for your business.
The question is how to get at it? As it turns out, Machine learning is particularly good at accurately identifying specific items of interest inside vast swathes of text (such as finding company names in analyst reports), and can learn the sentiment hidden inside language (identifying negative reviews, or positive customer interactions with customer service agents), at almost limitless scale.
Amazon Comprehend uses machine learning to help you uncover the insights and relationships in your unstructured data. The service identifies the language of the text; extracts key phrases, places, people, brands, or events; understands how positive or negative the text is; analyzes text using tokenization and parts of speech; and automatically organizes a collection of text files by topic. You can also use AutoML capabilities in Amazon Comprehend to build a custom set of entities or text classification models that are tailored uniquely to your organization’s needs.
For extracting complex medical information from unstructured text, you can use Amazon Comprehend Medical. The service can identify medical information, such as medical conditions, medications, dosages, strengths, and frequencies from a variety of sources like doctor’s notes, clinical trial reports, and patient health records. Amazon Comprehend Medical also identifies the relationship among the extracted medication and test, treatment and procedure information for easier analysis. For example, the service identifies a particular dosage, strength, and frequency related to a specific medication from unstructured clinical notes.
Amazon Comprehend is fully managed, so there are no servers to provision, and no machine learning models to build, train, or deploy. You pay only for what you use, and there are no minimum fees and no upfront commitments.

AWS QUICK STARTS :
----------------------
AWS Quick Start Reference Deployments outline the architectures for popular enterprise solutions on AWS and provide AWS CloudFormation templates to automate their deployment.
Each Quick Start launches, configures, and runs the AWS compute, network, storage, and other services required to deploy a specific workload on AWS, using AWS best practices for security and availability.
Quick Starts are built by AWS solutions architects and partners to help you deploy popular technologies on AWS, based on AWS best practices.
These accelerators reduce hundreds of manual installation and configuration procedures into just a few steps, so you can build your production environment quickly and start using it immediately.
Each Quick Start includes AWS CloudFormation templates that automate the deployment and a guide that discusses the architecture and provides step-by-step deployment instructions.

AWS MARKET-PLACE :  Buy & Sell - Paid or Free AWS Software Solutions .
--------------------
AWS Marketplace is an online store where you can buy or sell software that runs on Amazon Web Services (AWS).
AWS Marketplace is a curated digital catalog customers can use to find, buy, deploy, and manage third-party software, data, and services that customers need to build solutions and run their businesses.
AWS Marketplace includes thousands of software listings from popular categories such as security, networking, storage, machine learning, business intelligence, database, as well as related professional services to help you manage and support those solutions.
AWS Marketplace also simplifies software licensing and procurement with flexible pricing options and multiple deployment methods.
Customers can quickly launch pre-configured software with just a few clicks, and choose software solutions in AMI and SaaS formats, as well as other formats.
Flexible pricing options include free trial, hourly, monthly, annual, multi-year, and BYOL, and get billed from one source, AWS.

TECHNICAL ACCOUNT MANAGER - TAM :
------------------------------------
Specific to Enterprise customers. work with a technical account manager (TAM) for your specific use cases and applications. Technical account managers play an important role in winning sales.
They are responsible for analyzing prospects' business and technical requirements and developing solutions that meet those needs.
In some cases, they may work with product development teams to customize products for individual customers.
A Technical Account Manager (TAM) is your designated technical point of contact who helps you onboard, provides advocacy and guidance to help plan and build solutions using best practices, coordinates access to subject matter experts, assists with case management, presents insights and recommendations on your AWS spend

CLOUD ADOPTION FRAMEWORK - CAF :
----------------------------------
AWS developed the AWS Cloud Adoption Framework (AWS CAF), which helps organizations understand how cloud adoption transforms the way they work.
AWS CAF leverages our experiences assisting companies around the world with their Cloud Adoption Journey.
Assessing migration readiness across key business and technical areas, referred to as Perspectives, helps determine the most effective approach to an enterprise cloud migration effort.
First, let’s outline what we mean by perspective. AWS CAF is organized into six areas of focus, which span your entire organization.
We describe these areas of focus as Perspectives: Business, People, Governance, Platform, Security, and Operations.
AWS CAF provides a mental model to establish areas of focus in determining readiness to migrate and creating a set of migration execution workstreams.
As these are key areas of the business impacted by cloud adoption, it’s important that we create a migration plan that considers and incorporates the necessary requirements across each area.

The AWS Migration Acceleration Program - MAP :
------------------------------------------------
A comprehensive and proven cloud migration program based upon AWS’s experience of migrating a thousand enterprise customers to the cloud.
The program packages best practices, tools, expertise, financial incentives, and a partner ecosystem to make cloud adoption easier.
MAP consists of a three-phase journey that helps you achieve your migration goals by reducing migration complexity and costs.
Leverage the performance, security, and reliability of the cloud

INFRASTRUCTURE EVENT MANAGEMENT - IEM :
------------------------------------------
Short-term engagement with AWS Support to get a deep understanding of your use case.
After analysis, provide architectural and scaling guidance for an event.
AWS Infrastructure Event Management is a short-term engagement with AWS Support, included in the Enterprise-level Support product offering, and available for additional purchase for Business-level Support subscribers.
AWS Infrastructure Event Management partners with your technical and project resources to gain a deep understanding of your use case and provide architectural and scaling guidance for an event.
Common use-case examples for AWS Event Management include advertising launches, new product launches, and infrastructure migrations to AWS.

AWS PARTNER NETWORK - APN :  // APN Consulting partners & APN Technology Partners
----------------------------
The AWS Partner Network (APN) is the global partner program for technology and consulting businesses that leverage Amazon Web Services to build solutions and services for customers.
The startup can work with experts from APN to build a custom solution for this infrastructure migration.

     CONSULTING : professional services firms that help customers of all types and sizes design, architect, build, migrate, and manage their workloads and applications on AWS, accelerating their migration to AWS cloud.
     TECHNOLOGY : provide hardware, connectivity services, or software solutions that are either hosted on or integrated with, the AWS Cloud. APN Technology Partners cannot help in migrating to AWS and managing applications on AWS Cloud.

TOTAL COST OF OWNERSHIP - TCO  :
------------------------------------
The Total Cost of Ownership (TCO) is often the financial metric used to estimate and compare costs of a product or a service.
When comparing AWS with on-premises TCO, customers should consider all costs of owning and operating a data center. Examples of these costs include facilities, physical servers, storage devices, networking equipment, cooling and power consumption, data center space, and Labor IT cost.
The AWS TCO (Total Cost of Ownership) Calculator is a free tool that provides directional guidance on possible realized savings when deploying AWS.
This tool is built on an underlying calculation model, that generates a fair assessment of value that a customer may achieve given the data provided by the user which includes the number of servers migrated to AWS, the server type, the number of processors and so on.

AWS ABUSE TEAM :
------------------
AWS Support can't assist with reports of abuse or questions about notifications from the AWS Abuse team.
The AWS Abuse team can assist you when AWS resources are used to engage in the following types of abusive behaviour :
  --> Spam  : You are receiving unwanted emails from an AWS-owned IP address, or AWS resources are used to spam websites or forums.
  --> Port scanning: Your logs show that one or more AWS-owned IP addresses are sending packets to multiple ports on your server, and you believe this is an attempt to discover unsecured ports.
  --> DOS attacks: Your logs show that one or more AWS-owned IP addresses are used to flood ports on your resources with packets, and you believe that this is an attempt to overwhelm or crash your server or the software running on your server.
  --> Intrusion attempts: Your logs show that one or more AWS-owned IP addresses are used to attempt to log in to your resources.
  --> Hosting objectionable or copyrighted content: You have evidence that AWS resources are used to host or distribute illegal content or distribute copyrighted content without the consent of the copyright holder.
  --> Distributing malware: You have evidence that AWS resources are used to distribute software that was knowingly created to compromise or cause harm to computers or machines on which it is installed.
If you suspect that AWS resources are used for abusive purposes, contact the AWS Abuse team using the Report Amazon AWS abuse form, or by contacting abuse@amazonaws.com.
Provide all the necessary information, including logs in plaintext, email headers, and so on, when you submit your request.
The AWS Abuse team doesn't open attachments under any circumstance. You must provide any necessary information in plaintext.

AWS TRUSTED ADVISOR :
------------------------
Cost Optimization -  Performance -  Security - Fault Tolerance -  Service Limits
AWS Trusted Advisor offers a rich set of best practice checks and recommendations across five categories: cost optimization, security, fault tolerance, performance, and service limits.
AWS Trusted Advisor is your customized cloud expert! It helps you to observe best practices for the use of AWS by inspecting your AWS environment with an eye toward saving money, improving system performance and reliability, and closing security gaps.
All AWS customers can start optimizing their cloud resources by using four of AWS Trusted Advisor’s most popular best-practice recommendations on service limits and security configurations.
Access to the four free checks does not expire, and you can access the remaining 33 checks and additional features by upgrading to Business or Enterprise-level Support.

The four free checks include : Security Groups - Specific Ports Unrestricted, Service Limits, IAM Use, and MFA on Root Account
You can also subscribe to Business and Enterprise-level support to access the full suite of Trusted Advisor best-practice checks, API access, and a host of other support features, such as phone and chat access to support personnel, architecture guidance, API access, and help with third-party software configuration.
AWS Trusted Advisor is an online tool that provides you real time guidance to help you provision your resources following AWS best practices.
Trusted Advisor checks help optimize your AWS infrastructure, increase security and performance, reduce your overall costs, and monitor service limits.
Whether establishing new workflows, developing applications, or as part of ongoing improvement, take advantage of the recommendations provided by Trusted Advisor on a regular basis to help keep your solutions provisioned optimally.

AWS Basic Support and AWS Developer Support customers get access to 6 security checks (S3 Bucket Permissions, Security Groups - Specific Ports Unrestricted, IAM Use, MFA on Root Account, EBS Public Snapshots, RDS Public Snapshots) and 50 service limit checks.
AWS Business Support and AWS Enterprise Support customers get access to all 115 Trusted Advisor checks (14 cost optimization, 17 security, 24 fault tolerance, 10 performance, and 50 service limits) and recommendations.


####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
A policy is an object in AWS that, when associated with an identity or resource, defines their permissions.
AWS evaluates these policies when an IAM principal (user or role) makes a request. Permissions in the policies determine whether the request is allowed or denied.

Each policy consists of:

1- Principal: Who needs access.
2- Action: What action to allow or deny.
3- Resource: Which resource to allow or deny the action on.
4- Effect: What will be the effect when the user requests access - either allow or deny.
5- Condition: Which conditions must be present for the policy to take effect.
              For example, you might allow access only to the specific S3 buckets if the user is connecting from a specific IP range or has used multi-factor authentication at login.
Note: Permissions are granted to IAM identities (users, groups, and roles) to determine whether they are authorized to perform an action or not.

####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################

Chef and Puppet are automation platforms that allow you to use code to automate the configurations of your servers.
OpsWorks lets you use Chef and Puppet to automate how servers are configured, deployed, and managed across your Amazon EC2 instances or on-premises compute environments.

Placement Groups are logical groupings or clusters of EC2 instances within a single Availability Zone.
Placement groups are recommended for applications that require low network latency, high network throughput, or both.

S3 pricing is based on four factors:
1) Total amount of data (in GB) stored on S3
2) Storage class (S3 Standard, S3 Intelligent-Tiering, S3 Standard-Infrequent Access, S3 One Zone-IA, S3 Glacier, or S3 Glacier Deep Archive)
3) Amount of data transferred out of AWS from S3. // data transferred in to S3 is not at all a factor, it is free of cost
4) Number of requests to S3

Data protection refers to protecting data while in-transit (as it travels to and from Amazon S3) and at rest (while it is stored on disks in Amazon data centers).
You can protect data in transit by using SSL/TLS or by using client-side encryption.
Also, you have the following options of protecting data at rest in Amazon S3.
1- Use Server-Side Encryption – You configure Amazon S3 to encrypt your object before saving it on disks in its data centers and decrypt it when you download the objects. ( by encrypting data prior to uploading it )
2- Use Client-Side Encryption – You can encrypt your data on the client-side and upload the encrypted data to Amazon S3. In this case, you manage the encryption process, the encryption keys, and related tools. ( by using s3 encryption while at rest )

Amazon DynamoDB Accelerator (DAX) is a fully managed, highly available, in-memory cache for DynamoDB that delivers performance improvements from milliseconds to microseconds – even at millions of requests per second.
DAX adds in-memory acceleration to your DynamoDB tables without requiring you to manage cache invalidation, data population, or cluster management.
Multi-AZ and Read Replica are Amazon RDS features not for Dynamo Db.

AWS Directory Service is a managed Microsoft Active Directory in the AWS Cloud.
Customers can use it to manage users and groups, provide single sign-on (SSO) to applications and services, as well as create and apply group policies.
Note: What is Single sign-on (SSO)? AWS Single sign-on (AWS SSO) enables a company’s employees to sign in to AWS using their existing corporate Microsoft Active Directory credentials.

Amazon EBS pricing has two factors:
1- Volumes: Volume storage for all EBS volume types is charged by the amount of GB you provision per month, until you release the storage.
2- Snapshots: Snapshot storage is based on the amount of space your data consumes in Amazon S3.
   Because Amazon EBS does not save empty blocks, it is likely that the snapshot size will be considerably less than your volume size.
   Copying EBS snapshots is charged based on the volume of data transferred across regions.
   For the first snapshot of a volume, Amazon EBS saves a full copy of your data to Amazon S3.
   For each incremental snapshot, only the changed part of your Amazon EBS volume is saved.
   After the snapshot is copied, standard EBS snapshot charges apply for storage in the destination region.

  When you want to reduce the costs of Amazon EBS consider the following:
  1- Delete Unattached Amazon EBS Volumes: An easy way to reduce wasted spend is to find and delete unattached volumes. However, when EC2 instances are stopped or terminated, attached EBS volumes are not automatically deleted and will continue to accrue charges since they are still operating.
  2- Resize or Change the EBS Volume Type: Another way to optimize storage costs is to identify volumes that are underutilized and downsize them or change the volume type.
  3- Delete Stale Amazon EBS Snapshots: If you have a backup policy that takes EBS volume snapshots daily or weekly, you will quickly accumulate snapshots. Check for stale snapshots that are over 30 days old and delete them to reduce storage costs.

AWS Virtual Private Network (AWS VPN connection goes over the public internet) provides an internet-based connection that enables customers to connect their on-premises network or branch office site to AWS.
Internet-based connectivity can have unpredictable performance and despite being encrypted, can present security concerns.
AWS Direct Connect bypasses the public Internet and uses a standard Ethernet fiber-optic cable to establish a secure, dedicated, and more consistent connectivity from on-premises data centers into AWS.
AWS VPN is incorrect because transferring large data sets over the Internet can be time consuming and expensive.
Additionally, AWS VPN is an internet-based connection and does not meet the requirement of consistent connectivity.
But Unlike AWS Direct Connect, VPN Connections can be configured in minutes and are a good solution if customers have an immediate need, have low to modest bandwidth requirements, and can tolerate the inherent variability in Internet-based connectivity.

AWS Global Accelerator is a networking service that improves the availability and performance of the applications that you offer to your global users.
Today, if you deliver applications to your global users over the public internet, your users might face inconsistent availability and performance as they traverse through multiple public networks to reach your application.
These public networks can be congested and each hop can introduce availability and performance risk.
AWS Global Accelerator uses the highly available and congestion-free AWS global network to direct internet traffic from your users to your applications on AWS, making your users’ experience more consistent.
To improve the availability of your application, you must monitor the health of your application endpoints and route traffic only to healthy endpoints.
AWS Global Accelerator improves application availability by continuously monitoring the health of your application endpoints and routing traffic to the closest healthy endpoints.

Amazon S3 Transfer Acceleration is used to enable fast transfers of files over long distances between your client and an S3 bucket.
You might want to use Transfer Acceleration on a bucket for various reasons, including the following:
1- You have customers that upload to a centralized bucket from all over the world.
2- You transfer gigabytes to terabytes of data on a regular basis across continents.
3- You are unable to utilize all of your available bandwidth over the Internet when uploading to Amazon S3.

AWS has eliminated "bidding" in the new AWS Spot instance pricing model.
The way the new pricing model works is that you just pay the Spot price that’s in effect for the current hour for the instances that you launch.
It’s that simple. Now you can request Spot capacity just like you would request On-Demand capacity, without having to spend time analyzing market prices or setting a bid price.
In the new model, the Spot prices are more predictable, updated less frequently, and are determined by the long-term supply and demand for Amazon EC2 spare capacity, not bid prices.
Your Spot Instance runs whenever capacity is available and the maximum price per hour for your request exceeds the Spot price.
An example to illustrate: If the current AWS Spot price is $0.08 per hour and you set a maximum price of $0.17, you’ll pay $0.08 and you will lose the instances if the AWS Spot price rises above $0.17 or if capacity is no longer available.

Reserved instance types include:
- Standard RIs: These provide the most significant discount (up to 72% off On-Demand) and are best suited for steady-state usage.
- Convertible RIs: These provide a discount (up to 54% off On-Demand) and the capability to change the attributes of the RI as long as the exchange results in the creation of Reserved Instances of equal or greater value.
Therefore, Standard RIs provides more discounts than Convertible RIs.


AWS Snowball is a petabyte-scale data transport solution that uses secure appliances to transfer large amounts of data into and out of the AWS cloud.
Using Snowball addresses common challenges with large-scale data transfers, including high network costs, long transfer times, and security concerns.
Transferring data with Snowball is simple, fast, secure, and can cost as little as one-fifth the cost of using high-speed internet.
 --> Additionally, With AWS Snowball, you can access the compute power of the AWS Cloud locally and cost-effectively in places where connecting to the internet might not be an option. AWS Snowball is a perfect choice if you need to run computing in rugged, austere, mobile, or disconnected (or intermittently connected) environments.
 --> With AWS Snowball, you have the choice of two devices, Snowball Edge Compute Optimized with more computing capabilities, suited for higher performance workloads, or Snowball Edge Storage Optimized with more storage, which is suited for large-scale data migrations and capacity-oriented workloads.
Snowball Edge Storage Optimized devices provides up to 80 TB of usable storage.


AWS Cost Explorer has an easy-to-use interface that lets you visualize, understand, and manage your AWS costs and usage over time.
Cost Explorer’s cost forecast capabilities use machine learning to learn each customer’s historical spend patterns and use that information to forecast expected costs.
Cost Explorer’s forecasting enables you to get a better idea of what your costs and usage may look like in the future, so that you can plan ahead.
Forecasting capabilities have been enhanced to support twelve month forecasts (previously forecasts were limited to three months) for multiple cost metrics, including unblended and amortized costs.
AWS Cost Explorer forecasts your future costs based on your past usage; NOT based on your expected usage.
The AWS tool that can provide accurate estimates of AWS service costs based on your expected usage is the AWS Simple Monthly Calculator.
For example, if you are planning to use 500 GB of S3 storage, you can input this value directly in the AWS Simple Monthly Calculator interface and the calculator provides an estimate of what you will pay monthly for this amount of storage.


When you launch instances on a Dedicated Host, the instances run on a physical server that is dedicated for your use.
While Dedicated instances also run on dedicated hardware, Dedicated Hosts provide further visibility and control by allowing you to place your instances on a specific, physical server.
This enables you to deploy instances using configurations that help address corporate compliance and regulatory requirements.
Note:   Amazon EC2 purchasing options include: On-Demand, Savings Plans, Reserved Instances, Spot Instances, Dedicated Hosts and Dedicated instances.
        Dedicated Instances also provides Hardware isolation. Dedicated Instances are Amazon EC2 instances that run in a virtual private cloud (VPC) on hardware that's dedicated to a single customer.
        Your Dedicated instances are physically isolated at the host hardware level from instances that belong to other AWS accounts. However, Dedicated Instances may share hardware with other instances from the same AWS account that are not Dedicated Instances.

The difference between Dedicated Hosts and Dedicated Instances:
1- Dedicated Instances guarantee that the instances will run on hardware that's dedicated to a single AWS account.
   But, as we mentioned above, Dedicated Instances may share hardware with other instances from the same AWS account that are not Dedicated Instances.
   That is not the case with Dedicated Hosts. Dedicated Hosts allow you to control how instances are placed on a specific physical server, and you can consistently deploy your instances to the same physical server over time.
   For that reason, Dedicated Hosts is a better option to handle compliance and regulatory requirements in most scenarios.
2- Dedicated Hosts enable you to benefit from the Bring Your Own License (BYOL) model for almost every BYOL scenario, while only certain scenarios are supported by Dedicated Instances.
   The BYOL model enables AWS customers to use their existing server-bound software licenses, including Windows Server, SQL Server, and SUSE Linux Enterprise Server.
   Dedicated Hosts provide additional control over your instances and visibility into Host level resources and tooling that allows you to manage software that consumes licenses on a per-core or per-socket basis, such as Windows Server and SQL Server.
   This is why most BYOL scenarios are supported through the use of Dedicated Hosts, while only certain scenarios are supported by Dedicated Instances.


When you enable Multi-AZ, Amazon Relational Database Service (Amazon RDS) maintains a redundant and consistent standby copy of your data.
If you encounter problems with the primary copy, Amazon RDS automatically switches to the standby copy (or to a read replica in the case of Amazon Aurora) to provide continued availability to the data.
The two copies are maintained in different Availability Zones (AZs), hence the name “Multi-AZ.” Each AZ runs on its own physically distinct, independent infrastructure, and is engineered to be highly reliable.
Having separate Availability Zones greatly reduces the likelihood that both copies will concurrently be affected by most types of disturbances.

Federation is an AWS feature that enables users to access and use AWS resources using their existing corporate credentials.
Federation uses open standards, such as Security Assertion Markup Language 2.0 (SAML), to exchange identity and security information between an identity provider (IdP) and an application.
AWS offers multiple options for federating your identities in AWS:
1- AWS Identity and Access Management (IAM): You can use AWS Identity and Access Management (IAM) to enable users to sign in to their AWS accounts with their existing corporate credentials.
2- AWS Directory Service: AWS Directory Service for Microsoft Active Directory, also known as AWS Microsoft AD, uses secure Windows trusts to enable users to sign in to the AWS Management Console, AWS Command Line Interface (CLI), and Windows applications running on AWS using their existing corporate Microsoft Active Directory credentials.
3- AWS Single-Sign-On (AWS SSO) Service: You can use the AWS SSO service to federate your identities into your AWS environment.


CREDENTIAL REPORT :
You can generate and download a credential report that lists all users in your account and the status of their various credentials, including passwords, access keys, and MFA devices.
You can get a credential report from the AWS Management Console, the AWS SDKs, and Command Line Tools.
You can use credential reports to assist in your auditing and compliance efforts.
You can use the report to audit the effects of credential lifecycle requirements, such as password and access key rotation.
You can provide the report to an external auditor, or grant permissions to an auditor so that he or she can download the report directly.


In relation to Amazon RDS databases:
  AWS is responsible for:
      1- Managing the underlying infrastructure and foundation services.
      2- Managing the operating system.
      3- Database setup.
      4- Patching and backups.
  The customer is still responsible for:
      1- Protecting the data stored in databases (through encryption and IAM access control).
      2- Managing the database settings that are specific to the application.
      3- Building the relational schema.
      4- Network traffic protection.


AWS Cost Governance Best Practices:
1- Resource controls (policy-based and automated) govern who can deploy resources and the process for identifying, monitoring, and categorizing these new resources. These controls can use tools such as AWS Service Catalog, AWS Identity and Access Management (IAM) roles and permissions, and AWS Organizations, as well as third-party tools such as ServiceNow.
2- Cost allocation applies to teams using resources, shifting the emphasis from the IT-as-cost-center mentality to one of shared responsibility.
3- Budgeting processes include reviewing budgets and realized costs, and then acting on them.
4- Architecture optimization focuses on the need to continually refine workloads to be more cost-conscious to create better architected systems.
5- Tagging and tagging enforcement ensure cost tracking and visibility across organization lines.

####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################

AWS environments are continuously audited, and its infrastructure and services are approved to operate under several compliance standards and industry certifications across geographies and industries, including PCI DSS, ISO 2700, ISO 9001, and HIPAA.
You can use these certifications to validate the implementation and effectiveness of AWS security controls.
For example, AWS companies that use AWS products and services to handle credit card information can rely on AWS technology infrastructure as they manage their PCI DSS compliance certification.
AWS services are assessed regularly to comply with common compliance standards NOT with local laws and regulations.

The Payment Card Industry Data Security Standard (PCI DSS) helps ensure that companies maintain a secure environment for storing, processing, and transmitting credit card information or sensitive authentication data (SAD).
AWS customers who use AWS services to store, process, or transmit cardholder data can rely on AWS infrastructure as they manage their own PCI DSS compliance certification.
Security and compliance are important shared responsibilities between AWS and the customer.
It is the customer’s responsibility to maintain their PCI DSS cardholder data environment (CDE) and scope, and be able to demonstrate compliance of all PCI controls, but customers are not alone in this journey.
The use of PCI DSS compliant AWS services can facilitate customer compliance, and the AWS Security Assurance Services team can assist customers with additional information specific to demonstrating the PCI DSS compliance of their AWS workloads.
AWS Services listed as PCI DSS compliant means that they have the ability to be configured by customers to meet their PCI DSS requirements. It does not mean that any use of that service is automatically compliant.
A good rule-of-thumb is that if a customer can set a particular configuration, they are responsible for setting it appropriately to meet PCI DSS requirements.
AWS customers are also responsible for creating a policy that addresses information security for all personnel, and implementing strong access controls to restrict any access to cardholder data.


There are seven design principles for security in the cloud:
1- Implement a strong identity foundation: Implement the principle of least privilege and enforce separation of duties with appropriate authorization for each interaction with your AWS resources. Centralize privilege management and reduce or even eliminate reliance on long-term credentials.
2- Enable traceability: Monitor, alert, and audit actions and changes to your environment in real time. Integrate logs and metrics with systems to automatically respond and take action.
3- Apply security at all layers: Rather than just focusing on protection of a single outer layer, apply a defense-in-depth approach with other security controls. Apply to all layers (e.g., edge network, VPC, subnet, load balancer, every instance, operating system, and application).
4- Automate security best practices: Automated software-based security mechanisms improve your ability to securely scale more rapidly and cost effectively. Create secure architectures, including the implementation of controls that are defined and managed as code in version-controlled templates.
5- Protect data in transit and at rest: Classify your data into sensitivity levels and use mechanisms, such as encryption, tokenization, and access control where appropriate.
6- Keep people away from data: Create mechanisms and tools to reduce or eliminate the need for direct access or manual processing of data. This reduces the risk of loss or modification and human error when handling sensitive data.
7- Prepare for security events: Prepare for an incident by having an incident management process that aligns to your organizational requirements. Run incident response simulations and use tools with automation to increase your speed for detection, investigation, and recovery.


AWS Service Control Policies (SCPs):
AWS Service Control Policies (SCPs) or AWS Organizations Policies are a type of organization policy that you can use to manage permissions for all accounts in your organization.
SCPs offer central control over the maximum available permissions for all member accounts in your organization. SCPs help you to ensure member accounts stay within your organization's access control guidelines.
In SCPs, you can restrict which AWS services, resources, and individual API actions the users and roles in each member account can access.
When AWS Organizations blocks access to a service, resource, or API action for a member account, a user or role in that account can't access it.
This block remains in effect even if an administrator of a member account explicitly grants such permissions in an IAM policy.

    Additional information:  What's the difference between an AWS Organizations service control policy (SCP) and an IAM policy?
    An IAM policy provides granular control over what users and roles in individual accounts can do. AWS Organizations expands that control to the account level by giving you control over what users and roles in an account or a group of accounts can do.
    The resulting permissions are the logical intersection of what is allowed by AWS Organizations at the account level and the permissions that are explicitly granted by IAM at the user or role level within that account.
    In other words, the user can access only what is allowed by both the AWS Organizations policies and IAM policies. If either blocks an operation, the user can't access that operation.
    For example, if an SCP applied to an account states that the only actions allowed are Amazon EC2 actions, and the permissions on a principal (IAM user or role) in the same AWS account allow both EC2 actions and Amazon S3 actions, the principal is able to access only the EC2 actions.

5 PILLARS :
-------------
    Performance Efficiency - >   // Horizontal & Vertical Scaling & Serverless architecture -- Experiment more often
    The performance efficiency pillar includes the ability to use computing resources efficiently to meet system requirements.
    Key topics include selecting the right resource types and sizes based on workload requirements, monitoring performance, and making informed decisions to maintain efficiency as business needs evolve.
    On how you can run services efficiently and scalably.

    Reliability            - >  // Availability Zone & Region -- Stop guessing about capacity
    The reliability pillar includes the ability of a system to recover from infrastructure or service disruptions, dynamically acquire computing resources to meet demand, and mitigate disruptions such as misconfigurations or transient network issues.
    A resilient workload quickly recovers from failures to meet business and customer demand.
    Key topics include distributed system design, recovery planning, and how to handle change.
    On how you can build services that are resilient to both service and infrastructure disruptions.

    Operational excellence - >   // Automation to escape human errors - Cloud formation to automate -- Anticipate failure -- Make frequent, small, reversible changes
    The operational excellence pillar includes the ability to run and monitor systems to deliver business value and to continually improve supporting processes and procedures.
    Key topics include automating changes, responding to events, and defining standards to manage daily operations.
    On how you can continuously improve your ability to run systems, create better procedures, and gain insights.

    Cost optimization      - >   // cost explorer - Budgets - Cost usage
    On the ability to avoid or eliminate unneeded cost or sub-optimal resources. CAPEX to OPEX

    Security               - >  Shared Model // Policies (IAM) , Network Security (VPC) , Encryption (KMS), IAM, KMS, MFA, Cloud Trail, Cloud Watch, SNS, Email. -- Enable traceability
    The security pillar includes the ability to protect information, systems, and assets while delivering business value through risk assessments and mitigation strategies.
    Key topics include confidentiality and integrity of data, identifying and managing who can do what with privilege management, protecting systems, and establishing controls to detect security events.

To protect your AWS infrastructure in this situation you should lock down your root user account and all IAM user accounts that the administrator had access to.
To protect your AWS infrastructure you should:
1- Change the email address and the password of the root user account
2- Enable MFA on the root user account
3- Rotate (change) all access keys for all accounts
4- Change the user name and password of all IAM users
5- Enable MFA on all IAM user accounts


Amazon CloudFront charges are based on the data transfer out of AWS and requests used to deliver content to your customers.
There are no upfront payments or fixed platform fees, no long-term commitments, no premiums for dynamic content, and no requirements for professional services to get started.
To estimate the costs of an Amazon CloudFront distribution consider the following:
  - Traffic Distribution: Data transfer and request pricing varies across geographic regions, and pricing is based on the edge location through which your content is served.
  - Requests: The number and type of requests (HTTP or HTTPS) made and the geographic region in which the requests are made.
  - Data Transfer OUT: The amount of data transferred out of your Amazon CloudFront edge locations.
Note: Data Transfer IN is free. There is no charge for inbound data transferred from AWS services such as Amazon S3 or Elastic Load Balancing.


Durability refers to the ability of a system to assure data is stored and data remains consistent in the system as long as it is not changed by legitimate access.
This means that data should not become corrupted or disappear due to a system malfunction. Durability is used to measure the likelihood of data loss.
For example, assume you have confidential data stored in your Laptop.
If you make a copy of it and store it in a secure place, you have just improved the durability of that data. It is much less likely that all copies will be simultaneously destroyed.
Amazon EBS volume data is replicated across multiple servers in an Availability Zone to prevent the loss of data from the failure of any single component.
The replication of data makes EBS volumes 20 times more durable than typical commodity disk drives, which fail with an AFR (annual failure rate) of around 4%. For example, if you have 1,000 EBS volumes running for 1 year, you should expect 1 to 2 will have a failure.
Additional information :  Amazon S3 is also considered a durable storage service. Amazon S3 is designed for 99.999999999% (11 9’s) durability. This means that if you store 100 billion objects in S3, you will lose one object at most.

AWS Organizations has five main benefits:
1) Centrally manage access polices across multiple AWS accounts.
2) Automate AWS account creation and management.
3) Control access to AWS services. - SCP's
4) Consolidate billing across multiple AWS accounts.
5) Configure AWS services across multiple accounts.
** Control access to AWS services: AWS Organizations allows you to restrict what services and actions are allowed in your accounts. You can use Service Control Policies (SCPs) to apply permission guardrails on AWS Identity and Access Management (IAM) users and roles. For example, you can apply an SCP that restricts users in accounts in your organization from launching any resources in regions that you do not explicitly allow.
** Consolidate billing across multiple AWS accounts: You can use AWS Organizations to set up a single payment method for all the AWS accounts in your organization through consolidated billing. With consolidated billing, you can see a combined view of charges incurred by all your accounts, as well as take advantage of pricing benefits from aggregated usage, such as volume discounts for Amazon EC2 and Amazon S3.

####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################

You can remove an account from your organization only if the account has the information that is required for it to operate as a standalone account.
For each account that you want to make standalone, you must accept the AWS Customer Agreement, choose a support plan, provide and verify the required contact information, and provide a current payment method.
AWS uses the payment method to charge for any billable (not AWS Free Tier) AWS activity that occurs while the account isn't attached to an organization.
The AWS account must not have any Service Control Policies (SCPs) attached to it ( IS INCORRECT ). This is not a pre-requisite to remove the AWS account.
The principals in the AWS account are no longer affected by any service control policies (SCPs) that were defined in the organization.
This means that restrictions imposed by those SCPs are gone, and the users and roles in the account might have more permissions than they had before.

AMAZON POLLY      - Amazon Polly is a service that turns text into lifelike speech, allowing you to create applications that talk, and build entirely new categories of speech-enabled products.
                    Polly's Text-to-Speech (TTS) service uses advanced deep learning technologies to synthesize natural sounding human speech. It cannot be used to discover and protect your sensitive data in AWS.
                    Polly's Text-to-Speech (TTS) service uses advanced deep learning technologies to synthesize natural sounding human speech.
AMAZON TRANSLATE  - Amazon Translate is used for language translation.
                    Amazon Translate uses neural machine translation via deep learning models to deliver more accurate and more natural-sounding translation than traditional statistical and rule-based translation algorithms.
AMAZON TRANSCRIBE - You can use Amazon Transcribe to add speech-to-text capability to your applications.
                    Amazon Transcribe uses a deep learning process called automatic speech recognition (ASR) to convert speech to text quickly and accurately.
                    Amazon Transcribe can be used to transcribe customer service calls, to automate closed captioning and subtitling, and to generate metadata for media assets.

ELASTIC TRANSCODER - Is a media transcoding service.
                     It is designed to be a highly scalable, easy-to-use, and cost-effective way to convert (or transcode) media files from their source format into versions that will play back on devices like smartphones, tablets, and PCs.
REKOGNITION        - With Amazon Rekognition, you can identify objects, people, text, scenes, and activities in images and videos, as well as detect any inappropriate content.
                     Amazon Rekognition also provides highly accurate facial analysis and facial search capabilities that you can use to detect, analyze, and compare faces for a wide variety of user verification, people counting, and public safety use cases.
AMAZON LEX         - Amazon Lex is a service for building conversational interfaces using voice and text.  ( To build a chatbot using NATURAL LANGUAGE UNDERSTAND -- NLU )
                     Powered by the same conversational engine as Alexa, Amazon Lex provides high-quality speech recognition and language understanding capabilities, enabling the addition of sophisticated, natural language ‘chatbots’ to new and existing applications.
SAGEMAKER          - Amazon SageMaker is a fully-managed platform that enables developers and data scientists to quickly and easily build, train, and deploy machine learning models at any scale.
                     Amazon SageMaker removes all the barriers that typically slow down developers who want to use machine learning.
AWS IoT Core       - AWS IoT Core lets you connect IoT devices to the AWS cloud without the need to provision or manage servers.
                     AWS IoT Core can support billions of devices and trillions of messages and can process and route those messages to AWS endpoints and to other devices reliably and securely.
                     With AWS IoT Core, your applications can keep track of and communicate with all your devices, all the time, even when they aren’t connected.
                     AWS IoT Core also makes it easy to use AWS and Amazon services like AWS Lambda, Amazon Kinesis, Amazon S3, Amazon SageMaker, Amazon DynamoDB, Amazon CloudWatch, AWS CloudTrail, Amazon QuickSight, and Alexa Voice Service to build IoT applications that gather, process, analyze and act on data generated by connected devices, without having to manage any infrastructure.

SQS & SNS --> Can be used to decouple components of a microservices based application on AWS Cloud.
Each AWS Region consists of two or more Availability Zones & Each Availability Zone (AZ) consists of one or more discrete data centers

You are not charged for data transfer IN into AWS and data transfer B/W services within a same region (S3 to EC2 etc;)
There are three fundamental drivers of cost with AWS: compute, storage, and outbound data transfer.
In most cases, there is no charge for inbound data transfer or data transfer between other AWS services within the same region.
Outbound data transfer is aggregated across services and then charged at the outbound data transfer rate.
Per AWS pricing, data transfer between S3 and EC2 instances within the same region is not charged, so there would be no data transfer charge for moving 500 GB of data from an EC2 instance to an S3 bucket in the same region.
You are also not charged for data retrieval from S3 Standard and S3 Intelligent Tiering storage types.


When you create a record, you choose a routing policy, which determines how Amazon Route 53 responds to queries:
Simple routing policy – Use for a single resource that performs a given function for your domain, for example, a web server that serves content for the example.com website.
Failover routing policy – Use when you want to configure active-passive failover.
Geolocation routing policy – Use when you want to route traffic based on the location of your users.
Geoproximity routing policy – Use when you want to route traffic based on the location of your resources and, optionally, shift traffic from resources in one location to resources in another.
Latency routing policy – Use when you have resources in multiple AWS Regions and you want to route traffic to the region that provides the best latency.
Multivalue answer routing policy – Use when you want Route 53 to respond to DNS queries with up to eight healthy records selected at random.
Weighted routing policy – Use to route traffic to multiple resources in proportions that you specify.

PRICES COMPARED TO ON-DEMAND INSTANCES ----- > SPOT - 90%, RESERVED STANDARD - 75%, DEDICATED HOSTS - 70%, RESERVED CONVERTABLE -50%
The percentage savings for each Reserved option is as follows:
    All you need to remember is that a 3 years term would always be more cost-effective than a 1-year term.
    Then within a term, "all upfront" is better than "partial upfront" which in turn is better than "no upfront" from a cost savings perspective.

          "No upfront payment option with the standard 1-year term"       - 36%
          "Partial upfront payment option with the standard 3-years term" - 39%
          "All upfront payment option with the standard 1-year term"      - 40%

          "No upfront payment option with the standard 3-years term"      - 56%
          "Partial upfront payment option with the standard 3-years term" - 59%
          "All upfront payment option with the standard 1-year term       - 62%

A VPC endpoint enables you to privately connect your VPC to supported AWS services and VPC endpoint services powered by AWS PrivateLink without requiring an internet gateway, NAT device, VPN connection, or AWS Direct Connect connection.
Instances in your VPC do not require public IP addresses to communicate with resources in the service. Traffic between your VPC and the other service does not leave the Amazon network.
There are two types of VPC endpoints: INTERFACE ENDPOINTS & GATEWAY ENDPOINTS.
INTERFACE ENDPOINTS : An elastic network interface with a private IP address from the IP address range of your subnet that serves as an entry point for traffic destined to a supported service.
                      Interface endpoints are powered by AWS Private Link, a technology that enables you to privately access services by using private IP addresses.
GATEWAY ENDPOINTS   : A gateway endpoint is a gateway that you specify as a target for a route in your route table for traffic destined to a supported AWS service.
                      The following AWS services are supported: AMAZON S3 & DYNAMO DB.

https://aws.amazon.com/premiumsupport/plans/
https://docs.aws.amazon.com/general/latest/gr/root-vs-iam.html#aws_tasks-that-require-root

#READ QUESTION CAREFULLY -- REGIONAL vs GLOBAL SERVICES
Most of the services that AWS offers are Region specific.
But few services, by definition, need to be in a global scope because of the underlying service they offer. AWS IAM, Amazon CloudFront, Route 53 and WAF are some of the global services.
Amazon S3 is a unique service in the sense that it follows a global namespace but the buckets are regional. You specify an AWS Region when you create your Amazon S3 bucket. This is a regional service.

AWS services can be accessed in three different ways: // API GATEWAY IS INCORRECT
AWS Management Console - This is a simple web interface for accessing AWS services.
AWS Command Line Interface (CLI) - You can access AWS services from the command line and automate service management with scripts.
AWS Software Developer Kit (SDK) - You can also access via AWS SDK that provides language-specific abstracted APIs for AWS services.

A research lab wants to optimize the caching capabilities for its scientific computations application running on EC2 instances. Which EC2 storage option is best suited for this use-case?
--> AMAZON EC2 INSTANCE STORE // question about scientific data -> instance store is correct option.

An Amazon Machine Image (AMI) provides the information required to launch an instance. You must specify an AMI when you launch an instance. You can launch multiple instances from a single AMI when you need multiple instances with the same configuration.

The AMI must be in the same region as that of the EC2 instance to be launched.
If the AMI exists in a different region, you can copy that AMI to the region where you want to launch the EC2 instance.
The region of AMI has no bearing on the performance of the EC2 instance.

Amazon EFS is a regional service storing data within and across multiple Availability Zones (AZs) for high availability and durability.
Amazon EC2 instances can access your file system across AZs, regions, and VPCs, while on-premises servers can access using AWS Direct Connect or AWS VPN.
EFS is highly available and durable . Data is replicated across AZ's by default.

DEVELOPER : provides general architectural guidance on how services can be used for various use-cases, workloads, or applications
BUISENESS : Contextual to your use-cases
ENTERPRISE : Consultative review and guidance based on your applications

Create separate AWS accounts for development and production environments to receive separate invoices". Every AWS account provides its own invoice end of the month.
You can get separate invoices for development and production environments by setting up separate AWS accounts for each environment.
You cannot create separate invoices based on tags. A Cost Allocation Tag is a label that you or AWS assigns to an AWS resource.
Each tag consists of a key and a value. For each resource, each tag key must be unique, and each tag key can have only one value.
You can use tags to organize your resources, and cost allocation tags to track your AWS costs on a detailed level.
AWS provides two types of cost allocation tags, an AWS generated tags and user-defined tags.
AWS defines, creates, and applies the AWS generated tags for you, and you define, create, and apply user-defined tags.
You must activate both types of tags separately before they can appear in Cost Explorer or on a cost allocation report.

// BY DEFAULT ENCRYPTION IS ENABLED FOR :
Amazon S3 Glacier - Amazon S3 Glacier (S3 Glacier), is a storage service optimized for infrequently used data, or "cold data. Data at rest stored in S3 Glacier is automatically server-side encrypted using 256-bit Advanced Encryption Standard (AES-256) with keys maintained by AWS.
AWS Storage Gateway - AWS Storage Gateway is a hybrid cloud storage service that gives you on-premises access to virtually unlimited cloud storage. All data transferred between the gateway and AWS storage is encrypted using SSL (for all three types of gateways - File, Volume and Tape Gateways).

// LAMBDA PRICING IS BASED ON :
Number of requests for the lambda function
The time it takes for the lambda function to execute

Examples for Linux\Ubuntu based instances:
--------------------------------------------------
1- If you run a Linux instance for 4 seconds or 20 seconds or 59 seconds, you will be charged for one minute. (this is what we mean by minimum of 1 minute)
2- If you run a Linux instance for 1 minute and 3 seconds, you will be charged for 1 minute and 3 seconds.
3- If you run a Linux instance for 3 hours, 25 minutes and 7 seconds, you will be charged for 3 hours, 25 minutes and 7 seconds.

Examples for non-Linux\Ubuntu instances:
---------------------------------------------------
1- If you run an instance for 4 seconds or 20 seconds or 59 seconds, you will be charged for one hour.
2- If you run an instance for 1 minute and 3 seconds, you will be charged for one hour.
3- If you run an instance for 3 hours, 25 minutes and 7 seconds, you will be charged for 4 hours.

For transferring 50PB of data , the cost efficient transport is SNOW MOBILE, not SNOW BALL.
Snow mobile can transport 100pb at once, while snow ball can do 100TB.

An engineering team is new to the AWS Cloud and it would like to launch a dev/test environment with low monthly pricing. Which AWS service can address this use-case --> LIGHT SAIL, not CLOUD FORMATION as it requires experience in working with AWS.
When using EBS Volumes, the volume and the instance must be in the same Availability Zone.

IAM ACCESS ADVISOR    : Shows the service permissions granted to a user and when those services were last accessed.
                        You can use this information to revise your policies. To summarize, you can identify unnecessary permissions so that you can revise your IAM policies accordingly.
IAM CREDENTIAL REPORT : You can generate and download a credential report that lists all users in your account and the status of their various credentials, including passwords, access keys, and MFA devices.
                        It is not used to review permissions granted to a user.

AWS CLOUDTRAIL INSIGHTS : AWS CloudTrail Insights helps AWS users identify and respond to unusual activity associated with write API calls by continuously analyzing CloudTrail management events.
                          Insights events are logged when CloudTrail detects unusual write management API activity in your account. If you have CloudTrail Insights enabled, and CloudTrail detects unusual activity, Insights events are delivered to the destination S3 bucket for your trail.

Memory Optimized instance types - Memory optimized instances are designed to deliver fast performance for workloads that process large data sets in memory. Memory-optimized instances offer large memory size for memory intensive applications including in-memory applications, in-memory databases, in-memory analytics solutions, High Performance Computing (HPC), scientific computing, and other memory-intensive applications.
Compute Optimized instance types - Compute Optimized instances are designed for applications that benefit from high compute power. These applications include compute-intensive applications like high-performance web servers, high-performance computing (HPC), scientific modelling, distributed analytics, and machine learning inference.
Storage Optimized instance types - Dense-storage instances are designed for workloads that require high sequential read and write access to very large data sets, such as Hadoop distributed computing, massively parallel processing data warehousing, and log processing applications. The Dense-storage instances offer the best price/GB-storage and price/disk-throughput across other EC2 instances.
Accelerated computing instance types - Accelerated Computing instance family is a family of instances that use hardware accelerators, or co-processors, to perform some functions, such as floating-point number calculation and graphics processing, more efficiently than is possible in software running on CPUs. Amazon EC2 provides three types of Accelerated Computing instances – GPU compute instances for general-purpose computing,

In CLOUDTRAIL, by default only MANAGEMENT events are logged.
DATA EVENTS & INSIGHT EVENTS are available for extra cost.
EFS is a regional service.
WORKSPACES is a global service.

AWS Elastic Beanstalk - AWS Elastic Beanstalk is an easy-to-use service for deploying and scaling web applications and services developed with Java, .NET, PHP, Node.js, Python, Ruby, Go, and Docker on familiar servers such as Apache, Nginx, Passenger, and IIS. You can simply upload your code and Elastic Beanstalk automatically handles the deployment, from capacity provisioning, load balancing, auto-scaling to application health monitoring.
At the same time, you retain full control over the AWS resources powering your application and can access the underlying resources at any time.

Control Tower is an AWS native service providing a pre-defined set of blueprints and guardrails to help customers implement a landing zone for new AWS accounts - Control Tower is an AWS native service providing a pre-defined set of blueprints and guardrails to help customers implement a landing zone for new AWS accounts.
Control Tower is designed to provide an easy, self-service setup experience and an interactive user interface for ongoing governance with guardrails.
While Control Tower automates creation of a new landing zone with pre-configured blueprints (e.g., AWS SSO for directory and access), the AWS Landing Zone solution provides a configurable setup of a landing zone with rich customization options through custom add-ons (e.g., Active Directory, Okta Directory) and ongoing modifications through a code deployment and configuration pipeline.

SCPs don't affect users or roles in the management account. They affect only the member accounts in your organization.
Service control policies (SCPs) are a type of organization policy that you can use to manage permissions in your organization.
SCPs offer central control over the maximum available permissions for all accounts in your organization.
SCPs help you to ensure your accounts stay within your organization’s access control guidelines. SCPs are available only in an organization that has all features enabled.
SCPs aren't available if your organization has enabled only the consolidated billing features.
SCPs alone are not sufficient to grant permissions to the accounts in your organization.
No permissions are granted by an SCP. An SCP defines a guardrail, or sets limits, on the actions that the account's administrator can delegate to the IAM users and roles in the affected accounts.
The administrator must still attach identity-based or resource-based policies to IAM users or roles, or to the resources in your accounts to actually grant permissions.
The effective permissions are the logical intersection between what is allowed by the SCP and what is allowed by the IAM and resource-based policies.

Amazon Quantum Ledger Database - Amazon QLDB is a fully managed ledger database that provides a transparent, immutable, and cryptographically verifiable transaction log ‎owned by a central trusted authority.
Amazon QLDB can be used to track each and every application data change and maintains a complete and verifiable history of changes over time.
Ledgers are typically used to record a history of economic and financial activity in an organization.
Many organizations build applications with ledger-like functionality because they want to maintain an accurate history of their applications' data, for example, tracking the history of credits and debits in banking transactions, verifying the data lineage of an insurance claim, or tracing the movement of an item in a supply chain network.

AWS offers two types of Savings Plans:
COMPUTE SAVINGS PLAN  provide the most flexibility and help to reduce your costs by up to 66%.
These plans automatically apply to EC2 instance usage regardless of instance family, size, AZ, region, OS or tenancy, and also apply to Fargate and Lambda usage. For example, with Compute Savings Plans, you can change from C4 to M5 instances, shift a workload from EU (Ireland) to EU (London), or move a workload from EC2 to Fargate or Lambda at any time and automatically continue to pay the Savings Plans price.
EC2 INSTANCE SAVINGS PLAN  Plans provide the lowest prices, offering savings up to 72% in exchange for a commitment to the usage of individual instance families in a region (e.g. M5 usage in N. Virginia). This automatically reduces your cost on the selected instance family in that region regardless of AZ, size, OS or tenancy. EC2 Instance Savings Plans give you the flexibility to change your usage between instances within a family in that region. For example, you can move from c5.xlarge running Windows to c5.2xlarge running Linux and automatically benefit from the Savings Plans prices.

SELECT TWO
SELECT THREE
SELECT TWO
SELECT THREE
SELECT TWO
SELECT THREE
SELECT TWO
SELECT THREE
SELECT TWO
SELECT THREE
SELECT TWO
SELECT THREE
SELECT TWO
SELECT THREE
SELECT TWO
SELECT THREE
SELECT TWO
SELECT THREE
SELECT TWO
SELECT THREE


























####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
